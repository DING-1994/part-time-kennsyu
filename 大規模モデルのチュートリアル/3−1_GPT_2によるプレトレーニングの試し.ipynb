{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5hjCcLDr2WC",
        "outputId": "5d22594f-fda0-4d6f-c295-2ab0e149d7da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-09 04:21:53--  https://raw.githubusercontent.com/DING-1994/llm2023/main/icat.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31611 (31K) [text/plain]\n",
            "Saving to: ‘icat.txt’\n",
            "\n",
            "\ricat.txt              0%[                    ]       0  --.-KB/s               \ricat.txt            100%[===================>]  30.87K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2024-05-09 04:21:53 (12.8 MB/s) - ‘icat.txt’ saved [31611/31611]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/DING-1994/llm2023/main/icat.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "with open('icat.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(text[:100])"
      ],
      "metadata": {
        "id": "O6medjfRsLD9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f711edbe-8f5a-44ba-b379-c028e11376ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "吾輩は猫である。名前はまだ無い。\n",
            "　どこで生れたかとんと見当《けんとう》がつかぬ。何でも薄暗いじめじめした所でニャーニャー泣いていた事だけは記憶している。吾輩はここで始めて人間というものを見た。しかも\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xWI_VyAsN8F",
        "outputId": "d6562382-7b9f-4aaa-fa87-7b19a3c539f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  10553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 100 characters\n",
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "349bace0-d3b2-4eb6-eeae-166113d9dc46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "吾輩は猫である。名前はまだ無い。\n",
            "　どこで生れたかとんと見当《けんとう》がつかぬ。何でも薄暗いじめじめした所でニャーニャー泣いていた事だけは記憶している。吾輩はここで始めて人間というものを見た。しかも\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "e454d5f3-9fed-4045-c5e2-9cc496d92184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "―　、。々《》「」あいうえおかがきぎくぐけげこごさざしじすずせぜそぞただちっつづてでとどなにぬねのはばぱひびぴふぶぷへべぺほぼぽまみむめもゃやゅゆょよらりるれろわをんァアイオカサシジスゼタッデトドニフブペマャヤリルレワンヴ・ー一丈三上下不世両並中丸主乗乱予事二云五交亭人今仕他付代以件任会伸佇低住体何余作例供侮便俳倍倒候偉側偸傍催像儀儘儲元兄先光入全具内円再写冷凝処出分切初判別利到刺刻前割剿力加助勇勉動勝勢勤包化匹十午卑原去友双反取受口句叩台合同名吐向君否含吶吸吹吾呈呑味呼咽問善喉喊喝喟喰嗅嘆嘲器四困国園地坐坪垂垣執報場増墻壁壊声壱変外多夜大天太失奇奥奪女奴好妙始姿嫌子存学守完宗定実室害家容宿寒寝察対射尊小少尻屁居屈屋層崩左己師帯帰常幅平年広床底度座庭廓廻弁弓引弟弱張強弾当形彩彼待後得御徳徴心必忍忘快念怒思急性恐息悟悪悲情惜想意愚愛感慢慨憤憶懐我或戦所手打投折抛抱押拶持指挨挫捕掃掌掠接掻揄描提揚揶撚敗教敬数敷文斎斑断斯新方族日早昂明易昔星春昼時景智暖暗暮暴更書最月有服望朝木末本杉来板枚果枝枯架柄柔栄校根格案桐梧棄棒椽業極楽構槌様権横樹機櫃欠次正歩死残段母毎毒比毛毫気水永池決波泣泥洒活流浩涎涙淡減渡渾満溌溝滅漆漫濁瀟火灰炎炬点烈焔無然煙煮熱燃燵爪片物犬猟猫猶獣獰玉王珀珍現理琥甘生産用由画畜畠番畳疋疑痛痞瘠癖癪発白百的皮盛盟目盲直相真眠眸眺眼着睡睨知矮石破碌磨神禁禽程種稼穏穴突窃窘窟立竹笑第笹筆等筋答箆節籠粋約純紙細終経結給絵総線縁繰缶置罵美義翌習考者耳聞聴職肉肝肥育胃胆背胡胸腕腹膏膚膝臍臓自臭至良色若苦英茂茶草菊華落葉蔑蔭薄薬藁行表衾袋装裏褐西要見規覗覚親観解言記訪訳訴詐評試詩話誘語読誰課調談論謡謹識議護豊買賞質走起越足路跳身車軍転軽較載輝輩輪辛辰辺込迎近返迫述追退送逅逆透這通速造逢遇運遍過道達違遠遥遭邂邸郎部酷醒采重野量金銭鏡鑑長開間関降限除険陽随隙際障隠隣雑雨震露静非面音頃頓頭頸頼額顔願顫風飛食飯飲飾養餓首馬馳駄騒験驚骨髭魔魚鰡鳴鴉鹿黄黒鼓鼠鼻鼾＃？［］｜\n",
            "843\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "#print(\"stoi\",stoi)\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"吾\"))\n",
        "print(decode(encode(\"吾\")))\n",
        "print(encode(\"吾輩\"))\n",
        "print(decode(encode(\"吾輩\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "dccc394c-face-44ed-b04f-7b20cf27f521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[234]\n",
            "吾\n",
            "[234, 731]\n",
            "吾輩\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ELmRYm9wlgWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long) #encoding the entire text\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100]) # the 100 characters we looked at earier will to the GPT look like this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "cfcf8356-b387-4ca5-c7a1-e311b5c3d9e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10553]) torch.int64\n",
            "tensor([234, 731,  50, 534,  42,  10,  78,   4, 225, 194,  50,  65,  36, 522,\n",
            "         11,   4,   0,   2,  44,  23,  42, 546,  79,  35,  15,  43,  83,  43,\n",
            "        683, 340,   6,  21,  83,  43,  12,   7,  16,  39,  15,  47,   4, 150,\n",
            "         42,  69, 671, 431,  11,  28,  68,  28,  68,  27,  35, 380,  42,  99,\n",
            "        104, 113,  99, 104, 113, 495,  11,  41,  11,  35, 129,  36,  21,  50,\n",
            "        691, 375,  27,  41,  11,  78,   4, 234, 731,  50,  23,  23,  42, 284,\n",
            "         68,  41, 135, 779,  43,  11,  12,  69,  49,  82, 683,  35,   4,  27,\n",
            "         15,  69])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "print(\"train_data[:block_size+1]\",train_data[:block_size+1])# ths first \"block_size\" elements"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "d0db46b2-c2e6-4876-cb7f-6e44c823d8ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_data[:block_size+1] tensor([234, 731,  50, 534,  42,  10,  78,   4, 225])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "8e0fe444-26b2-40c9-b358-e98c140f3328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([234]) the target: 731\n",
            "when input is tensor([234, 731]) the target: 50\n",
            "when input is tensor([234, 731,  50]) the target: 534\n",
            "when input is tensor([234, 731,  50, 534]) the target: 42\n",
            "when input is tensor([234, 731,  50, 534,  42]) the target: 10\n",
            "when input is tensor([234, 731,  50, 534,  42,  10]) the target: 78\n",
            "when input is tensor([234, 731,  50, 534,  42,  10,  78]) the target: 4\n",
            "when input is tensor([234, 731,  50, 534,  42,  10,  78,   4]) the target: 225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = decode(torch.Tensor.tolist(x[:t+1]))\n",
        "    target = decode([int(y[t])])\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aca935ad-54e9-461c-fa3d-6db092f386d7",
        "id": "C8AQq3GuAnWQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is 吾 the target: 輩\n",
            "when input is 吾輩 the target: は\n",
            "when input is 吾輩は the target: 猫\n",
            "when input is 吾輩は猫 the target: で\n",
            "when input is 吾輩は猫で the target: あ\n",
            "when input is 吾輩は猫であ the target: る\n",
            "when input is 吾輩は猫である the target: 。\n",
            "when input is 吾輩は猫である。 the target: 名\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(137)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) #tensor([ 611, 7648, 3056, 3583]) the start index for each block\n",
        "    print(\"ix\",ix)\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "b2d27f41-1c3e-4b4e-e5d4-869ce136bbe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ix tensor([ 611, 7648, 3056, 3583])\n",
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[326,   6,  43,  12,  41,  11,   7, 199],\n",
            "        [ 77,  69, 520,  27,  19, 834, 202,  27],\n",
            "        [ 42,  10,  78,  16,   3, 754, 363,  19],\n",
            "        [ 76,  78,  78,   4, 563, 228,  50, 172]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[  6,  43,  12,  41,  11,   7, 199,  15],\n",
            "        [ 69, 520,  27,  19, 834, 202,  27,  41],\n",
            "        [ 10,  78,  16,   3, 754, 363,  19, 306],\n",
            "        [ 78,  78,   4, 563, 228,  50, 172, 418]])\n",
            "----\n",
            "when input is [326] the target: 6\n",
            "when input is [326, 6] the target: 43\n",
            "when input is [326, 6, 43] the target: 12\n",
            "when input is [326, 6, 43, 12] the target: 41\n",
            "when input is [326, 6, 43, 12, 41] the target: 11\n",
            "when input is [326, 6, 43, 12, 41, 11] the target: 7\n",
            "when input is [326, 6, 43, 12, 41, 11, 7] the target: 199\n",
            "when input is [326, 6, 43, 12, 41, 11, 7, 199] the target: 15\n",
            "when input is [77] the target: 69\n",
            "when input is [77, 69] the target: 520\n",
            "when input is [77, 69, 520] the target: 27\n",
            "when input is [77, 69, 520, 27] the target: 19\n",
            "when input is [77, 69, 520, 27, 19] the target: 834\n",
            "when input is [77, 69, 520, 27, 19, 834] the target: 202\n",
            "when input is [77, 69, 520, 27, 19, 834, 202] the target: 27\n",
            "when input is [77, 69, 520, 27, 19, 834, 202, 27] the target: 41\n",
            "when input is [42] the target: 10\n",
            "when input is [42, 10] the target: 78\n",
            "when input is [42, 10, 78] the target: 16\n",
            "when input is [42, 10, 78, 16] the target: 3\n",
            "when input is [42, 10, 78, 16, 3] the target: 754\n",
            "when input is [42, 10, 78, 16, 3, 754] the target: 363\n",
            "when input is [42, 10, 78, 16, 3, 754, 363] the target: 19\n",
            "when input is [42, 10, 78, 16, 3, 754, 363, 19] the target: 306\n",
            "when input is [76] the target: 78\n",
            "when input is [76, 78] the target: 78\n",
            "when input is [76, 78, 78] the target: 4\n",
            "when input is [76, 78, 78, 4] the target: 563\n",
            "when input is [76, 78, 78, 4, 563] the target: 228\n",
            "when input is [76, 78, 78, 4, 563, 228] the target: 50\n",
            "when input is [76, 78, 78, 4, 563, 228, 50] the target: 172\n",
            "when input is [76, 78, 78, 4, 563, 228, 50, 172] the target: 418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # our input to the transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "682e23ab-873c-4cc5-adc9-bfce65de1917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[326,   6,  43,  12,  41,  11,   7, 199],\n",
            "        [ 77,  69, 520,  27,  19, 834, 202,  27],\n",
            "        [ 42,  10,  78,  16,   3, 754, 363,  19],\n",
            "        [ 76,  78,  78,   4, 563, 228,  50, 172]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size): # vocab_size: the number of  all the unique characters\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)  # ！！！ rnn has make incrental input automatically achieve ！！！\n",
        "        # the 1st is the number of all vocabs needed to be embeded;\n",
        "        # the 2nd is the dimension of embedding\n",
        "        # There exist initial weights in nn.Embedding which construct an embedding vector for each vocab\n",
        "\n",
        "    def forward(self, idx, targets=None): # targets are given for computing loss\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "        print(\"idx\",idx)\n",
        "        print(\"logits\",logits)# embedding results\n",
        "\n",
        "        #computate loss\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            print(f\"B:{B},T:{T},C:{C}\")\n",
        "            logits = logits.view(B*T, C)\n",
        "            print(f\"logits.view:{logits}\")\n",
        "            targets = targets.view(B*T)\n",
        "            print(f\"targets:{targets}\")\n",
        "            loss = F.cross_entropy(logits, targets) # targets element denote position of element 1 in one-hot vector\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)# initialize nn.Embedding\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "#print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "f0eafcb1-cc2e-4a34-e3e6-12d0823e9b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "idx tensor([[326,   6,  43,  12,  41,  11,   7, 199],\n",
            "        [ 77,  69, 520,  27,  19, 834, 202,  27],\n",
            "        [ 42,  10,  78,  16,   3, 754, 363,  19],\n",
            "        [ 76,  78,  78,   4, 563, 228,  50, 172]])\n",
            "logits tensor([[[ 6.6333e-01, -9.5372e-01, -4.4486e-02,  ...,  1.3551e+00,\n",
            "           4.9192e-01, -1.4185e+00],\n",
            "         [ 5.4067e-01,  2.4406e-01, -9.5749e-01,  ..., -4.7437e-01,\n",
            "           8.1878e-01, -1.5438e+00],\n",
            "         [-7.3893e-01,  1.6159e+00, -3.6602e-02,  ..., -8.3727e-01,\n",
            "          -3.4574e-01,  9.2198e-01],\n",
            "         ...,\n",
            "         [-1.8706e+00,  1.7910e-02, -6.6118e-01,  ...,  1.6273e+00,\n",
            "          -6.8075e-03,  1.6387e-01],\n",
            "         [-1.3660e-01, -1.8731e+00,  7.1989e-01,  ..., -2.1054e-01,\n",
            "          -2.6211e+00,  8.2743e-02],\n",
            "         [-6.7364e-01, -6.7938e-01,  2.7056e+00,  ..., -9.8645e-01,\n",
            "          -6.7105e-01,  1.5582e+00]],\n",
            "\n",
            "        [[-7.4066e-01, -9.5449e-01,  1.3629e+00,  ...,  3.4559e+00,\n",
            "          -3.9365e-02,  6.6512e-01],\n",
            "         [-1.3151e-02, -7.4039e-01,  7.1473e-01,  ..., -5.6893e-01,\n",
            "          -1.7951e+00,  5.7970e-01],\n",
            "         [ 1.0438e+00, -2.5836e+00, -5.7831e-01,  ..., -1.0142e+00,\n",
            "          -1.2008e+00,  7.9993e-01],\n",
            "         ...,\n",
            "         [-5.7184e-03,  5.7169e-01,  4.9703e-01,  ..., -7.1085e-01,\n",
            "           1.2835e+00,  5.1377e-01],\n",
            "         [ 2.4554e-01, -9.4610e-01, -1.5815e-01,  ...,  1.3236e+00,\n",
            "          -5.0408e-02,  1.9647e+00],\n",
            "         [ 1.2326e+00, -2.3109e-01,  9.6790e-01,  ...,  1.7733e+00,\n",
            "           8.6378e-01, -6.9929e-01]],\n",
            "\n",
            "        [[ 1.3332e+00,  4.9670e-01,  3.7962e-01,  ...,  3.3361e-01,\n",
            "          -1.4728e-01, -4.9979e-01],\n",
            "         [-8.9487e-01,  9.3099e-01,  1.5862e+00,  ..., -5.5611e-01,\n",
            "          -4.1855e-01, -6.0793e-01],\n",
            "         [-6.7915e-01,  9.3735e-01, -2.9028e-01,  ...,  6.5554e-01,\n",
            "           8.9318e-01, -1.2497e+00],\n",
            "         ...,\n",
            "         [ 6.5920e-01, -2.7343e-01, -7.7374e-01,  ...,  2.3908e-01,\n",
            "          -1.6835e-01,  4.4357e-01],\n",
            "         [ 5.7760e-01, -2.9588e-02, -1.4586e+00,  ..., -2.2193e-01,\n",
            "          -1.0948e+00,  1.9215e+00],\n",
            "         [ 7.6270e-01,  5.1591e-01, -5.9458e-01,  ...,  1.7924e+00,\n",
            "          -5.9322e-03,  8.0199e-01]],\n",
            "\n",
            "        [[ 5.2558e-01, -3.8264e-01,  2.8353e-01,  ..., -1.1279e+00,\n",
            "          -7.3827e-02,  5.0174e-01],\n",
            "         [-6.7915e-01,  9.3735e-01, -2.9028e-01,  ...,  6.5554e-01,\n",
            "           8.9318e-01, -1.2497e+00],\n",
            "         [-6.7915e-01,  9.3735e-01, -2.9028e-01,  ...,  6.5554e-01,\n",
            "           8.9318e-01, -1.2497e+00],\n",
            "         ...,\n",
            "         [-1.7919e+00, -3.8838e-01,  1.8960e-02,  ...,  9.8247e-01,\n",
            "           2.0686e+00,  2.4510e-01],\n",
            "         [ 1.3230e+00,  7.9705e-01,  2.1183e-01,  ..., -1.3937e-01,\n",
            "          -5.3266e-01, -6.4386e-01],\n",
            "         [ 2.0686e-01, -3.8545e-01, -2.9891e-01,  ..., -1.4747e+00,\n",
            "          -6.3584e-01, -7.9214e-04]]], grad_fn=<EmbeddingBackward0>)\n",
            "B:4,T:8,C:843\n",
            "logits.view:tensor([[ 6.6333e-01, -9.5372e-01, -4.4486e-02,  ...,  1.3551e+00,\n",
            "          4.9192e-01, -1.4185e+00],\n",
            "        [ 5.4067e-01,  2.4406e-01, -9.5749e-01,  ..., -4.7437e-01,\n",
            "          8.1878e-01, -1.5438e+00],\n",
            "        [-7.3893e-01,  1.6159e+00, -3.6602e-02,  ..., -8.3727e-01,\n",
            "         -3.4574e-01,  9.2198e-01],\n",
            "        ...,\n",
            "        [-1.7919e+00, -3.8838e-01,  1.8960e-02,  ...,  9.8247e-01,\n",
            "          2.0686e+00,  2.4510e-01],\n",
            "        [ 1.3230e+00,  7.9705e-01,  2.1183e-01,  ..., -1.3937e-01,\n",
            "         -5.3266e-01, -6.4386e-01],\n",
            "        [ 2.0686e-01, -3.8545e-01, -2.9891e-01,  ..., -1.4747e+00,\n",
            "         -6.3584e-01, -7.9214e-04]], grad_fn=<ViewBackward0>)\n",
            "targets:tensor([  6,  43,  12,  41,  11,   7, 199,  15,  69, 520,  27,  19, 834, 202,\n",
            "         27,  41,  10,  78,  16,   3, 754, 363,  19, 306,  78,  78,   4, 563,\n",
            "        228,  50, 172, 418])\n",
            "torch.Size([32, 843])\n",
            "tensor(6.9690, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size): # vocab_size: the number of  all the unique characters\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for j in range(max_new_tokens): # given only initial promt and then the output/input incrementally generated\n",
        "            # get the predictions\n",
        "\n",
        "            logits, loss = self(idx) # go to forward function and loss is not considered\n",
        "            print(f\"idx:{idx},logits:{logits},loss:{loss}\")\n",
        "\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C) where T → 1 (last time step) # the previous steps' outputs have been known\n",
        "            print(f\"logits[:, -1, :]:{logits}\")\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            print(f\"probs:{probs}\")\n",
        "\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) output one character based on the 'probs'\n",
        "            print(f\"idx_next:{idx_next}\")\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "            if j >3:\n",
        "              break\n",
        "        return idx\n",
        "\n",
        "#print(f\"xb:{xb},yb:{yb}\")\n",
        "m = BigramLanguageModel(vocab_size)  #with generate function\n",
        "#logits, loss = m(xb, yb)\n",
        "#print(logits.shape)\n",
        "#print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n",
        "# given only initial promt and then the output/input incrementally generated\n"
      ],
      "metadata": {
        "id": "bgRiEr7DY7OP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8186662b-0ece-4c1d-b0bc-457c62e56e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "idx:tensor([[0]]),logits:tensor([[[ 1.8077e-01, -6.9988e-02, -3.5962e-01, -9.1520e-01,  6.2577e-01,\n",
            "           2.5510e-02,  9.5451e-01,  6.4349e-02,  3.6115e-01,  1.1679e+00,\n",
            "          -1.3499e+00, -5.1018e-01,  2.3596e-01, -2.3978e-01, -9.2111e-01,\n",
            "           1.5433e+00,  1.3488e+00, -1.3964e-01,  2.8580e-01,  9.6512e-01,\n",
            "          -2.0371e+00,  4.9314e-01,  1.4870e+00,  5.9103e-01,  1.2603e-01,\n",
            "          -1.5627e+00, -1.1601e+00, -3.3484e-01,  4.4777e-01, -8.0164e-01,\n",
            "           1.5236e+00,  2.5086e+00, -6.6310e-01, -2.5128e-01,  1.0101e+00,\n",
            "           1.2155e-01,  1.5840e-01,  1.1340e+00, -1.1539e+00, -2.9840e-01,\n",
            "          -5.0754e-01, -9.2392e-01,  5.4671e-01, -1.4948e+00, -1.2057e+00,\n",
            "           5.7182e-01, -5.9735e-01, -6.9368e-01,  1.6455e+00, -8.0299e-01,\n",
            "           1.3514e+00, -2.7592e-01, -1.5108e+00,  2.1048e+00,  2.7630e+00,\n",
            "          -1.7465e+00,  1.4516e+00, -1.5103e+00,  8.2115e-01, -2.1153e-01,\n",
            "           7.7890e-01,  1.5333e+00,  1.6097e+00, -4.0323e-01, -8.3447e-01,\n",
            "           5.9780e-01, -5.1406e-02, -6.4559e-02, -4.9701e-01,  4.6576e-01,\n",
            "          -2.5726e-01, -1.0673e+00,  2.0089e+00, -5.3698e-01,  2.2280e-01,\n",
            "           6.9705e-01, -1.4267e+00,  9.0594e-01,  1.4459e-01,  2.2800e-01,\n",
            "           2.4900e+00, -1.2237e+00,  1.0107e+00,  5.5600e-01, -1.5935e+00,\n",
            "          -1.2706e+00,  6.9033e-01, -1.9614e-01,  3.4491e-01, -3.4189e-01,\n",
            "           4.7587e-01, -7.6634e-01, -4.1896e-01, -4.3699e-01, -1.0012e+00,\n",
            "          -4.0943e-01, -1.6669e+00, -1.3651e+00, -1.6552e-01,  9.6225e-01,\n",
            "           3.1549e-02, -7.4190e-01, -2.9779e-01,  1.7166e-02, -1.7722e-01,\n",
            "          -1.3343e-01,  2.9396e-01,  1.3850e+00,  1.2091e-01,  2.5418e+00,\n",
            "          -6.4046e-01, -1.9740e+00, -3.2957e-01,  7.9589e-03,  9.2623e-01,\n",
            "          -1.8846e+00,  1.6696e-01,  4.5862e-01, -1.7662e+00,  5.8599e-01,\n",
            "           1.7510e+00,  2.8072e-01,  3.1096e-01, -6.5376e-01, -6.5763e-01,\n",
            "           3.1845e-01, -5.4959e-01, -1.4649e+00, -2.0555e+00,  1.8275e+00,\n",
            "           1.3035e+00, -4.5013e-01,  1.3471e+00,  1.6910e+00, -1.2445e-01,\n",
            "          -1.6824e+00, -2.6608e-02,  7.4049e-02,  1.0517e+00,  6.7789e-01,\n",
            "           3.0665e-01, -7.4723e-01,  7.4349e-01,  8.8766e-01,  2.2874e+00,\n",
            "           9.6114e-01, -1.5297e+00, -2.9122e-01, -1.1395e-01, -3.1367e-01,\n",
            "          -6.2931e-01,  1.1385e+00, -9.9127e-01,  1.6999e-01,  1.2249e+00,\n",
            "          -2.3454e-01, -1.0572e+00, -6.5427e-01,  1.5909e+00, -6.9949e-01,\n",
            "          -8.9606e-01,  6.6191e-02, -5.6280e-02,  2.3412e+00, -2.7234e+00,\n",
            "           5.0967e-01, -8.1447e-01, -2.4604e-01,  4.5085e-03,  2.0474e+00,\n",
            "          -1.5755e-01, -2.1867e-01, -1.3519e+00, -5.7281e-02, -1.8540e+00,\n",
            "          -1.3849e+00, -3.4540e-01, -1.1625e+00,  1.4448e-01,  1.6632e-01,\n",
            "           7.5070e-01,  9.1317e-01, -1.7277e+00,  1.3055e+00,  9.5932e-01,\n",
            "           1.0600e+00,  6.2986e-01, -1.2867e+00, -6.8748e-01,  2.1382e+00,\n",
            "           5.1141e-01,  1.2191e+00,  1.9098e-01, -3.4251e-01,  1.7955e+00,\n",
            "           1.3915e+00,  1.0785e+00, -6.1495e-01, -4.5885e-01,  5.6748e-01,\n",
            "           1.8289e-02, -1.6608e+00,  1.1169e+00,  5.1965e-01, -1.2423e+00,\n",
            "          -9.6182e-01, -8.4998e-02,  1.1854e-01,  2.9843e-01, -7.2636e-01,\n",
            "          -3.1187e-01, -4.5604e-01,  6.4407e-01,  6.0728e-01,  1.2397e+00,\n",
            "           7.3249e-01,  5.0418e-01,  8.7135e-01, -2.7416e-01, -7.4689e-01,\n",
            "          -5.8324e-01,  3.6988e-01, -5.5562e-01, -3.9828e-01, -5.8188e-01,\n",
            "          -2.2083e-01,  1.3537e-02, -3.0574e-01, -3.0384e-02,  8.2161e-01,\n",
            "           3.8670e-04, -4.4742e-01,  8.2040e-01, -1.5178e+00,  6.1587e-01,\n",
            "          -1.8648e+00, -9.7773e-01,  6.3224e-02, -4.5483e-01, -4.1474e-01,\n",
            "           1.4987e+00, -3.9867e-02, -8.0510e-01, -1.1624e+00,  4.2716e-01,\n",
            "          -2.8192e-01, -1.2773e-02, -8.7792e-01, -3.2248e-01,  1.8299e-01,\n",
            "          -9.3030e-01, -1.2488e+00,  1.1192e+00, -1.9079e+00, -5.2756e-01,\n",
            "           1.0807e+00,  4.5618e-01, -1.0917e+00, -8.2073e-01,  1.8634e+00,\n",
            "           8.1485e-01, -6.4297e-02,  1.4237e+00,  2.6173e-01, -1.8528e+00,\n",
            "           2.0186e-01, -1.1787e+00, -1.0358e-01, -1.7830e+00, -8.3234e-01,\n",
            "          -4.3462e-01, -1.2480e+00, -2.8797e-01,  8.8086e-01, -7.1896e-01,\n",
            "           1.7449e-01,  7.5198e-01, -6.2878e-02, -7.1113e-01,  9.8100e-01,\n",
            "          -7.2443e-01, -1.5010e+00, -2.8348e+00, -2.8272e+00, -1.7358e-01,\n",
            "           5.1187e-02, -6.5764e-01, -2.5729e+00,  2.1011e-02,  1.0060e+00,\n",
            "          -1.2492e+00,  2.4413e-01, -6.3866e-01, -3.1861e-01, -1.2942e+00,\n",
            "          -1.0726e+00,  2.2901e-01, -9.0008e-01,  6.6140e-01,  5.1178e-01,\n",
            "           6.7622e-01, -1.3639e+00,  5.4861e-01,  8.9502e-02,  3.5746e-01,\n",
            "          -1.6521e+00, -7.5838e-01,  6.9533e-02,  9.9369e-01, -2.8205e-01,\n",
            "           1.1088e+00, -1.9881e+00, -1.3916e+00,  1.2734e+00, -1.1732e+00,\n",
            "           5.8200e-01, -1.3185e+00,  7.8586e-01, -1.1501e+00,  1.3132e+00,\n",
            "           2.2007e+00, -2.1945e-01,  5.4272e-01,  2.5867e+00, -4.6874e-01,\n",
            "          -1.3375e-01,  3.8994e-01, -2.8840e-01, -1.4651e+00,  1.0136e-02,\n",
            "          -3.0044e-01, -1.5733e+00,  1.4811e-02, -4.4722e-02, -5.3666e-01,\n",
            "          -5.2229e-01, -2.1811e-01, -2.1608e+00,  7.8646e-01,  6.8536e-01,\n",
            "          -1.2576e+00,  6.0938e-01, -2.0551e+00, -4.4305e-01, -6.4995e-01,\n",
            "          -6.8697e-01,  2.5674e-01, -1.2669e+00,  2.6449e-01, -6.4450e-01,\n",
            "           1.0834e+00, -7.9946e-01,  2.9225e-01,  1.3143e+00,  1.2607e+00,\n",
            "          -3.5046e-01, -2.0660e+00,  1.0575e+00, -1.0572e+00,  9.9107e-01,\n",
            "          -7.9706e-02,  1.0751e+00,  2.3813e-01,  5.7572e-01,  1.6685e+00,\n",
            "           5.9758e-01, -1.8736e+00,  1.2910e+00, -3.7531e-01, -1.8943e+00,\n",
            "           5.5568e-01,  8.5669e-01, -8.4607e-01,  5.0153e-01, -9.6565e-01,\n",
            "          -7.2546e-01,  9.8995e-02,  5.9284e-01, -4.2208e-02, -9.5664e-01,\n",
            "           1.4424e+00,  4.3408e-01, -4.2923e-01,  3.6661e-01,  1.2748e-01,\n",
            "          -5.6004e-02,  8.3154e-01, -5.5116e-01,  1.0477e+00,  1.6187e+00,\n",
            "           4.1604e-01,  3.3619e-01, -4.5124e-01, -6.9964e-01,  9.2076e-01,\n",
            "          -9.9634e-01,  1.2962e+00, -2.2434e+00,  5.2718e-01, -1.5849e-01,\n",
            "           9.3309e-02,  6.9736e-02, -1.1470e+00, -1.0414e+00, -2.5724e-01,\n",
            "          -7.2227e-01,  1.6433e-01, -1.3590e+00,  9.6216e-01, -7.6414e-01,\n",
            "          -1.7653e+00,  6.8838e-01, -2.2454e-01,  2.4676e-01,  1.7475e-01,\n",
            "           5.2430e-01,  3.0906e-01,  1.1661e+00, -2.1821e+00, -1.0422e+00,\n",
            "           1.0207e+00,  3.2082e+00, -3.7624e+00, -5.3301e-01,  6.6301e-01,\n",
            "          -1.5717e+00, -5.6216e-01, -2.9642e-01,  5.5124e-01, -1.2364e+00,\n",
            "           9.4089e-01,  7.6084e-01, -1.3756e+00,  1.2168e+00,  2.6774e-02,\n",
            "          -2.1902e+00, -1.1730e+00,  2.5181e+00,  1.6212e+00, -1.8134e+00,\n",
            "           2.0867e+00,  1.5351e-01,  1.1355e-01, -1.9785e-01,  1.6621e+00,\n",
            "           6.1514e-01,  6.7634e-01,  6.2280e-01,  9.4300e-02, -3.1558e-01,\n",
            "           7.8496e-01, -8.6994e-01, -1.6525e+00, -8.8157e-01, -1.4546e+00,\n",
            "           2.4102e-01, -1.6206e+00,  4.4878e-01,  5.0098e-01, -7.7269e-01,\n",
            "          -1.0796e+00,  3.5018e-03,  6.8618e-04,  7.9731e-02,  1.9934e-01,\n",
            "          -1.5677e+00, -2.1165e+00, -1.1813e+00,  7.5047e-02,  6.5496e-01,\n",
            "          -5.0057e-01, -2.1613e-01,  2.8637e-02, -7.7717e-02,  6.6956e-03,\n",
            "          -1.2177e+00,  1.1479e+00, -1.5735e+00,  1.3876e+00,  7.2512e-01,\n",
            "           6.4547e-01, -3.3132e-01, -1.0390e+00,  9.1116e-01,  1.2984e+00,\n",
            "           5.5509e-01, -4.6531e-01, -5.5186e-01,  1.1925e+00, -6.6420e-01,\n",
            "          -9.1165e-03, -1.1712e+00,  4.8306e-01,  3.5048e-01, -5.7443e-01,\n",
            "           1.2531e+00, -6.7409e-01,  3.9710e-01,  1.9287e-01, -2.1749e+00,\n",
            "           1.6730e+00, -4.2359e-02, -1.1758e-01,  1.0546e+00, -1.5694e-02,\n",
            "           2.4782e-01,  5.0760e-01, -9.0286e-01,  1.7872e+00,  8.9457e-02,\n",
            "          -3.7475e-01, -4.7815e-01, -6.0669e-01,  1.8328e+00,  2.9308e-01,\n",
            "           4.3631e-02,  2.1370e+00, -6.8247e-01, -1.6026e+00, -1.3362e-01,\n",
            "           1.1407e+00,  8.9348e-01, -2.4000e+00, -1.3420e+00, -1.0023e+00,\n",
            "          -1.9870e+00, -1.2166e-01, -8.0738e-01,  9.2551e-01, -1.4814e+00,\n",
            "           1.2910e-01, -1.7058e+00,  2.0330e+00, -5.0623e-01, -1.0159e-01,\n",
            "          -3.0157e-01,  1.2501e+00, -2.1368e-01, -1.1994e+00, -8.4656e-01,\n",
            "          -3.3513e-01,  1.0041e+00,  8.6564e-01,  1.6879e-01, -2.3525e-01,\n",
            "          -2.5862e-01,  1.3112e-02,  8.7190e-01,  9.1022e-01, -1.8745e-01,\n",
            "           7.2289e-01,  1.4742e+00, -4.0481e-01, -1.2273e+00,  3.3822e-01,\n",
            "           3.6413e-01, -1.4508e+00, -3.8141e-01,  7.2202e-01,  3.4612e-01,\n",
            "           9.9673e-01,  3.5749e-01,  1.1871e-01, -1.0623e-01,  2.9900e-01,\n",
            "           1.1986e-01, -1.2433e+00,  1.7859e+00,  9.1911e-01,  2.3263e-01,\n",
            "           4.5911e-01,  2.5560e-01, -3.5423e-01,  6.6902e-01,  7.5347e-01,\n",
            "          -5.3594e-01, -1.0277e+00,  5.3469e-01, -7.9583e-01,  4.7801e-01,\n",
            "           1.5877e-01, -2.3106e+00,  3.2266e-01,  1.5431e+00, -1.0392e+00,\n",
            "          -9.6783e-01, -6.9605e-01,  4.6839e-01,  3.5861e-01, -8.7726e-01,\n",
            "           3.2667e-01, -5.6669e-01,  4.3759e-02, -4.4546e-01, -2.6725e-02,\n",
            "           1.7926e+00, -2.0707e+00, -1.8788e+00, -8.2754e-01,  3.1572e-01,\n",
            "           1.2089e+00,  2.7238e-01,  2.8822e-02,  9.4213e-01, -1.5618e+00,\n",
            "           1.0711e+00,  4.9008e-01, -4.8759e-01,  1.2397e+00, -2.1052e+00,\n",
            "           9.2977e-01, -1.5635e+00,  3.4689e-01, -9.2321e-01,  2.3999e+00,\n",
            "          -6.8508e-01,  2.5947e+00,  9.3561e-01, -4.3714e-01, -1.0092e-01,\n",
            "           4.6841e-03, -7.5602e-02, -2.2571e+00,  5.8273e-03, -2.8562e-02,\n",
            "           4.7413e-01,  3.5467e-01,  3.1459e-01,  8.0750e-02,  2.6207e+00,\n",
            "          -1.2825e+00, -4.5248e-01, -2.7716e-01, -1.2963e+00, -1.3049e-01,\n",
            "          -2.4631e-02, -9.5709e-01,  6.3591e-01,  2.5234e-01,  5.0451e-01,\n",
            "          -6.8577e-01,  1.8943e-01,  1.3230e+00, -7.3024e-01, -5.4440e-01,\n",
            "           3.9989e-01, -2.1747e+00,  2.4936e-01,  1.3540e-01, -3.0465e+00,\n",
            "          -1.3441e+00, -2.8268e-01, -6.8872e-01, -6.8965e-01,  5.8992e-01,\n",
            "           5.5318e-01,  6.5104e-02, -1.7956e+00,  1.3145e+00,  1.7042e+00,\n",
            "           5.2542e-01, -1.2803e+00, -1.1621e+00,  6.6521e-01,  2.9128e-02,\n",
            "           3.6271e+00, -1.3567e-01, -4.6482e-01, -1.4324e+00,  1.2539e-01,\n",
            "          -1.1245e+00,  4.8809e-01, -6.8958e-01, -7.0803e-01, -3.1519e-01,\n",
            "           7.1964e-01, -1.7802e-02, -1.2635e+00,  8.9144e-01, -1.2858e+00,\n",
            "          -2.1067e+00, -1.9922e+00,  7.6294e-01, -5.9484e-01,  9.8280e-01,\n",
            "          -4.1515e-01, -2.0259e-01, -1.8955e+00,  6.1170e-01,  1.0955e-01,\n",
            "           1.5681e-02, -1.0636e+00,  8.3979e-01,  4.2112e-01, -2.0257e+00,\n",
            "           1.0383e+00,  5.1819e-01,  5.2830e-01, -5.6484e-01,  3.8275e-02,\n",
            "           3.0485e-01, -2.0662e+00, -1.1418e+00, -1.3911e-01,  1.0827e+00,\n",
            "           1.1522e+00,  5.1984e-01, -8.9818e-01,  3.7491e-01, -4.2227e-02,\n",
            "           7.1970e-01,  1.8447e+00,  1.4385e+00, -1.3166e+00,  1.2690e+00,\n",
            "          -2.3594e-01,  4.0538e-01, -3.3332e-01, -1.9416e+00, -1.4176e+00,\n",
            "          -6.0523e-03, -1.3411e+00, -1.0705e+00,  1.3173e+00,  3.7193e-01,\n",
            "           5.1977e-01,  9.8285e-01,  8.0482e-01,  2.3235e+00, -1.5808e+00,\n",
            "          -1.7794e+00,  4.0939e-01,  9.2944e-02,  4.9701e-01,  5.8485e-02,\n",
            "           1.0326e-01,  7.2012e-02,  1.1080e+00,  7.2926e-01,  3.6650e-01,\n",
            "          -9.2689e-01, -1.2297e+00,  1.0633e-01,  5.8153e-02,  5.3531e-01,\n",
            "           1.1746e+00,  8.5400e-02,  1.1270e-02, -4.3248e-01, -2.6942e-01,\n",
            "          -2.5596e+00, -1.7529e-01, -9.7407e-01,  1.2510e-01,  1.6380e+00,\n",
            "          -5.7628e-01, -1.4705e+00, -2.6125e-01, -5.3680e-01, -7.8720e-01,\n",
            "           1.4845e-02,  4.6483e-01,  6.1097e-01, -1.5670e+00, -7.8682e-01,\n",
            "           3.9667e-01, -9.7545e-01,  5.1219e-01,  3.3299e-01,  1.0995e+00,\n",
            "           4.0335e-01, -1.1089e+00, -5.3754e-02, -1.2741e+00, -7.1055e-01,\n",
            "           7.4904e-01,  6.7999e-01, -2.7101e-01,  2.9964e-02,  2.4001e-01,\n",
            "          -2.0333e+00, -5.5379e-01,  1.6957e-01,  6.9847e-01,  3.7444e-01,\n",
            "          -1.4854e+00,  6.5020e-01, -2.0446e-01, -1.8770e-01,  4.4862e-01,\n",
            "           5.5716e-01, -5.4564e-01,  1.6367e-01, -1.8262e+00,  6.8537e-01,\n",
            "          -8.7624e-01, -1.4931e+00,  1.6634e+00, -4.7180e-01,  5.8567e-01,\n",
            "          -9.5790e-01,  9.4345e-01, -2.1992e+00,  4.1636e-01, -3.5643e-01,\n",
            "          -1.0305e+00, -1.0478e+00,  2.1709e+00, -1.0414e+00, -1.3088e+00,\n",
            "           1.0169e+00,  4.7949e-01, -1.8578e+00,  2.3297e-01, -2.2054e-01,\n",
            "          -1.4869e+00, -6.2718e-01,  2.0144e+00, -6.9383e-01,  1.6325e+00,\n",
            "          -7.5949e-01,  5.8486e-01,  1.0116e+00, -3.8033e-01, -4.3962e-01,\n",
            "           1.4687e-01,  1.2890e+00, -2.0776e-02,  6.9753e-02, -7.2962e-01,\n",
            "           1.6526e-01, -3.3901e-01,  1.5416e+00,  1.0231e+00,  1.3392e+00,\n",
            "           1.6886e+00,  7.0994e-01,  1.3693e+00, -7.0759e-01,  1.5387e+00,\n",
            "          -9.5591e-01,  2.0515e+00,  9.6234e-03]]],\n",
            "       grad_fn=<EmbeddingBackward0>),loss:None\n",
            "logits[:, -1, :]:tensor([[ 1.8077e-01, -6.9988e-02, -3.5962e-01, -9.1520e-01,  6.2577e-01,\n",
            "          2.5510e-02,  9.5451e-01,  6.4349e-02,  3.6115e-01,  1.1679e+00,\n",
            "         -1.3499e+00, -5.1018e-01,  2.3596e-01, -2.3978e-01, -9.2111e-01,\n",
            "          1.5433e+00,  1.3488e+00, -1.3964e-01,  2.8580e-01,  9.6512e-01,\n",
            "         -2.0371e+00,  4.9314e-01,  1.4870e+00,  5.9103e-01,  1.2603e-01,\n",
            "         -1.5627e+00, -1.1601e+00, -3.3484e-01,  4.4777e-01, -8.0164e-01,\n",
            "          1.5236e+00,  2.5086e+00, -6.6310e-01, -2.5128e-01,  1.0101e+00,\n",
            "          1.2155e-01,  1.5840e-01,  1.1340e+00, -1.1539e+00, -2.9840e-01,\n",
            "         -5.0754e-01, -9.2392e-01,  5.4671e-01, -1.4948e+00, -1.2057e+00,\n",
            "          5.7182e-01, -5.9735e-01, -6.9368e-01,  1.6455e+00, -8.0299e-01,\n",
            "          1.3514e+00, -2.7592e-01, -1.5108e+00,  2.1048e+00,  2.7630e+00,\n",
            "         -1.7465e+00,  1.4516e+00, -1.5103e+00,  8.2115e-01, -2.1153e-01,\n",
            "          7.7890e-01,  1.5333e+00,  1.6097e+00, -4.0323e-01, -8.3447e-01,\n",
            "          5.9780e-01, -5.1406e-02, -6.4559e-02, -4.9701e-01,  4.6576e-01,\n",
            "         -2.5726e-01, -1.0673e+00,  2.0089e+00, -5.3698e-01,  2.2280e-01,\n",
            "          6.9705e-01, -1.4267e+00,  9.0594e-01,  1.4459e-01,  2.2800e-01,\n",
            "          2.4900e+00, -1.2237e+00,  1.0107e+00,  5.5600e-01, -1.5935e+00,\n",
            "         -1.2706e+00,  6.9033e-01, -1.9614e-01,  3.4491e-01, -3.4189e-01,\n",
            "          4.7587e-01, -7.6634e-01, -4.1896e-01, -4.3699e-01, -1.0012e+00,\n",
            "         -4.0943e-01, -1.6669e+00, -1.3651e+00, -1.6552e-01,  9.6225e-01,\n",
            "          3.1549e-02, -7.4190e-01, -2.9779e-01,  1.7166e-02, -1.7722e-01,\n",
            "         -1.3343e-01,  2.9396e-01,  1.3850e+00,  1.2091e-01,  2.5418e+00,\n",
            "         -6.4046e-01, -1.9740e+00, -3.2957e-01,  7.9589e-03,  9.2623e-01,\n",
            "         -1.8846e+00,  1.6696e-01,  4.5862e-01, -1.7662e+00,  5.8599e-01,\n",
            "          1.7510e+00,  2.8072e-01,  3.1096e-01, -6.5376e-01, -6.5763e-01,\n",
            "          3.1845e-01, -5.4959e-01, -1.4649e+00, -2.0555e+00,  1.8275e+00,\n",
            "          1.3035e+00, -4.5013e-01,  1.3471e+00,  1.6910e+00, -1.2445e-01,\n",
            "         -1.6824e+00, -2.6608e-02,  7.4049e-02,  1.0517e+00,  6.7789e-01,\n",
            "          3.0665e-01, -7.4723e-01,  7.4349e-01,  8.8766e-01,  2.2874e+00,\n",
            "          9.6114e-01, -1.5297e+00, -2.9122e-01, -1.1395e-01, -3.1367e-01,\n",
            "         -6.2931e-01,  1.1385e+00, -9.9127e-01,  1.6999e-01,  1.2249e+00,\n",
            "         -2.3454e-01, -1.0572e+00, -6.5427e-01,  1.5909e+00, -6.9949e-01,\n",
            "         -8.9606e-01,  6.6191e-02, -5.6280e-02,  2.3412e+00, -2.7234e+00,\n",
            "          5.0967e-01, -8.1447e-01, -2.4604e-01,  4.5085e-03,  2.0474e+00,\n",
            "         -1.5755e-01, -2.1867e-01, -1.3519e+00, -5.7281e-02, -1.8540e+00,\n",
            "         -1.3849e+00, -3.4540e-01, -1.1625e+00,  1.4448e-01,  1.6632e-01,\n",
            "          7.5070e-01,  9.1317e-01, -1.7277e+00,  1.3055e+00,  9.5932e-01,\n",
            "          1.0600e+00,  6.2986e-01, -1.2867e+00, -6.8748e-01,  2.1382e+00,\n",
            "          5.1141e-01,  1.2191e+00,  1.9098e-01, -3.4251e-01,  1.7955e+00,\n",
            "          1.3915e+00,  1.0785e+00, -6.1495e-01, -4.5885e-01,  5.6748e-01,\n",
            "          1.8289e-02, -1.6608e+00,  1.1169e+00,  5.1965e-01, -1.2423e+00,\n",
            "         -9.6182e-01, -8.4998e-02,  1.1854e-01,  2.9843e-01, -7.2636e-01,\n",
            "         -3.1187e-01, -4.5604e-01,  6.4407e-01,  6.0728e-01,  1.2397e+00,\n",
            "          7.3249e-01,  5.0418e-01,  8.7135e-01, -2.7416e-01, -7.4689e-01,\n",
            "         -5.8324e-01,  3.6988e-01, -5.5562e-01, -3.9828e-01, -5.8188e-01,\n",
            "         -2.2083e-01,  1.3537e-02, -3.0574e-01, -3.0384e-02,  8.2161e-01,\n",
            "          3.8670e-04, -4.4742e-01,  8.2040e-01, -1.5178e+00,  6.1587e-01,\n",
            "         -1.8648e+00, -9.7773e-01,  6.3224e-02, -4.5483e-01, -4.1474e-01,\n",
            "          1.4987e+00, -3.9867e-02, -8.0510e-01, -1.1624e+00,  4.2716e-01,\n",
            "         -2.8192e-01, -1.2773e-02, -8.7792e-01, -3.2248e-01,  1.8299e-01,\n",
            "         -9.3030e-01, -1.2488e+00,  1.1192e+00, -1.9079e+00, -5.2756e-01,\n",
            "          1.0807e+00,  4.5618e-01, -1.0917e+00, -8.2073e-01,  1.8634e+00,\n",
            "          8.1485e-01, -6.4297e-02,  1.4237e+00,  2.6173e-01, -1.8528e+00,\n",
            "          2.0186e-01, -1.1787e+00, -1.0358e-01, -1.7830e+00, -8.3234e-01,\n",
            "         -4.3462e-01, -1.2480e+00, -2.8797e-01,  8.8086e-01, -7.1896e-01,\n",
            "          1.7449e-01,  7.5198e-01, -6.2878e-02, -7.1113e-01,  9.8100e-01,\n",
            "         -7.2443e-01, -1.5010e+00, -2.8348e+00, -2.8272e+00, -1.7358e-01,\n",
            "          5.1187e-02, -6.5764e-01, -2.5729e+00,  2.1011e-02,  1.0060e+00,\n",
            "         -1.2492e+00,  2.4413e-01, -6.3866e-01, -3.1861e-01, -1.2942e+00,\n",
            "         -1.0726e+00,  2.2901e-01, -9.0008e-01,  6.6140e-01,  5.1178e-01,\n",
            "          6.7622e-01, -1.3639e+00,  5.4861e-01,  8.9502e-02,  3.5746e-01,\n",
            "         -1.6521e+00, -7.5838e-01,  6.9533e-02,  9.9369e-01, -2.8205e-01,\n",
            "          1.1088e+00, -1.9881e+00, -1.3916e+00,  1.2734e+00, -1.1732e+00,\n",
            "          5.8200e-01, -1.3185e+00,  7.8586e-01, -1.1501e+00,  1.3132e+00,\n",
            "          2.2007e+00, -2.1945e-01,  5.4272e-01,  2.5867e+00, -4.6874e-01,\n",
            "         -1.3375e-01,  3.8994e-01, -2.8840e-01, -1.4651e+00,  1.0136e-02,\n",
            "         -3.0044e-01, -1.5733e+00,  1.4811e-02, -4.4722e-02, -5.3666e-01,\n",
            "         -5.2229e-01, -2.1811e-01, -2.1608e+00,  7.8646e-01,  6.8536e-01,\n",
            "         -1.2576e+00,  6.0938e-01, -2.0551e+00, -4.4305e-01, -6.4995e-01,\n",
            "         -6.8697e-01,  2.5674e-01, -1.2669e+00,  2.6449e-01, -6.4450e-01,\n",
            "          1.0834e+00, -7.9946e-01,  2.9225e-01,  1.3143e+00,  1.2607e+00,\n",
            "         -3.5046e-01, -2.0660e+00,  1.0575e+00, -1.0572e+00,  9.9107e-01,\n",
            "         -7.9706e-02,  1.0751e+00,  2.3813e-01,  5.7572e-01,  1.6685e+00,\n",
            "          5.9758e-01, -1.8736e+00,  1.2910e+00, -3.7531e-01, -1.8943e+00,\n",
            "          5.5568e-01,  8.5669e-01, -8.4607e-01,  5.0153e-01, -9.6565e-01,\n",
            "         -7.2546e-01,  9.8995e-02,  5.9284e-01, -4.2208e-02, -9.5664e-01,\n",
            "          1.4424e+00,  4.3408e-01, -4.2923e-01,  3.6661e-01,  1.2748e-01,\n",
            "         -5.6004e-02,  8.3154e-01, -5.5116e-01,  1.0477e+00,  1.6187e+00,\n",
            "          4.1604e-01,  3.3619e-01, -4.5124e-01, -6.9964e-01,  9.2076e-01,\n",
            "         -9.9634e-01,  1.2962e+00, -2.2434e+00,  5.2718e-01, -1.5849e-01,\n",
            "          9.3309e-02,  6.9736e-02, -1.1470e+00, -1.0414e+00, -2.5724e-01,\n",
            "         -7.2227e-01,  1.6433e-01, -1.3590e+00,  9.6216e-01, -7.6414e-01,\n",
            "         -1.7653e+00,  6.8838e-01, -2.2454e-01,  2.4676e-01,  1.7475e-01,\n",
            "          5.2430e-01,  3.0906e-01,  1.1661e+00, -2.1821e+00, -1.0422e+00,\n",
            "          1.0207e+00,  3.2082e+00, -3.7624e+00, -5.3301e-01,  6.6301e-01,\n",
            "         -1.5717e+00, -5.6216e-01, -2.9642e-01,  5.5124e-01, -1.2364e+00,\n",
            "          9.4089e-01,  7.6084e-01, -1.3756e+00,  1.2168e+00,  2.6774e-02,\n",
            "         -2.1902e+00, -1.1730e+00,  2.5181e+00,  1.6212e+00, -1.8134e+00,\n",
            "          2.0867e+00,  1.5351e-01,  1.1355e-01, -1.9785e-01,  1.6621e+00,\n",
            "          6.1514e-01,  6.7634e-01,  6.2280e-01,  9.4300e-02, -3.1558e-01,\n",
            "          7.8496e-01, -8.6994e-01, -1.6525e+00, -8.8157e-01, -1.4546e+00,\n",
            "          2.4102e-01, -1.6206e+00,  4.4878e-01,  5.0098e-01, -7.7269e-01,\n",
            "         -1.0796e+00,  3.5018e-03,  6.8618e-04,  7.9731e-02,  1.9934e-01,\n",
            "         -1.5677e+00, -2.1165e+00, -1.1813e+00,  7.5047e-02,  6.5496e-01,\n",
            "         -5.0057e-01, -2.1613e-01,  2.8637e-02, -7.7717e-02,  6.6956e-03,\n",
            "         -1.2177e+00,  1.1479e+00, -1.5735e+00,  1.3876e+00,  7.2512e-01,\n",
            "          6.4547e-01, -3.3132e-01, -1.0390e+00,  9.1116e-01,  1.2984e+00,\n",
            "          5.5509e-01, -4.6531e-01, -5.5186e-01,  1.1925e+00, -6.6420e-01,\n",
            "         -9.1165e-03, -1.1712e+00,  4.8306e-01,  3.5048e-01, -5.7443e-01,\n",
            "          1.2531e+00, -6.7409e-01,  3.9710e-01,  1.9287e-01, -2.1749e+00,\n",
            "          1.6730e+00, -4.2359e-02, -1.1758e-01,  1.0546e+00, -1.5694e-02,\n",
            "          2.4782e-01,  5.0760e-01, -9.0286e-01,  1.7872e+00,  8.9457e-02,\n",
            "         -3.7475e-01, -4.7815e-01, -6.0669e-01,  1.8328e+00,  2.9308e-01,\n",
            "          4.3631e-02,  2.1370e+00, -6.8247e-01, -1.6026e+00, -1.3362e-01,\n",
            "          1.1407e+00,  8.9348e-01, -2.4000e+00, -1.3420e+00, -1.0023e+00,\n",
            "         -1.9870e+00, -1.2166e-01, -8.0738e-01,  9.2551e-01, -1.4814e+00,\n",
            "          1.2910e-01, -1.7058e+00,  2.0330e+00, -5.0623e-01, -1.0159e-01,\n",
            "         -3.0157e-01,  1.2501e+00, -2.1368e-01, -1.1994e+00, -8.4656e-01,\n",
            "         -3.3513e-01,  1.0041e+00,  8.6564e-01,  1.6879e-01, -2.3525e-01,\n",
            "         -2.5862e-01,  1.3112e-02,  8.7190e-01,  9.1022e-01, -1.8745e-01,\n",
            "          7.2289e-01,  1.4742e+00, -4.0481e-01, -1.2273e+00,  3.3822e-01,\n",
            "          3.6413e-01, -1.4508e+00, -3.8141e-01,  7.2202e-01,  3.4612e-01,\n",
            "          9.9673e-01,  3.5749e-01,  1.1871e-01, -1.0623e-01,  2.9900e-01,\n",
            "          1.1986e-01, -1.2433e+00,  1.7859e+00,  9.1911e-01,  2.3263e-01,\n",
            "          4.5911e-01,  2.5560e-01, -3.5423e-01,  6.6902e-01,  7.5347e-01,\n",
            "         -5.3594e-01, -1.0277e+00,  5.3469e-01, -7.9583e-01,  4.7801e-01,\n",
            "          1.5877e-01, -2.3106e+00,  3.2266e-01,  1.5431e+00, -1.0392e+00,\n",
            "         -9.6783e-01, -6.9605e-01,  4.6839e-01,  3.5861e-01, -8.7726e-01,\n",
            "          3.2667e-01, -5.6669e-01,  4.3759e-02, -4.4546e-01, -2.6725e-02,\n",
            "          1.7926e+00, -2.0707e+00, -1.8788e+00, -8.2754e-01,  3.1572e-01,\n",
            "          1.2089e+00,  2.7238e-01,  2.8822e-02,  9.4213e-01, -1.5618e+00,\n",
            "          1.0711e+00,  4.9008e-01, -4.8759e-01,  1.2397e+00, -2.1052e+00,\n",
            "          9.2977e-01, -1.5635e+00,  3.4689e-01, -9.2321e-01,  2.3999e+00,\n",
            "         -6.8508e-01,  2.5947e+00,  9.3561e-01, -4.3714e-01, -1.0092e-01,\n",
            "          4.6841e-03, -7.5602e-02, -2.2571e+00,  5.8273e-03, -2.8562e-02,\n",
            "          4.7413e-01,  3.5467e-01,  3.1459e-01,  8.0750e-02,  2.6207e+00,\n",
            "         -1.2825e+00, -4.5248e-01, -2.7716e-01, -1.2963e+00, -1.3049e-01,\n",
            "         -2.4631e-02, -9.5709e-01,  6.3591e-01,  2.5234e-01,  5.0451e-01,\n",
            "         -6.8577e-01,  1.8943e-01,  1.3230e+00, -7.3024e-01, -5.4440e-01,\n",
            "          3.9989e-01, -2.1747e+00,  2.4936e-01,  1.3540e-01, -3.0465e+00,\n",
            "         -1.3441e+00, -2.8268e-01, -6.8872e-01, -6.8965e-01,  5.8992e-01,\n",
            "          5.5318e-01,  6.5104e-02, -1.7956e+00,  1.3145e+00,  1.7042e+00,\n",
            "          5.2542e-01, -1.2803e+00, -1.1621e+00,  6.6521e-01,  2.9128e-02,\n",
            "          3.6271e+00, -1.3567e-01, -4.6482e-01, -1.4324e+00,  1.2539e-01,\n",
            "         -1.1245e+00,  4.8809e-01, -6.8958e-01, -7.0803e-01, -3.1519e-01,\n",
            "          7.1964e-01, -1.7802e-02, -1.2635e+00,  8.9144e-01, -1.2858e+00,\n",
            "         -2.1067e+00, -1.9922e+00,  7.6294e-01, -5.9484e-01,  9.8280e-01,\n",
            "         -4.1515e-01, -2.0259e-01, -1.8955e+00,  6.1170e-01,  1.0955e-01,\n",
            "          1.5681e-02, -1.0636e+00,  8.3979e-01,  4.2112e-01, -2.0257e+00,\n",
            "          1.0383e+00,  5.1819e-01,  5.2830e-01, -5.6484e-01,  3.8275e-02,\n",
            "          3.0485e-01, -2.0662e+00, -1.1418e+00, -1.3911e-01,  1.0827e+00,\n",
            "          1.1522e+00,  5.1984e-01, -8.9818e-01,  3.7491e-01, -4.2227e-02,\n",
            "          7.1970e-01,  1.8447e+00,  1.4385e+00, -1.3166e+00,  1.2690e+00,\n",
            "         -2.3594e-01,  4.0538e-01, -3.3332e-01, -1.9416e+00, -1.4176e+00,\n",
            "         -6.0523e-03, -1.3411e+00, -1.0705e+00,  1.3173e+00,  3.7193e-01,\n",
            "          5.1977e-01,  9.8285e-01,  8.0482e-01,  2.3235e+00, -1.5808e+00,\n",
            "         -1.7794e+00,  4.0939e-01,  9.2944e-02,  4.9701e-01,  5.8485e-02,\n",
            "          1.0326e-01,  7.2012e-02,  1.1080e+00,  7.2926e-01,  3.6650e-01,\n",
            "         -9.2689e-01, -1.2297e+00,  1.0633e-01,  5.8153e-02,  5.3531e-01,\n",
            "          1.1746e+00,  8.5400e-02,  1.1270e-02, -4.3248e-01, -2.6942e-01,\n",
            "         -2.5596e+00, -1.7529e-01, -9.7407e-01,  1.2510e-01,  1.6380e+00,\n",
            "         -5.7628e-01, -1.4705e+00, -2.6125e-01, -5.3680e-01, -7.8720e-01,\n",
            "          1.4845e-02,  4.6483e-01,  6.1097e-01, -1.5670e+00, -7.8682e-01,\n",
            "          3.9667e-01, -9.7545e-01,  5.1219e-01,  3.3299e-01,  1.0995e+00,\n",
            "          4.0335e-01, -1.1089e+00, -5.3754e-02, -1.2741e+00, -7.1055e-01,\n",
            "          7.4904e-01,  6.7999e-01, -2.7101e-01,  2.9964e-02,  2.4001e-01,\n",
            "         -2.0333e+00, -5.5379e-01,  1.6957e-01,  6.9847e-01,  3.7444e-01,\n",
            "         -1.4854e+00,  6.5020e-01, -2.0446e-01, -1.8770e-01,  4.4862e-01,\n",
            "          5.5716e-01, -5.4564e-01,  1.6367e-01, -1.8262e+00,  6.8537e-01,\n",
            "         -8.7624e-01, -1.4931e+00,  1.6634e+00, -4.7180e-01,  5.8567e-01,\n",
            "         -9.5790e-01,  9.4345e-01, -2.1992e+00,  4.1636e-01, -3.5643e-01,\n",
            "         -1.0305e+00, -1.0478e+00,  2.1709e+00, -1.0414e+00, -1.3088e+00,\n",
            "          1.0169e+00,  4.7949e-01, -1.8578e+00,  2.3297e-01, -2.2054e-01,\n",
            "         -1.4869e+00, -6.2718e-01,  2.0144e+00, -6.9383e-01,  1.6325e+00,\n",
            "         -7.5949e-01,  5.8486e-01,  1.0116e+00, -3.8033e-01, -4.3962e-01,\n",
            "          1.4687e-01,  1.2890e+00, -2.0776e-02,  6.9753e-02, -7.2962e-01,\n",
            "          1.6526e-01, -3.3901e-01,  1.5416e+00,  1.0231e+00,  1.3392e+00,\n",
            "          1.6886e+00,  7.0994e-01,  1.3693e+00, -7.0759e-01,  1.5387e+00,\n",
            "         -9.5591e-01,  2.0515e+00,  9.6234e-03]], grad_fn=<SliceBackward0>)\n",
            "probs:tensor([[8.6742e-04, 6.7503e-04, 5.0529e-04, 2.8990e-04, 1.3536e-03, 7.4268e-04,\n",
            "         1.8804e-03, 7.7209e-04, 1.0389e-03, 2.3277e-03, 1.8770e-04, 4.3466e-04,\n",
            "         9.1663e-04, 5.6962e-04, 2.8819e-04, 3.3882e-03, 2.7894e-03, 6.2961e-04,\n",
            "         9.6348e-04, 1.9005e-03, 9.4406e-05, 1.1855e-03, 3.2027e-03, 1.3074e-03,\n",
            "         8.2121e-04, 1.5172e-04, 2.2693e-04, 5.1797e-04, 1.1329e-03, 3.2477e-04,\n",
            "         3.3221e-03, 8.8958e-03, 3.7303e-04, 5.6311e-04, 1.9880e-03, 8.1754e-04,\n",
            "         8.4823e-04, 2.2502e-03, 2.2834e-04, 5.3719e-04, 4.3581e-04, 2.8739e-04,\n",
            "         1.2507e-03, 1.6239e-04, 2.1681e-04, 1.2825e-03, 3.9838e-04, 3.6179e-04,\n",
            "         3.7529e-03, 3.2433e-04, 2.7966e-03, 5.4940e-04, 1.5980e-04, 5.9403e-03,\n",
            "         1.1473e-02, 1.2624e-04, 3.0913e-03, 1.5989e-04, 1.6457e-03, 5.8594e-04,\n",
            "         1.5776e-03, 3.3544e-03, 3.6209e-03, 4.8373e-04, 3.1428e-04, 1.3163e-03,\n",
            "         6.8769e-04, 6.7871e-04, 4.4043e-04, 1.1534e-03, 5.5975e-04, 2.4901e-04,\n",
            "         5.3975e-03, 4.2317e-04, 9.0465e-04, 1.4536e-03, 1.7383e-04, 1.7913e-03,\n",
            "         8.3660e-04, 9.0937e-04, 8.7317e-03, 2.1295e-04, 1.9891e-03, 1.2624e-03,\n",
            "         1.4712e-04, 2.0318e-04, 1.4439e-03, 5.9503e-04, 1.0222e-03, 5.1433e-04,\n",
            "         1.1652e-03, 3.3644e-04, 4.7618e-04, 4.6767e-04, 2.6601e-04, 4.8074e-04,\n",
            "         1.3670e-04, 1.8486e-04, 6.1353e-04, 1.8951e-03, 7.4717e-04, 3.4476e-04,\n",
            "         5.3752e-04, 7.3650e-04, 6.0639e-04, 6.3354e-04, 9.7137e-04, 2.8920e-03,\n",
            "         8.1702e-04, 9.1961e-03, 3.8157e-04, 1.0056e-04, 5.2070e-04, 7.2975e-04,\n",
            "         1.8280e-03, 1.0997e-04, 8.5552e-04, 1.1452e-03, 1.2378e-04, 1.3008e-03,\n",
            "         4.1705e-03, 9.5860e-04, 9.8803e-04, 3.7653e-04, 3.7507e-04, 9.9545e-04,\n",
            "         4.1787e-04, 1.6731e-04, 9.2689e-05, 4.5017e-03, 2.6658e-03, 4.6156e-04,\n",
            "         2.7846e-03, 3.9274e-03, 6.3926e-04, 1.3461e-04, 7.0496e-04, 7.7961e-04,\n",
            "         2.0724e-03, 1.4260e-03, 9.8378e-04, 3.4293e-04, 1.5227e-03, 1.7588e-03,\n",
            "         7.1309e-03, 1.8929e-03, 1.5681e-04, 5.4106e-04, 6.4600e-04, 5.2905e-04,\n",
            "         3.8585e-04, 2.2603e-03, 2.6867e-04, 8.5812e-04, 2.4642e-03, 5.7261e-04,\n",
            "         2.5154e-04, 3.7634e-04, 3.5533e-03, 3.5970e-04, 2.9551e-04, 7.7351e-04,\n",
            "         6.8435e-04, 7.5249e-03, 4.7528e-05, 1.2052e-03, 3.2063e-04, 5.6607e-04,\n",
            "         7.2724e-04, 5.6093e-03, 6.1844e-04, 5.8177e-04, 1.8732e-04, 6.8367e-04,\n",
            "         1.1338e-04, 1.8125e-04, 5.1253e-04, 2.2639e-04, 8.3650e-04, 8.5498e-04,\n",
            "         1.5337e-03, 1.8043e-03, 1.2864e-04, 2.6712e-03, 1.8895e-03, 2.0896e-03,\n",
            "         1.3591e-03, 1.9995e-04, 3.6404e-04, 6.1424e-03, 1.2073e-03, 2.4500e-03,\n",
            "         8.7632e-04, 5.1401e-04, 4.3601e-03, 2.9109e-03, 2.1287e-03, 3.9143e-04,\n",
            "         4.5755e-04, 1.2770e-03, 7.3733e-04, 1.3754e-04, 2.2120e-03, 1.2173e-03,\n",
            "         2.0902e-04, 2.7670e-04, 6.6498e-04, 8.1508e-04, 9.7573e-04, 3.5016e-04,\n",
            "         5.3000e-04, 4.5884e-04, 1.3786e-03, 1.3288e-03, 2.5009e-03, 1.5060e-03,\n",
            "         1.1986e-03, 1.7304e-03, 5.5037e-04, 3.4304e-04, 4.0404e-04, 1.0480e-03,\n",
            "         4.1535e-04, 4.8613e-04, 4.0459e-04, 5.8051e-04, 7.3384e-04, 5.3326e-04,\n",
            "         7.0230e-04, 1.6464e-03, 7.2425e-04, 4.6282e-04, 1.6444e-03, 1.5870e-04,\n",
            "         1.3403e-03, 1.1216e-04, 2.7233e-04, 7.7122e-04, 4.5940e-04, 4.7819e-04,\n",
            "         3.2404e-03, 6.9568e-04, 3.2365e-04, 2.2640e-04, 1.1098e-03, 5.4611e-04,\n",
            "         7.1478e-04, 3.0092e-04, 5.2441e-04, 8.6935e-04, 2.8556e-04, 2.0768e-04,\n",
            "         2.2170e-03, 1.0743e-04, 4.2717e-04, 2.1333e-03, 1.1424e-03, 2.4301e-04,\n",
            "         3.1863e-04, 4.6664e-03, 1.6353e-03, 6.7889e-04, 3.0061e-03, 9.4056e-04,\n",
            "         1.1352e-04, 8.8590e-04, 2.2276e-04, 6.5273e-04, 1.2172e-04, 3.1495e-04,\n",
            "         4.6878e-04, 2.0783e-04, 5.4282e-04, 1.7469e-03, 3.5276e-04, 8.6198e-04,\n",
            "         1.5357e-03, 6.7985e-04, 3.5553e-04, 1.9309e-03, 3.5084e-04, 1.6138e-04,\n",
            "         4.2518e-05, 4.2843e-05, 6.0860e-04, 7.6199e-04, 3.7507e-04, 5.5249e-05,\n",
            "         7.3934e-04, 1.9798e-03, 2.0760e-04, 9.2416e-04, 3.8225e-04, 5.2644e-04,\n",
            "         1.9846e-04, 2.4768e-04, 9.1029e-04, 2.9432e-04, 1.4027e-03, 1.2078e-03,\n",
            "         1.4236e-03, 1.8509e-04, 1.2531e-03, 7.9175e-04, 1.0351e-03, 1.3874e-04,\n",
            "         3.3913e-04, 7.7610e-04, 1.9556e-03, 5.4604e-04, 2.1942e-03, 9.9148e-05,\n",
            "         1.8004e-04, 2.5867e-03, 2.2398e-04, 1.2956e-03, 1.9370e-04, 1.5886e-03,\n",
            "         2.2921e-04, 2.6918e-03, 6.5382e-03, 5.8132e-04, 1.2457e-03, 9.6189e-03,\n",
            "         4.5305e-04, 6.3333e-04, 1.0692e-03, 5.4259e-04, 1.6728e-04, 7.3135e-04,\n",
            "         5.3609e-04, 1.5012e-04, 7.3477e-04, 6.9231e-04, 4.2330e-04, 4.2943e-04,\n",
            "         5.8210e-04, 8.3428e-05, 1.5896e-03, 1.4367e-03, 2.0585e-04, 1.3316e-03,\n",
            "         9.2726e-05, 4.6484e-04, 3.7797e-04, 3.6423e-04, 9.3588e-04, 2.0395e-04,\n",
            "         9.4316e-04, 3.8003e-04, 2.1390e-03, 3.2548e-04, 9.6971e-04, 2.6948e-03,\n",
            "         2.5541e-03, 5.0994e-04, 9.1719e-05, 2.0844e-03, 2.5152e-04, 1.9505e-03,\n",
            "         6.6850e-04, 2.1214e-03, 9.1862e-04, 1.2875e-03, 3.8401e-03, 1.3160e-03,\n",
            "         1.1118e-04, 2.6328e-03, 4.9742e-04, 1.0890e-04, 1.2620e-03, 1.7052e-03,\n",
            "         3.1066e-04, 1.1954e-03, 2.7564e-04, 3.5048e-04, 7.9931e-04, 1.3097e-03,\n",
            "         6.9405e-04, 2.7814e-04, 3.0630e-03, 1.1175e-03, 4.7131e-04, 1.0446e-03,\n",
            "         8.2241e-04, 6.8454e-04, 1.6629e-03, 4.1721e-04, 2.0642e-03, 3.6534e-03,\n",
            "         1.0975e-03, 1.0133e-03, 4.6105e-04, 3.5964e-04, 1.8180e-03, 2.6731e-04,\n",
            "         2.6464e-03, 7.6815e-05, 1.2265e-03, 6.1786e-04, 7.9477e-04, 7.7626e-04,\n",
            "         2.2992e-04, 2.5553e-04, 5.5976e-04, 3.5159e-04, 8.5328e-04, 1.8601e-04,\n",
            "         1.8949e-03, 3.3718e-04, 1.2390e-04, 1.4411e-03, 5.7837e-04, 9.2659e-04,\n",
            "         8.6221e-04, 1.2230e-03, 9.8616e-04, 2.3236e-03, 8.1664e-05, 2.5533e-04,\n",
            "         2.0091e-03, 1.7906e-02, 1.6816e-05, 4.2485e-04, 1.4050e-03, 1.5036e-04,\n",
            "         4.1264e-04, 5.3825e-04, 1.2564e-03, 2.1027e-04, 1.8550e-03, 1.5494e-03,\n",
            "         1.8293e-04, 2.4444e-03, 7.4362e-04, 8.1007e-05, 2.2403e-04, 8.9805e-03,\n",
            "         3.6625e-03, 1.1807e-04, 5.8339e-03, 8.4409e-04, 8.1102e-04, 5.9401e-04,\n",
            "         3.8156e-03, 1.3393e-03, 1.4238e-03, 1.3496e-03, 7.9556e-04, 5.2804e-04,\n",
            "         1.5872e-03, 3.0333e-04, 1.3869e-04, 2.9982e-04, 1.6905e-04, 9.2128e-04,\n",
            "         1.4318e-04, 1.1340e-03, 1.1948e-03, 3.3431e-04, 2.4596e-04, 7.2651e-04,\n",
            "         7.2447e-04, 7.8406e-04, 8.8368e-04, 1.5097e-04, 8.7203e-05, 2.2217e-04,\n",
            "         7.8039e-04, 1.3937e-03, 4.3886e-04, 5.8325e-04, 7.4500e-04, 6.6984e-04,\n",
            "         7.2883e-04, 2.1424e-04, 2.2816e-03, 1.5009e-04, 2.8998e-03, 1.4950e-03,\n",
            "         1.3805e-03, 5.1979e-04, 2.5615e-04, 1.8007e-03, 2.6521e-03, 1.2612e-03,\n",
            "         4.5461e-04, 4.1692e-04, 2.3856e-03, 3.7262e-04, 7.1740e-04, 2.2443e-04,\n",
            "         1.1736e-03, 1.0279e-03, 4.0761e-04, 2.5347e-03, 3.6895e-04, 1.0769e-03,\n",
            "         8.7798e-04, 8.2260e-05, 3.8573e-03, 6.9394e-04, 6.4366e-04, 2.0785e-03,\n",
            "         7.1270e-04, 9.2757e-04, 1.2027e-03, 2.9350e-04, 4.3241e-03, 7.9172e-04,\n",
            "         4.9770e-04, 4.4881e-04, 3.9467e-04, 4.5256e-03, 9.7052e-04, 7.5626e-04,\n",
            "         6.1352e-03, 3.6587e-04, 1.4578e-04, 6.3341e-04, 2.2652e-03, 1.7691e-03,\n",
            "         6.5678e-05, 1.8920e-04, 2.6571e-04, 9.9265e-05, 6.4104e-04, 3.2291e-04,\n",
            "         1.8267e-03, 1.6458e-04, 8.2374e-04, 1.3149e-04, 5.5291e-03, 4.3638e-04,\n",
            "         6.5403e-04, 5.3549e-04, 2.5271e-03, 5.8468e-04, 2.1818e-04, 3.1050e-04,\n",
            "         5.1782e-04, 1.9760e-03, 1.7205e-03, 8.5709e-04, 5.7221e-04, 5.5899e-04,\n",
            "         7.3353e-04, 1.7313e-03, 1.7990e-03, 6.0022e-04, 1.4917e-03, 3.1621e-03,\n",
            "         4.8296e-04, 2.1218e-04, 1.0153e-03, 1.0420e-03, 1.6968e-04, 4.9440e-04,\n",
            "         1.4904e-03, 1.0234e-03, 1.9615e-03, 1.0351e-03, 8.1522e-04, 6.5100e-04,\n",
            "         9.7628e-04, 8.1616e-04, 2.0882e-04, 4.3183e-03, 1.8150e-03, 9.1359e-04,\n",
            "         1.1458e-03, 9.3481e-04, 5.0802e-04, 1.4134e-03, 1.5380e-03, 4.2361e-04,\n",
            "         2.5905e-04, 1.2358e-03, 3.2666e-04, 1.1677e-03, 8.4854e-04, 7.1818e-05,\n",
            "         9.9966e-04, 3.3877e-03, 2.5610e-04, 2.7504e-04, 3.6093e-04, 1.1565e-03,\n",
            "         1.0362e-03, 3.0111e-04, 1.0037e-03, 4.1078e-04, 7.5635e-04, 4.6373e-04,\n",
            "         7.0488e-04, 4.3473e-03, 9.1293e-05, 1.1061e-04, 3.1646e-04, 9.9274e-04,\n",
            "         2.4252e-03, 9.5063e-04, 7.4514e-04, 1.8573e-03, 1.5186e-04, 2.1129e-03,\n",
            "         1.1818e-03, 4.4460e-04, 2.5009e-03, 8.8194e-05, 1.8345e-03, 1.5161e-04,\n",
            "         1.0242e-03, 2.8759e-04, 7.9796e-03, 3.6492e-04, 9.6958e-03, 1.8452e-03,\n",
            "         4.6760e-04, 6.5447e-04, 7.2737e-04, 6.7125e-04, 7.5770e-05, 7.2820e-04,\n",
            "         7.0358e-04, 1.1631e-03, 1.0322e-03, 9.9162e-04, 7.8486e-04, 9.9510e-03,\n",
            "         2.0080e-04, 4.6048e-04, 5.4872e-04, 1.9803e-04, 6.3540e-04, 7.0636e-04,\n",
            "         2.7801e-04, 1.3674e-03, 9.3178e-04, 1.1990e-03, 3.6466e-04, 8.7496e-04,\n",
            "         2.7182e-03, 3.4880e-04, 4.2004e-04, 1.0799e-03, 8.2272e-05, 9.2900e-04,\n",
            "         8.2894e-04, 3.4405e-05, 1.8879e-04, 5.4570e-04, 3.6359e-04, 3.6325e-04,\n",
            "         1.3059e-03, 1.2588e-03, 7.7267e-04, 1.2020e-04, 2.6952e-03, 3.9796e-03,\n",
            "         1.2244e-03, 2.0123e-04, 2.2647e-04, 1.4080e-03, 7.4537e-04, 2.7224e-02,\n",
            "         6.3212e-04, 4.5483e-04, 1.7284e-04, 8.2068e-04, 2.3515e-04, 1.1795e-03,\n",
            "         3.6328e-04, 3.5664e-04, 5.2824e-04, 1.4868e-03, 7.1120e-04, 2.0465e-04,\n",
            "         1.7655e-03, 2.0012e-04, 8.8060e-05, 9.8741e-05, 1.5526e-03, 3.9938e-04,\n",
            "         1.9344e-03, 4.7799e-04, 5.9120e-04, 1.0877e-04, 1.3347e-03, 8.0779e-04,\n",
            "         7.3541e-04, 2.4992e-04, 1.6766e-03, 1.1031e-03, 9.5492e-05, 2.0447e-03,\n",
            "         1.2155e-03, 1.2279e-03, 4.1154e-04, 7.5222e-04, 9.8201e-04, 9.1705e-05,\n",
            "         2.3111e-04, 6.2995e-04, 2.1377e-03, 2.2915e-03, 1.2175e-03, 2.9488e-04,\n",
            "         1.0533e-03, 6.9403e-04, 1.4869e-03, 4.5802e-03, 3.0512e-03, 1.9405e-04,\n",
            "         2.5755e-03, 5.7181e-04, 1.0859e-03, 5.1875e-04, 1.0387e-04, 1.7541e-04,\n",
            "         7.1960e-04, 1.8936e-04, 2.4822e-04, 2.7027e-03, 1.0501e-03, 1.2175e-03,\n",
            "         1.9345e-03, 1.6190e-03, 7.3929e-03, 1.4901e-04, 1.2217e-04, 1.0902e-03,\n",
            "         7.9448e-04, 1.1901e-03, 7.6757e-04, 8.0272e-04, 7.7803e-04, 2.1923e-03,\n",
            "         1.5012e-03, 1.0445e-03, 2.8653e-04, 2.1168e-04, 8.0519e-04, 7.6732e-04,\n",
            "         1.2365e-03, 2.3433e-03, 7.8851e-04, 7.3217e-04, 4.6978e-04, 5.5298e-04,\n",
            "         5.5990e-05, 6.0757e-04, 2.7333e-04, 8.2044e-04, 3.7249e-03, 4.0686e-04,\n",
            "         1.6638e-04, 5.5752e-04, 4.2324e-04, 3.2949e-04, 7.3480e-04, 1.1524e-03,\n",
            "         1.3337e-03, 1.5108e-04, 3.2962e-04, 1.0764e-03, 2.7295e-04, 1.2083e-03,\n",
            "         1.0100e-03, 2.1738e-03, 1.0837e-03, 2.3885e-04, 6.8608e-04, 2.0248e-04,\n",
            "         3.5574e-04, 1.5312e-03, 1.4290e-03, 5.5210e-04, 7.4599e-04, 9.2036e-04,\n",
            "         9.4772e-05, 4.1611e-04, 8.5776e-04, 1.4557e-03, 1.0528e-03, 1.6391e-04,\n",
            "         1.3871e-03, 5.9010e-04, 6.0007e-04, 1.1338e-03, 1.2638e-03, 4.1952e-04,\n",
            "         8.5271e-04, 1.1658e-04, 1.4367e-03, 3.0142e-04, 1.6265e-04, 3.8204e-03,\n",
            "         4.5167e-04, 1.3004e-03, 2.7779e-04, 1.8598e-03, 8.0280e-05, 1.0978e-03,\n",
            "         5.0691e-04, 2.5833e-04, 2.5390e-04, 6.3463e-03, 2.5554e-04, 1.9558e-04,\n",
            "         2.0015e-03, 1.1694e-03, 1.1295e-04, 9.1390e-04, 5.8069e-04, 1.6367e-04,\n",
            "         3.8667e-04, 5.4268e-03, 3.6174e-04, 3.7044e-03, 3.3875e-04, 1.2993e-03,\n",
            "         1.9910e-03, 4.9493e-04, 4.6644e-04, 8.3851e-04, 2.6274e-03, 7.0908e-04,\n",
            "         7.7627e-04, 3.4902e-04, 8.5406e-04, 5.1581e-04, 3.3826e-03, 2.0140e-03,\n",
            "         2.7628e-03, 3.9180e-03, 1.4725e-03, 2.8471e-03, 3.5680e-04, 3.3726e-03,\n",
            "         2.7834e-04, 5.6320e-03, 7.3097e-04]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next:tensor([[359]])\n",
            "idx:tensor([[  0, 359]]),logits:tensor([[[ 0.1808, -0.0700, -0.3596,  ..., -0.9559,  2.0515,  0.0096],\n",
            "         [ 0.5750,  0.9197, -0.1923,  ..., -2.0579,  0.9896,  1.8094]]],\n",
            "       grad_fn=<EmbeddingBackward0>),loss:None\n",
            "logits[:, -1, :]:tensor([[ 5.7500e-01,  9.1973e-01, -1.9226e-01,  1.2654e+00,  3.0312e-01,\n",
            "          9.6976e-01, -2.0059e-01,  1.7848e-01,  1.2678e+00, -6.9380e-01,\n",
            "          9.1445e-01, -4.3069e-01,  6.6417e-02, -4.2039e-01,  1.0447e+00,\n",
            "         -2.7024e-02,  1.6704e+00, -8.1013e-01, -1.1403e+00, -5.4322e-01,\n",
            "          3.4235e-01, -2.0067e-01,  1.3743e+00,  5.8977e-01, -2.5024e-01,\n",
            "         -8.8304e-01,  9.3981e-01, -1.4239e+00,  1.2945e+00, -1.0707e-01,\n",
            "         -6.7441e-01,  5.9668e-01, -8.2972e-02,  7.1992e-01,  3.0594e-01,\n",
            "          8.2205e-01, -9.6733e-02, -1.7893e+00, -2.3377e-01, -3.3773e-02,\n",
            "          9.4602e-01,  7.6547e-01,  3.6938e-01, -7.0056e-01, -4.8128e-01,\n",
            "         -1.3087e-01,  2.4773e+00, -9.3375e-02, -5.9108e-01,  9.9807e-01,\n",
            "          7.0592e-01, -6.7862e-01,  1.3587e+00, -1.3955e+00, -9.6180e-01,\n",
            "         -4.3638e-01, -1.3281e+00, -1.7968e-02, -3.5152e-01, -5.6192e-01,\n",
            "         -2.8927e-01, -3.9193e-01,  5.2398e-01,  1.9187e-01, -7.7369e-01,\n",
            "         -3.4846e+00,  2.8444e-01, -1.4652e-01, -9.3205e-01,  2.0341e+00,\n",
            "          1.7089e-01,  3.7188e-01, -3.7629e-01,  1.0011e+00, -2.1116e-01,\n",
            "         -1.3832e+00, -1.2431e+00, -8.4668e-01,  8.8737e-01,  1.9861e+00,\n",
            "         -2.7500e-01, -6.9604e-01,  1.9131e+00,  8.1918e-01, -8.7563e-01,\n",
            "         -3.2449e-01,  6.1156e-01, -3.9929e-01,  3.4813e-01, -1.3625e+00,\n",
            "          6.6231e-01, -3.4381e-01,  3.2519e-01,  2.0886e-01, -1.1147e+00,\n",
            "         -5.8999e-01, -5.8135e-01,  4.8569e-01,  2.7350e-01,  3.9207e-02,\n",
            "          5.9698e-01,  2.3619e-01,  1.0278e-01,  1.4541e-01,  5.0385e-01,\n",
            "          9.9176e-01, -9.6687e-01, -6.7852e-01, -1.9362e+00,  1.3619e+00,\n",
            "          1.6434e-01,  1.2648e+00, -1.9098e+00, -2.4803e-01, -1.3246e+00,\n",
            "          5.8399e-01,  5.7813e-01, -3.1691e-01,  1.4216e-01, -4.3731e-01,\n",
            "          8.2262e-01, -8.1879e-01,  7.4809e-02,  4.9513e-01,  8.2219e-01,\n",
            "          9.3265e-01, -1.1942e+00, -1.8047e-01,  6.4856e-01, -1.5267e+00,\n",
            "          1.6330e+00, -8.1008e-02, -3.3469e-02,  1.6752e+00,  3.9219e-02,\n",
            "          1.1898e+00,  1.4346e-01,  4.8211e-01, -1.9310e+00, -5.2173e-02,\n",
            "          1.1288e+00, -3.5983e-01,  7.6919e-01,  5.3787e-01, -7.6968e-01,\n",
            "         -1.7792e+00,  8.0789e-01, -3.4068e-01, -4.3183e-01,  1.8211e+00,\n",
            "          2.0410e-01,  6.6775e-01,  2.8367e-01, -3.3394e-01, -1.6687e+00,\n",
            "          1.0618e+00, -8.3494e-01,  1.2097e+00,  1.8501e+00,  1.1506e+00,\n",
            "         -1.5846e-01,  3.2349e-02,  4.8333e-01,  1.2128e+00,  8.7706e-01,\n",
            "          3.6304e-01,  3.6773e-01,  1.4109e+00,  1.5531e+00, -1.5218e+00,\n",
            "         -1.0215e-01,  7.5243e-01,  6.4376e-01,  4.7324e-01, -9.6702e-01,\n",
            "         -4.1522e-01, -6.0650e-01, -8.1512e-01,  1.0255e+00,  2.3260e+00,\n",
            "         -9.1430e-01, -1.0792e+00,  1.1721e+00,  2.3333e-01,  1.6695e+00,\n",
            "          3.5342e-01, -9.5619e-01, -2.0035e-01, -7.7746e-01, -2.6499e+00,\n",
            "         -1.2596e+00,  4.3251e-01, -1.2941e+00, -7.3001e-01, -1.5031e+00,\n",
            "          9.6644e-02, -1.2664e+00,  5.7118e-01, -6.5496e-01,  1.2456e+00,\n",
            "          2.2393e-01,  1.8502e+00, -1.4691e+00,  2.9735e-01,  5.8869e-01,\n",
            "         -2.2392e-01, -7.7401e-01,  8.5484e-01,  1.7347e+00,  5.4757e-01,\n",
            "         -1.4275e+00,  3.5245e-02, -1.3406e+00, -3.4479e-01,  1.3105e+00,\n",
            "         -1.3727e+00,  4.1041e-01,  5.1053e-01,  6.8411e-01,  1.1682e+00,\n",
            "         -6.1058e-01,  8.3737e-01,  6.9618e-01,  6.2242e-01, -1.6571e-01,\n",
            "         -9.8961e-01,  7.2132e-01, -8.3273e-01, -1.8494e+00,  1.7906e-01,\n",
            "         -9.8122e-01,  1.6053e-02,  4.8035e-01,  1.0466e+00,  2.5090e+00,\n",
            "          7.4634e-01, -7.1176e-01, -6.9261e-01,  1.6485e+00, -7.7439e-01,\n",
            "          1.0968e+00,  1.2322e+00,  8.3681e-01, -3.5850e-01, -1.4683e+00,\n",
            "         -1.5421e+00, -7.1180e-01,  1.2557e+00, -6.7964e-01, -1.2004e+00,\n",
            "         -1.5530e+00,  1.1122e+00,  7.9480e-02, -2.5306e-01, -7.9241e-01,\n",
            "         -4.0315e-01,  1.1210e+00,  1.8314e-01,  9.3811e-02,  7.4903e-01,\n",
            "         -4.9782e-01,  3.3380e-02,  1.5806e+00, -1.2092e+00,  2.9146e+00,\n",
            "          5.2208e-01,  2.9538e-01, -7.5263e-01,  1.9270e-01,  5.1569e-01,\n",
            "          5.3657e-01, -5.9124e-01, -3.9297e-01,  4.8231e-01, -6.2833e-02,\n",
            "         -1.6669e+00, -4.4759e-01,  3.1531e-01,  6.3727e-01, -1.6218e+00,\n",
            "          5.0721e-01, -1.6198e+00, -1.1050e+00, -5.0568e-01, -5.4697e-01,\n",
            "          3.7690e-01, -1.5910e+00, -1.0721e+00, -1.1500e+00,  1.4455e+00,\n",
            "          2.7816e+00,  2.4358e-01, -8.5713e-01,  4.0632e-01, -5.0542e-01,\n",
            "         -7.3691e-03,  3.3723e-01,  1.0962e+00,  2.1258e-01,  5.6786e-01,\n",
            "         -1.8771e-01, -3.3723e-01,  1.0425e-01, -1.1945e+00,  7.4323e-01,\n",
            "          5.2632e-01, -4.8755e-01, -1.3173e-01,  1.2747e+00,  1.8610e-01,\n",
            "         -1.4671e+00,  1.6081e+00,  4.2829e-01, -5.9838e-01,  1.7704e-02,\n",
            "          5.8287e-01,  8.5180e-01,  1.8824e+00, -5.4717e-01, -1.8566e+00,\n",
            "          7.6794e-01, -1.7830e-01,  1.2421e-01,  1.2276e-01,  5.9737e-01,\n",
            "          6.4851e-01,  1.1140e+00,  1.3006e+00,  4.7762e-01,  3.1884e-01,\n",
            "         -2.1620e+00,  1.4182e+00,  1.7790e+00, -8.3879e-01, -1.1167e+00,\n",
            "         -1.5473e+00,  2.9333e+00, -1.6762e+00,  4.2521e-01,  5.8937e-01,\n",
            "         -3.4792e+00,  1.4399e-01,  1.0333e+00, -1.5916e+00, -8.8920e-02,\n",
            "          1.9046e-01, -6.4809e-01, -9.4619e-01, -3.5301e-01, -4.5785e-01,\n",
            "          1.1094e+00,  5.3895e-02,  4.6634e-01, -8.0778e-01,  6.5927e-02,\n",
            "         -6.3351e-01, -1.3145e+00,  1.4567e+00,  1.3228e-02,  4.2569e-01,\n",
            "         -2.1527e+00, -1.6711e-02,  8.4040e-01, -5.2759e-01, -2.2886e-01,\n",
            "          1.1712e+00, -9.2819e-01,  1.8967e-01, -1.0197e+00, -8.7949e-01,\n",
            "         -1.0422e+00, -4.5547e-01, -3.2455e-01,  3.1760e-01,  9.9913e-01,\n",
            "         -1.3148e+00, -8.6559e-02,  1.4960e-01,  1.1326e+00,  8.8832e-01,\n",
            "         -1.3284e-02,  3.0237e-01, -1.7453e+00,  5.9475e-01,  1.0446e+00,\n",
            "          3.8475e-01, -4.0411e-01, -4.1433e-03, -1.0396e-01, -4.7479e-01,\n",
            "         -3.9097e-01,  1.1997e-01, -3.2090e-01,  3.9951e-01, -3.5106e-01,\n",
            "         -4.5927e-01, -1.2258e+00, -1.4867e-01,  1.2132e+00, -7.3984e-01,\n",
            "         -1.0421e+00,  1.4979e-01,  3.2363e-01,  2.4453e-01,  4.7391e-01,\n",
            "          2.7607e-01, -6.2252e-01, -7.6070e-01,  9.8190e-01,  5.8022e-01,\n",
            "          2.9468e-01, -5.0181e-03,  4.0782e-01, -8.2699e-01,  9.7709e-01,\n",
            "         -7.7534e-01,  1.1238e+00,  9.3641e-01,  2.5879e+00,  1.1380e+00,\n",
            "         -3.4994e-02,  9.3123e-01,  5.7106e-01, -6.8650e-02,  6.0746e-02,\n",
            "         -1.2740e-01, -2.2041e+00, -8.4259e-01, -2.0869e+00, -1.0956e+00,\n",
            "          3.0396e-01, -1.4619e+00, -1.4788e+00,  1.0076e+00,  2.3223e+00,\n",
            "         -5.6824e-01, -5.5882e-01,  1.5693e+00, -2.8098e-01,  3.4244e-02,\n",
            "          8.2960e-01, -6.1559e-01,  4.9820e-01, -7.4281e-01,  2.3889e-01,\n",
            "          1.0741e+00, -8.5483e-01,  1.2908e-01,  7.9278e-01,  4.8311e-01,\n",
            "          1.3278e+00,  9.4222e-02, -9.4054e-02,  1.1377e+00,  1.1212e+00,\n",
            "         -2.3593e+00,  1.2556e+00,  7.0056e-01,  5.9227e-01, -9.0085e-01,\n",
            "          1.4936e-01, -6.1301e-01, -1.0381e+00,  1.0471e+00,  4.3294e-01,\n",
            "          2.3441e+00,  1.7838e+00,  7.5966e-01, -1.5749e-01, -1.2294e+00,\n",
            "         -1.8962e-01, -1.0959e+00,  1.5605e-01, -4.6495e-01, -5.7380e-01,\n",
            "         -8.4659e-01, -8.0066e-01,  2.4975e+00, -8.3756e-01,  1.7596e+00,\n",
            "          1.7015e+00,  8.8696e-02,  2.2222e+00, -4.8732e-01, -3.1382e-01,\n",
            "          4.4689e-01,  6.3214e-01, -1.0114e+00, -5.3739e-01, -5.6862e-03,\n",
            "         -3.1971e-01, -1.4209e+00, -1.1629e-01, -2.3041e+00, -6.6322e-01,\n",
            "         -1.1784e+00,  5.5191e-01,  7.9020e-03,  1.0501e+00, -1.5493e-02,\n",
            "         -1.3605e+00,  5.8907e-01, -7.5266e-01,  9.9462e-01, -1.5962e+00,\n",
            "          8.1548e-01, -1.2518e-01,  1.2949e+00,  9.6758e-01, -5.2140e-01,\n",
            "         -7.2713e-01,  1.5171e+00, -4.4601e-01,  3.0907e-01, -1.7211e-01,\n",
            "         -1.2587e+00, -1.3815e+00,  8.8477e-01, -1.5461e-01,  4.2238e-01,\n",
            "          2.1514e+00, -1.3231e+00,  1.1856e+00,  3.8526e-02,  9.3651e-02,\n",
            "          6.3085e-01,  1.1889e+00, -1.5727e+00, -8.4225e-01,  5.3469e-01,\n",
            "         -1.2844e+00,  1.0346e+00,  8.1680e-01,  2.6839e-01, -5.8794e-01,\n",
            "          1.7326e-01, -2.7755e-01,  1.4151e+00,  1.4439e+00,  3.8935e-02,\n",
            "         -5.7169e-01,  1.6070e-01, -9.2626e-01, -3.0780e-02,  1.7693e-01,\n",
            "         -7.7127e-01,  2.4409e-02, -6.0075e-01, -8.4725e-01, -1.3395e-01,\n",
            "          3.6057e-02,  6.2352e-01, -1.2556e+00, -5.3935e-01, -1.1351e+00,\n",
            "          1.7988e-02,  1.9202e+00, -2.4433e+00, -7.2586e-01,  9.1268e-01,\n",
            "         -6.1512e-01,  1.0992e+00,  1.4274e+00, -1.3813e+00, -6.7661e-01,\n",
            "         -1.1869e-02,  4.4540e-01, -7.6747e-01, -1.1798e+00, -5.6673e-01,\n",
            "         -5.6045e-01,  2.7811e-01,  1.2256e+00,  6.0474e-01, -1.4404e+00,\n",
            "         -7.1431e-01, -6.7812e-01, -1.1150e+00, -6.2154e-01, -9.5409e-01,\n",
            "         -1.2453e+00, -2.1219e-01,  6.0456e-02, -7.6400e-01,  1.8560e-01,\n",
            "         -6.2334e-02,  5.9447e-02,  4.7206e-01, -5.4750e-01, -2.5931e-01,\n",
            "         -1.2635e+00,  5.8410e-01, -5.5567e-02, -1.3126e+00,  6.7465e-01,\n",
            "          5.8425e-01, -1.9970e+00,  5.8205e-01,  1.6632e+00,  1.1562e+00,\n",
            "          1.6837e+00,  2.2689e-01, -2.0453e+00, -3.0614e-01,  9.6434e-01,\n",
            "         -4.6734e-01,  3.9878e-01, -1.1401e+00,  9.7099e-01,  8.4626e-01,\n",
            "         -1.6014e+00,  1.1490e+00, -4.2742e-01,  4.5049e-01, -4.8818e-01,\n",
            "         -8.0693e-01, -5.6885e-01, -4.6312e-01, -1.7601e+00, -7.4341e-01,\n",
            "         -2.4687e-02, -1.0034e+00,  4.2146e-01,  1.9111e+00,  1.1083e+00,\n",
            "         -6.9877e-01, -2.9092e-01,  2.9961e+00,  4.2185e-01,  4.6769e-01,\n",
            "         -1.3758e+00, -1.2890e+00, -5.6140e-01,  8.7134e-01,  1.4578e-01,\n",
            "          7.2914e-01,  1.5897e-01, -2.9334e+00, -2.5646e-01, -4.7627e-01,\n",
            "          8.8220e-01,  2.5314e-01, -2.5758e-01, -3.5124e-02,  7.0351e-04,\n",
            "         -2.8782e-01,  2.0636e-01, -1.1553e+00, -5.0789e-01, -1.4208e+00,\n",
            "          4.0814e-01,  2.3145e-01,  6.9088e-02,  7.7474e-01, -9.0089e-01,\n",
            "          1.5462e+00,  8.0570e-01, -1.1908e+00,  6.3160e-01, -1.3758e+00,\n",
            "         -1.8608e-01, -1.8310e-01, -2.7315e-01,  3.8989e-01, -1.1651e+00,\n",
            "         -3.6663e-01, -6.8441e-01,  1.0586e+00,  1.8608e-01,  2.8345e-01,\n",
            "         -8.7352e-01,  6.9066e-01,  1.0562e+00, -1.9233e+00,  1.0027e+00,\n",
            "          1.1920e-01,  7.1407e-01,  9.6204e-01,  9.1317e-01,  1.2554e-01,\n",
            "          1.0911e-01,  8.6627e-01, -1.9064e-01, -1.4163e+00,  1.4034e+00,\n",
            "         -1.1257e+00, -1.2565e+00, -1.7993e+00,  2.2332e-02,  1.2611e+00,\n",
            "          1.0542e+00,  1.0464e+00,  1.4538e+00, -1.3007e+00, -3.8080e-01,\n",
            "          7.5085e-01, -1.4623e+00,  9.5512e-01,  7.8744e-01,  3.9698e-02,\n",
            "          1.9927e-01, -9.1458e-01, -1.0481e+00,  1.0530e+00, -1.1455e-01,\n",
            "         -2.0964e-01,  7.1209e-01, -4.0082e-01,  1.6783e+00, -5.3922e-01,\n",
            "         -3.3877e-01,  7.4858e-01,  9.4164e-01, -2.0289e+00, -9.6407e-01,\n",
            "         -1.6529e+00,  8.6493e-01, -2.9944e-01,  2.9717e+00, -7.2582e-01,\n",
            "          1.0769e+00, -8.5403e-01, -4.1832e-01, -1.4565e-01, -1.1738e-01,\n",
            "          3.2928e-02,  2.3935e+00,  1.4963e-01, -2.1744e-01,  9.6496e-01,\n",
            "          6.0297e-01, -3.4258e-01,  1.2818e+00, -5.3910e-01, -2.2010e-01,\n",
            "          3.2538e-01, -2.8382e-01,  2.1956e+00,  2.6576e-01, -4.3668e-01,\n",
            "          1.6889e-01,  1.0428e-01, -1.2702e+00,  2.9122e-02, -4.9662e-01,\n",
            "          2.3955e+00, -4.2070e-01,  9.4760e-01, -1.5533e+00, -9.1334e-01,\n",
            "         -2.5161e-01,  6.7240e-01, -7.6212e-01, -7.7641e-01,  5.8240e-01,\n",
            "          9.1160e-01, -1.4704e+00,  3.2886e-01,  6.1757e-01,  9.5432e-01,\n",
            "         -1.2186e+00, -1.7168e+00,  4.0882e-01,  1.7071e+00,  8.6896e-01,\n",
            "         -1.1762e+00,  1.9884e-01, -1.7081e-02,  1.8934e+00, -7.9243e-01,\n",
            "          1.3581e+00,  1.0492e+00,  1.4786e+00,  7.4618e-01, -6.5873e-01,\n",
            "          5.6207e-01,  8.2133e-01, -3.0058e-01,  2.0855e-01,  6.5920e-01,\n",
            "         -3.2728e-01, -1.1433e+00,  7.1621e-01,  1.4151e-01, -8.3330e-01,\n",
            "          1.1162e+00,  1.8628e-01,  1.0691e+00,  4.7476e-02,  4.9787e-01,\n",
            "         -4.2541e-01, -1.1533e+00, -3.5393e-01, -1.2654e-01,  2.9403e-01,\n",
            "         -5.1437e-01, -3.7319e-01, -1.1688e-01, -8.2514e-01,  2.1534e-01,\n",
            "         -6.5966e-01, -3.9661e-01, -8.4297e-01,  2.0190e+00,  1.7911e-01,\n",
            "          4.4753e-01,  1.5277e+00,  3.7224e-02, -9.1280e-01, -3.3580e-01,\n",
            "          7.4141e-01, -2.2746e-01, -9.4010e-03, -1.5833e+00,  6.9512e-01,\n",
            "         -1.9171e-01,  6.5657e-01, -1.8046e-02, -9.0353e-01, -9.3362e-01,\n",
            "          6.5706e-01,  4.7232e-03, -5.2421e-01, -8.2516e-01, -3.3033e-02,\n",
            "          1.2871e-01,  6.5339e-01, -7.1821e-01,  1.3490e+00,  6.7026e-01,\n",
            "          1.3471e-01,  1.5674e+00,  1.5940e+00, -7.8779e-01,  8.6674e-01,\n",
            "         -1.2899e+00, -1.1237e-01,  1.4285e+00, -5.5517e-01,  8.6509e-01,\n",
            "         -2.0579e+00,  9.8957e-01,  1.8094e+00]], grad_fn=<SliceBackward0>)\n",
            "probs:tensor([[1.2633e-03, 1.7833e-03, 5.8653e-04, 2.5197e-03, 9.6257e-04, 1.8748e-03,\n",
            "         5.8166e-04, 8.4977e-04, 2.5257e-03, 3.5520e-04, 1.7739e-03, 4.6210e-04,\n",
            "         7.5968e-04, 4.6689e-04, 2.0207e-03, 6.9191e-04, 3.7779e-03, 3.1619e-04,\n",
            "         2.2727e-04, 4.1292e-04, 1.0011e-03, 5.8161e-04, 2.8096e-03, 1.2821e-03,\n",
            "         5.5349e-04, 2.9396e-04, 1.8195e-03, 1.7115e-04, 2.5941e-03, 6.3868e-04,\n",
            "         3.6216e-04, 1.2910e-03, 6.5426e-04, 1.4603e-03, 9.6528e-04, 1.6173e-03,\n",
            "         6.4532e-04, 1.1877e-04, 5.6268e-04, 6.8726e-04, 1.8308e-03, 1.5284e-03,\n",
            "         1.0285e-03, 3.5281e-04, 4.3931e-04, 6.2366e-04, 8.4660e-03, 6.4749e-04,\n",
            "         3.9363e-04, 1.9286e-03, 1.4400e-03, 3.6063e-04, 2.7662e-03, 1.7609e-04,\n",
            "         2.7170e-04, 4.5948e-04, 1.8837e-04, 6.9820e-04, 5.0018e-04, 4.0527e-04,\n",
            "         5.3230e-04, 4.8036e-04, 1.2005e-03, 8.6122e-04, 3.2793e-04, 2.1800e-05,\n",
            "         9.4475e-04, 6.1398e-04, 2.7990e-04, 5.4349e-03, 8.4334e-04, 1.0311e-03,\n",
            "         4.8794e-04, 1.9345e-03, 5.7554e-04, 1.7826e-04, 2.0508e-04, 3.0484e-04,\n",
            "         1.7265e-03, 5.1802e-03, 5.3995e-04, 3.5440e-04, 4.8152e-03, 1.6127e-03,\n",
            "         2.9615e-04, 5.1388e-04, 1.3103e-03, 4.7684e-04, 1.0069e-03, 1.8199e-04,\n",
            "         1.3786e-03, 5.0405e-04, 9.8404e-04, 8.7598e-04, 2.3317e-04, 3.9406e-04,\n",
            "         3.9747e-04, 1.1554e-03, 9.3447e-04, 7.3929e-04, 1.2914e-03, 9.0024e-04,\n",
            "         7.8781e-04, 8.2212e-04, 1.1765e-03, 1.9165e-03, 2.7032e-04, 3.6067e-04,\n",
            "         1.0254e-04, 2.7748e-03, 8.3784e-04, 2.5182e-03, 1.0528e-04, 5.5471e-04,\n",
            "         1.8902e-04, 1.2747e-03, 1.2673e-03, 5.1779e-04, 8.1946e-04, 4.5905e-04,\n",
            "         1.6183e-03, 3.1347e-04, 7.6608e-04, 1.1663e-03, 1.6176e-03, 1.8065e-03,\n",
            "         2.1536e-04, 5.9348e-04, 1.3597e-03, 1.5443e-04, 3.6391e-03, 6.5555e-04,\n",
            "         6.8746e-04, 3.7958e-03, 7.3930e-04, 2.3362e-03, 8.2052e-04, 1.1512e-03,\n",
            "         1.0308e-04, 6.7473e-04, 2.1980e-03, 4.9603e-04, 1.5341e-03, 1.2173e-03,\n",
            "         3.2925e-04, 1.1998e-04, 1.5946e-03, 5.0563e-04, 4.6158e-04, 4.3921e-03,\n",
            "         8.7182e-04, 1.3861e-03, 9.4402e-04, 5.0905e-04, 1.3399e-04, 2.0556e-03,\n",
            "         3.0844e-04, 2.3830e-03, 4.5215e-03, 2.2465e-03, 6.0669e-04, 7.3423e-04,\n",
            "         1.1526e-03, 2.3904e-03, 1.7088e-03, 1.0220e-03, 1.0268e-03, 2.9142e-03,\n",
            "         3.3597e-03, 1.5520e-04, 6.4183e-04, 1.5086e-03, 1.3532e-03, 1.1411e-03,\n",
            "         2.7028e-04, 4.6931e-04, 3.8760e-04, 3.1462e-04, 1.9823e-03, 7.2773e-03,\n",
            "         2.8491e-04, 2.4159e-04, 2.2951e-03, 8.9767e-04, 3.7744e-03, 1.0122e-03,\n",
            "         2.7322e-04, 5.8180e-04, 3.2669e-04, 5.0226e-05, 2.0172e-04, 1.0955e-03,\n",
            "         1.9489e-04, 3.4257e-04, 1.5813e-04, 7.8299e-04, 2.0035e-04, 1.2585e-03,\n",
            "         3.6927e-04, 2.4703e-03, 8.8928e-04, 4.5218e-03, 1.6360e-04, 9.5703e-04,\n",
            "         1.2807e-03, 5.6825e-04, 3.2782e-04, 1.6712e-03, 4.0285e-03, 1.2291e-03,\n",
            "         1.7054e-04, 7.3636e-04, 1.8602e-04, 5.0355e-04, 2.6358e-03, 1.8014e-04,\n",
            "         1.0716e-03, 1.1844e-03, 1.4089e-03, 2.2862e-03, 3.8602e-04, 1.6423e-03,\n",
            "         1.4260e-03, 1.3246e-03, 6.0231e-04, 2.6424e-04, 1.4623e-03, 3.0913e-04,\n",
            "         1.1184e-04, 8.5026e-04, 2.6647e-04, 7.2237e-04, 1.1492e-03, 2.0245e-03,\n",
            "         8.7384e-03, 1.4994e-03, 3.4888e-04, 3.5562e-04, 3.6958e-03, 3.2770e-04,\n",
            "         2.1288e-03, 2.4374e-03, 1.6414e-03, 4.9669e-04, 1.6373e-04, 1.5208e-04,\n",
            "         3.4886e-04, 2.4954e-03, 3.6027e-04, 2.1402e-04, 1.5042e-04, 2.1618e-03,\n",
            "         7.6967e-04, 5.5193e-04, 3.2185e-04, 4.7501e-04, 2.1808e-03, 8.5373e-04,\n",
            "         7.8078e-04, 1.5034e-03, 4.3210e-04, 7.3499e-04, 3.4533e-03, 2.1215e-04,\n",
            "         1.3109e-02, 1.1982e-03, 9.5514e-04, 3.3491e-04, 8.6194e-04, 1.1906e-03,\n",
            "         1.2157e-03, 3.9356e-04, 4.7987e-04, 1.1515e-03, 6.6757e-04, 1.3424e-04,\n",
            "         4.5436e-04, 9.7437e-04, 1.3445e-03, 1.4042e-04, 1.1805e-03, 1.4071e-04,\n",
            "         2.3545e-04, 4.2872e-04, 4.1138e-04, 1.0363e-03, 1.4482e-04, 2.4331e-04,\n",
            "         2.2509e-04, 3.0170e-03, 1.1476e-02, 9.0692e-04, 3.0168e-04, 1.0672e-03,\n",
            "         4.2883e-04, 7.0564e-04, 9.9597e-04, 2.1274e-03, 8.7924e-04, 1.2543e-03,\n",
            "         5.8920e-04, 5.0738e-04, 7.8897e-04, 2.1528e-04, 1.4947e-03, 1.2033e-03,\n",
            "         4.3656e-04, 6.2312e-04, 2.5432e-03, 8.5626e-04, 1.6391e-04, 3.5494e-03,\n",
            "         1.0909e-03, 3.9076e-04, 7.2356e-04, 1.2733e-03, 1.6662e-03, 4.6698e-03,\n",
            "         4.1130e-04, 1.1104e-04, 1.5321e-03, 5.9477e-04, 8.0488e-04, 8.0371e-04,\n",
            "         1.2919e-03, 1.3597e-03, 2.1657e-03, 2.6099e-03, 1.1461e-03, 9.7782e-04,\n",
            "         8.1815e-05, 2.9356e-03, 4.2109e-03, 3.0726e-04, 2.3270e-04, 1.5128e-04,\n",
            "         1.3356e-02, 1.3300e-04, 1.0876e-03, 1.2816e-03, 2.1917e-05, 8.2096e-04,\n",
            "         1.9977e-03, 1.4473e-04, 6.5038e-04, 8.6001e-04, 3.7181e-04, 2.7597e-04,\n",
            "         4.9943e-04, 4.4972e-04, 2.1557e-03, 7.5023e-04, 1.1332e-03, 3.1693e-04,\n",
            "         7.5931e-04, 3.7727e-04, 1.9095e-04, 3.0510e-03, 7.2033e-04, 1.0881e-03,\n",
            "         8.2583e-05, 6.9908e-04, 1.6473e-03, 4.1943e-04, 5.6545e-04, 2.2931e-03,\n",
            "         2.8098e-04, 8.5933e-04, 2.5640e-04, 2.9500e-04, 2.5070e-04, 4.5079e-04,\n",
            "         5.1385e-04, 9.7660e-04, 1.9306e-03, 1.9088e-04, 6.5192e-04, 8.2558e-04,\n",
            "         2.2062e-03, 1.7281e-03, 7.0148e-04, 9.6185e-04, 1.2412e-04, 1.2885e-03,\n",
            "         2.0204e-03, 1.0444e-03, 4.7455e-04, 7.0792e-04, 6.4068e-04, 4.4217e-04,\n",
            "         4.8083e-04, 8.0147e-04, 5.1573e-04, 1.0600e-03, 5.0040e-04, 4.4908e-04,\n",
            "         2.0866e-04, 6.1266e-04, 2.3916e-03, 3.3922e-04, 2.5072e-04, 8.2573e-04,\n",
            "         9.8251e-04, 9.0779e-04, 1.1418e-03, 9.3688e-04, 3.8144e-04, 3.3221e-04,\n",
            "         1.8977e-03, 1.2699e-03, 9.5447e-04, 7.0730e-04, 1.0688e-03, 3.1090e-04,\n",
            "         1.8886e-03, 3.2739e-04, 2.1869e-03, 1.8133e-03, 9.4554e-03, 2.2182e-03,\n",
            "         6.8642e-04, 1.8039e-03, 1.2583e-03, 6.6370e-04, 7.5538e-04, 6.2583e-04,\n",
            "         7.8447e-05, 3.0609e-04, 8.8196e-05, 2.3767e-04, 9.6337e-04, 1.6477e-04,\n",
            "         1.6202e-04, 1.9472e-03, 7.2498e-03, 4.0272e-04, 4.0653e-04, 3.4146e-03,\n",
            "         5.3673e-04, 7.3563e-04, 1.6296e-03, 3.8410e-04, 1.1699e-03, 3.3821e-04,\n",
            "         9.0268e-04, 2.0809e-03, 3.0237e-04, 8.0880e-04, 1.5707e-03, 1.1524e-03,\n",
            "         2.6818e-03, 7.8110e-04, 6.4705e-04, 2.2175e-03, 2.1814e-03, 6.7169e-05,\n",
            "         2.4950e-03, 1.4323e-03, 1.2853e-03, 2.8877e-04, 8.2538e-04, 3.8509e-04,\n",
            "         2.5173e-04, 2.0254e-03, 1.0960e-03, 7.4101e-03, 4.2313e-03, 1.5195e-03,\n",
            "         6.0728e-04, 2.0791e-04, 5.8808e-04, 2.3759e-04, 8.3091e-04, 4.4654e-04,\n",
            "         4.0049e-04, 3.0487e-04, 3.1920e-04, 8.6383e-03, 3.0764e-04, 4.1303e-03,\n",
            "         3.8970e-03, 7.7679e-04, 6.5592e-03, 4.3666e-04, 5.1939e-04, 1.1114e-03,\n",
            "         1.3376e-03, 2.5856e-04, 4.1534e-04, 7.0683e-04, 5.1634e-04, 1.7167e-04,\n",
            "         6.3283e-04, 7.0980e-05, 3.6623e-04, 2.1879e-04, 1.2345e-03, 7.1650e-04,\n",
            "         2.0316e-03, 6.9993e-04, 1.8236e-04, 1.2812e-03, 3.3490e-04, 1.9220e-03,\n",
            "         1.4407e-04, 1.6067e-03, 6.2722e-04, 2.5952e-03, 1.8707e-03, 4.2203e-04,\n",
            "         3.4355e-04, 3.2409e-03, 4.5508e-04, 9.6831e-04, 5.9847e-04, 2.0190e-04,\n",
            "         1.7857e-04, 1.7220e-03, 6.0903e-04, 1.0845e-03, 6.1113e-03, 1.8930e-04,\n",
            "         2.3265e-03, 7.3878e-04, 7.8065e-04, 1.3359e-03, 2.3340e-03, 1.4749e-04,\n",
            "         3.0620e-04, 1.2134e-03, 1.9678e-04, 2.0003e-03, 1.6089e-03, 9.2971e-04,\n",
            "         3.9486e-04, 8.4534e-04, 5.3858e-04, 2.9265e-03, 3.0121e-03, 7.3909e-04,\n",
            "         4.0133e-04, 8.3479e-04, 2.8152e-04, 6.8932e-04, 8.4845e-04, 3.2872e-04,\n",
            "         7.2843e-04, 3.8984e-04, 3.0467e-04, 6.2174e-04, 7.3696e-04, 1.3261e-03,\n",
            "         2.0252e-04, 4.1452e-04, 2.2847e-04, 7.2377e-04, 4.8498e-03, 6.1758e-05,\n",
            "         3.4399e-04, 1.7707e-03, 3.8428e-04, 2.1338e-03, 2.9627e-03, 1.7861e-04,\n",
            "         3.6136e-04, 7.0248e-04, 1.1097e-03, 3.2997e-04, 2.1847e-04, 4.0333e-04,\n",
            "         4.0587e-04, 9.3878e-04, 2.4213e-03, 1.3014e-03, 1.6835e-04, 3.4799e-04,\n",
            "         3.6081e-04, 2.3311e-04, 3.8182e-04, 2.7380e-04, 2.0463e-04, 5.7495e-04,\n",
            "         7.5516e-04, 3.3112e-04, 8.5583e-04, 6.6790e-04, 7.5440e-04, 1.1397e-03,\n",
            "         4.1116e-04, 5.4849e-04, 2.0094e-04, 1.2748e-03, 6.7244e-04, 1.9131e-04,\n",
            "         1.3957e-03, 1.2750e-03, 9.6494e-05, 1.2722e-03, 3.7506e-03, 2.2590e-03,\n",
            "         3.8285e-03, 8.9191e-04, 9.1945e-05, 5.2340e-04, 1.8646e-03, 4.4547e-04,\n",
            "         1.0592e-03, 2.2733e-04, 1.8771e-03, 1.6570e-03, 1.4332e-04, 2.2429e-03,\n",
            "         4.6362e-04, 1.1154e-03, 4.3629e-04, 3.1720e-04, 4.0247e-04, 4.4736e-04,\n",
            "         1.2229e-04, 3.3801e-04, 6.9353e-04, 2.6062e-04, 1.0835e-03, 4.8058e-03,\n",
            "         2.1534e-03, 3.5344e-04, 5.3142e-04, 1.4222e-02, 1.0839e-03, 1.1347e-03,\n",
            "         1.7959e-04, 1.9588e-04, 4.0548e-04, 1.6990e-03, 8.2242e-04, 1.4738e-03,\n",
            "         8.3334e-04, 3.7828e-05, 5.5006e-04, 4.4151e-04, 1.7176e-03, 9.1564e-04,\n",
            "         5.4944e-04, 6.8633e-04, 7.1136e-04, 5.3308e-04, 8.7379e-04, 2.2389e-04,\n",
            "         4.2777e-04, 1.7168e-04, 1.0692e-03, 8.9599e-04, 7.6171e-04, 1.5426e-03,\n",
            "         2.8876e-04, 3.3364e-03, 1.5911e-03, 2.1608e-04, 1.3369e-03, 1.7960e-04,\n",
            "         5.9017e-04, 5.9192e-04, 5.4095e-04, 1.0498e-03, 2.2171e-04, 4.9267e-04,\n",
            "         3.5855e-04, 2.0489e-03, 8.5625e-04, 9.4381e-04, 2.9677e-04, 1.4182e-03,\n",
            "         2.0441e-03, 1.0387e-04, 1.9375e-03, 8.0085e-04, 1.4518e-03, 1.8604e-03,\n",
            "         1.7716e-03, 8.0595e-04, 7.9282e-04, 1.6904e-03, 5.8748e-04, 1.7245e-04,\n",
            "         2.8925e-03, 2.3062e-04, 2.0235e-04, 1.1759e-04, 7.2692e-04, 2.5089e-03,\n",
            "         2.0400e-03, 2.0240e-03, 3.0419e-03, 1.9359e-04, 4.8574e-04, 1.5062e-03,\n",
            "         1.6471e-04, 1.8475e-03, 1.5623e-03, 7.3965e-04, 8.6761e-04, 2.8483e-04,\n",
            "         2.4922e-04, 2.0375e-03, 6.3393e-04, 5.7642e-04, 1.4489e-03, 4.7612e-04,\n",
            "         3.8078e-03, 4.1458e-04, 5.0659e-04, 1.5028e-03, 1.8228e-03, 9.3460e-05,\n",
            "         2.7108e-04, 1.3613e-04, 1.6882e-03, 5.2691e-04, 1.3879e-02, 3.4400e-04,\n",
            "         2.0867e-03, 3.0261e-04, 4.6786e-04, 6.1451e-04, 6.3213e-04, 7.3466e-04,\n",
            "         7.7853e-03, 8.2560e-04, 5.7194e-04, 1.8658e-03, 1.2991e-03, 5.0467e-04,\n",
            "         2.5612e-03, 4.1463e-04, 5.7042e-04, 9.8423e-04, 5.3521e-04, 6.3871e-03,\n",
            "         9.2726e-04, 4.5934e-04, 8.4165e-04, 7.8899e-04, 1.9958e-04, 7.3187e-04,\n",
            "         4.3262e-04, 7.8008e-03, 4.6675e-04, 1.8337e-03, 1.5039e-04, 2.8519e-04,\n",
            "         5.5273e-04, 1.3925e-03, 3.3174e-04, 3.2703e-04, 1.2727e-03, 1.7688e-03,\n",
            "         1.6338e-04, 9.8766e-04, 1.3182e-03, 1.8460e-03, 2.1016e-04, 1.2770e-04,\n",
            "         1.0699e-03, 3.9191e-03, 1.6950e-03, 2.1927e-04, 8.6725e-04, 6.9882e-04,\n",
            "         4.7214e-03, 3.2184e-04, 2.7644e-03, 2.0297e-03, 3.1183e-03, 1.4992e-03,\n",
            "         3.6788e-04, 1.2471e-03, 1.6162e-03, 5.2631e-04, 8.7571e-04, 1.3743e-03,\n",
            "         5.1245e-04, 2.2661e-04, 1.4549e-03, 8.1892e-04, 3.0895e-04, 2.1705e-03,\n",
            "         8.5642e-04, 2.0706e-03, 7.4543e-04, 1.1695e-03, 4.6455e-04, 2.2435e-04,\n",
            "         4.9897e-04, 6.2637e-04, 9.5385e-04, 4.2501e-04, 4.8945e-04, 6.3245e-04,\n",
            "         3.1148e-04, 8.8167e-04, 3.6753e-04, 4.7812e-04, 3.0598e-04, 5.3535e-03,\n",
            "         8.5030e-04, 1.1121e-03, 3.2755e-03, 7.3782e-04, 2.8534e-04, 5.0810e-04,\n",
            "         1.4920e-03, 5.6624e-04, 7.0421e-04, 1.4594e-04, 1.4245e-03, 5.8685e-04,\n",
            "         1.3707e-03, 6.9815e-04, 2.8800e-04, 2.7946e-04, 1.3713e-03, 7.1423e-04,\n",
            "         4.2085e-04, 3.1148e-04, 6.8776e-04, 8.0850e-04, 1.3663e-03, 3.4663e-04,\n",
            "         2.7393e-03, 1.3896e-03, 8.1337e-04, 3.4079e-03, 3.4998e-03, 3.2333e-04,\n",
            "         1.6913e-03, 1.9570e-04, 6.3531e-04, 2.9661e-03, 4.0802e-04, 1.6885e-03,\n",
            "         9.0795e-05, 1.9123e-03, 4.3412e-03]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next:tensor([[86]])\n",
            "idx:tensor([[  0, 359,  86]]),logits:tensor([[[ 0.1808, -0.0700, -0.3596,  ..., -0.9559,  2.0515,  0.0096],\n",
            "         [ 0.5750,  0.9197, -0.1923,  ..., -2.0579,  0.9896,  1.8094],\n",
            "         [ 0.3261, -1.0293,  0.9530,  ...,  2.0647, -0.2782,  1.1425]]],\n",
            "       grad_fn=<EmbeddingBackward0>),loss:None\n",
            "logits[:, -1, :]:tensor([[ 3.2611e-01, -1.0293e+00,  9.5296e-01, -7.2279e-01, -1.9727e+00,\n",
            "          3.2113e-01,  1.7945e-01, -3.9376e-01, -1.3016e+00, -3.8396e-01,\n",
            "         -7.8161e-01, -9.2075e-02,  1.3261e+00,  1.0194e+00,  2.9345e-01,\n",
            "          3.3997e-01, -6.2893e-01,  5.8233e-01,  1.1792e+00,  2.2897e-01,\n",
            "          9.6109e-01,  5.9004e-01,  9.3108e-01, -9.1161e-01,  1.2499e+00,\n",
            "         -1.7282e+00,  1.3220e+00, -2.4234e-01, -1.2972e+00, -4.8410e-01,\n",
            "         -3.5295e-01,  8.3787e-01, -1.0615e+00,  1.0019e+00,  3.9529e-01,\n",
            "         -9.3419e-01,  1.4459e+00, -3.2793e-01, -5.5187e-01, -7.1182e-01,\n",
            "          1.1550e+00,  7.3074e-01,  1.0423e+00,  4.1687e-01,  1.9473e-01,\n",
            "          1.0986e+00, -8.4203e-01,  8.5244e-02, -9.7799e-01, -1.0563e+00,\n",
            "          2.2605e+00, -1.8181e+00,  9.3206e-01, -3.9074e-01, -5.8729e-01,\n",
            "         -6.9537e-01, -4.2327e-01,  1.4161e-01,  5.5293e-01,  1.0877e+00,\n",
            "          1.2852e+00,  1.2223e+00, -9.4617e-01, -2.6766e-01,  9.7835e-01,\n",
            "         -1.0781e+00,  1.1283e-01,  1.3030e+00,  1.4780e+00,  5.8343e-01,\n",
            "         -6.4548e-01, -1.1348e+00, -9.2022e-01,  2.7400e+00,  9.2349e-01,\n",
            "          1.8193e+00, -1.9456e-01, -5.0136e-01,  4.9265e-01,  1.4847e-01,\n",
            "          1.3241e+00, -1.1525e+00, -5.6535e-02,  2.6674e-01, -6.0002e-02,\n",
            "          5.8628e-01, -9.6457e-01, -4.8186e-01,  1.4390e+00,  7.1083e-01,\n",
            "         -1.1183e+00, -1.2584e+00,  3.1088e-01,  2.4280e-02, -1.8678e+00,\n",
            "         -5.8032e-01, -4.2810e-01,  1.3216e+00, -2.2344e-01,  1.5794e+00,\n",
            "          1.3696e-01,  7.8594e-01,  1.8677e+00, -9.1705e-02,  8.9258e-02,\n",
            "         -1.9635e+00, -2.2121e-01, -8.6264e-01,  1.3069e+00,  1.0181e+00,\n",
            "          6.2208e-02,  1.5131e-01,  1.2100e+00,  7.3097e-02, -3.5212e-01,\n",
            "         -1.0056e+00, -9.1557e-02, -1.1827e+00,  6.6203e-01,  1.5167e-01,\n",
            "          2.4803e+00,  7.1299e-01,  8.4489e-01, -2.7196e-01, -2.4372e-01,\n",
            "         -9.5392e-01, -2.8651e-02,  1.5982e-01, -1.3903e+00, -6.5019e-01,\n",
            "         -4.3995e-01,  2.6023e+00, -1.4451e+00, -3.5929e-01, -1.0905e-01,\n",
            "         -6.4960e-02,  9.8550e-03,  2.9857e-01,  1.3453e-01,  1.8371e+00,\n",
            "         -5.2089e-01,  1.0964e-01,  1.1155e-01,  8.8077e-01,  8.1410e-01,\n",
            "         -2.3545e-01,  1.3650e+00, -4.0680e-01,  1.0955e+00,  1.3645e+00,\n",
            "         -7.9863e-01,  1.1234e+00, -4.8004e-02,  1.2714e-01, -2.3357e-01,\n",
            "          2.5105e-01, -1.7076e-01, -1.7394e+00, -1.0520e+00,  2.0978e+00,\n",
            "         -7.5605e-01,  1.5374e+00, -5.0039e-01,  6.4651e-01,  5.3704e-01,\n",
            "         -1.6264e-01, -3.4734e-01, -1.6936e-01, -1.8305e-02, -9.3172e-01,\n",
            "          5.1110e-01,  5.9816e-01,  5.2415e-01, -8.8022e-01, -7.8030e-01,\n",
            "          1.3131e+00,  3.0159e-02,  1.7118e+00, -2.3216e+00,  6.3720e-02,\n",
            "          2.8646e-02, -5.1600e-02,  6.8960e-01,  9.8021e-01, -3.1687e+00,\n",
            "         -3.5172e+00, -1.2459e-01,  1.2550e+00, -1.8661e+00,  6.9038e-02,\n",
            "         -6.8166e-01,  3.7544e-01, -2.3747e-01, -1.1570e+00, -2.1258e-01,\n",
            "          5.8131e-01,  8.3812e-01,  8.4994e-01, -1.2467e-02,  1.0761e+00,\n",
            "         -8.9709e-01, -4.8215e-02,  3.6640e-01,  9.0127e-01, -1.1797e+00,\n",
            "         -5.7459e-01, -1.1659e+00, -1.5677e+00,  1.9949e+00, -1.0748e+00,\n",
            "         -1.1379e+00, -1.2555e+00, -5.8846e-01, -6.6352e-01,  1.0447e-01,\n",
            "          1.5724e+00,  2.3705e-01, -3.8268e-02, -1.9711e-01, -2.1299e+00,\n",
            "         -8.7813e-01, -1.8406e+00, -7.4562e-01,  5.7057e-01,  5.5195e-01,\n",
            "         -1.0908e+00,  1.2034e+00,  1.5218e+00, -7.7191e-01, -1.4609e+00,\n",
            "         -6.4584e-01, -4.4728e-01, -1.3875e-01, -1.0574e+00,  5.0371e-01,\n",
            "          6.5228e-01, -8.5557e-01,  4.3885e-01,  1.0163e+00, -9.5315e-01,\n",
            "         -2.2028e+00,  1.4022e+00, -2.4845e+00,  1.8083e+00, -2.2563e+00,\n",
            "          2.9513e-01,  1.0627e+00,  1.6660e+00, -1.4454e-01,  6.6649e-01,\n",
            "          1.7957e+00, -8.5242e-01, -9.2652e-01, -9.7865e-02, -1.0152e+00,\n",
            "         -3.4317e-01, -1.4036e+00,  1.2632e-01, -1.4441e-01, -6.4991e-01,\n",
            "          8.1046e-01, -5.9993e-01,  9.7653e-01, -8.4339e-01, -4.6985e-01,\n",
            "          5.9122e-01, -6.4867e-01,  3.8567e-01, -1.5895e+00,  7.0966e-01,\n",
            "          1.9619e-01,  1.0754e+00,  1.1209e+00, -5.5313e-01,  1.5292e+00,\n",
            "          2.9246e-01, -9.6164e-01, -2.0620e+00, -4.1127e-01, -2.9087e-01,\n",
            "         -2.2996e-01, -1.0990e+00, -1.1014e+00,  9.0465e-02, -1.0016e+00,\n",
            "         -5.6985e-01,  8.7195e-01,  1.4844e+00,  1.3953e+00, -7.3874e-02,\n",
            "          6.9507e-01,  8.0148e-01, -2.2080e-01, -1.8110e+00,  3.4936e-01,\n",
            "         -6.6806e-01,  6.4068e-02, -1.1802e+00, -4.9958e-01,  3.3966e-01,\n",
            "         -5.4042e-01,  1.1274e-01, -3.7787e-01,  2.1214e-01, -6.5653e-01,\n",
            "         -2.1739e-01,  3.8192e-01, -7.9674e-01,  8.7138e-01,  5.9668e-01,\n",
            "         -8.4606e-01,  1.1094e+00, -5.6706e-01, -5.7056e-01, -7.6937e-01,\n",
            "         -3.3702e-01,  1.5998e-01,  7.2159e-01,  8.4827e-01,  1.5556e+00,\n",
            "         -6.5399e-02, -1.1521e+00, -1.4018e-01, -1.9276e+00,  9.8800e-01,\n",
            "         -1.4457e+00,  1.4594e+00,  4.5664e-01, -1.1030e+00, -2.6641e-01,\n",
            "         -5.9998e-01,  1.2183e+00, -3.7957e-01, -1.4352e+00,  1.8362e-01,\n",
            "          4.1482e-01, -2.1179e-01,  2.5139e-01, -7.1398e-02, -1.7364e-01,\n",
            "          6.1943e-01,  7.0729e-01,  7.1533e-01, -8.2381e-01, -1.7182e+00,\n",
            "         -1.3487e+00,  1.3890e+00,  2.0729e-01, -7.9977e-01,  2.5082e+00,\n",
            "         -3.5098e-01,  4.7595e-01,  1.2442e+00, -3.5920e-01, -2.0074e-01,\n",
            "          1.1184e+00, -8.5163e-01,  2.5190e+00,  9.4236e-01,  6.7666e-01,\n",
            "         -1.1375e+00, -8.6810e-01,  6.7799e-01, -1.6225e+00, -2.8552e-01,\n",
            "         -6.0641e-01, -1.6345e+00, -4.7860e-01, -9.6540e-01, -1.1421e+00,\n",
            "          1.7858e+00,  3.7062e-01,  3.8165e-01,  6.0956e-01, -5.1207e-01,\n",
            "         -1.5670e+00, -3.6005e-01, -5.4510e-01,  1.4197e+00,  6.5202e-02,\n",
            "          2.9377e-01, -4.6166e-01, -2.1528e-01,  1.4656e-01,  1.9235e+00,\n",
            "          1.2015e+00,  3.8031e-01,  7.6232e-01,  3.2460e-01,  1.9789e+00,\n",
            "          8.0677e-02, -7.2484e-01,  9.2322e-01,  1.9819e-01, -1.4640e-01,\n",
            "          1.6237e-01, -1.0558e+00,  9.0909e-01, -2.6463e+00, -1.6950e+00,\n",
            "          5.2414e-01,  3.7555e-01, -1.1336e+00, -4.1947e-01,  1.6816e-01,\n",
            "         -6.5047e-01,  2.8318e-01,  3.1922e-01,  3.7924e-01,  2.5851e+00,\n",
            "          2.8175e+00, -5.7142e-01, -6.3926e-02,  6.8053e-01,  2.8968e-01,\n",
            "          5.2853e-01, -5.1157e-01, -4.7411e-01,  1.6279e+00, -5.2072e-02,\n",
            "         -4.5697e-01,  8.3862e-01,  1.4033e+00,  1.2574e+00,  4.1726e-01,\n",
            "         -1.6665e-01,  6.6733e-01,  4.3770e-01,  1.2026e+00, -6.0394e-01,\n",
            "          2.4788e+00, -3.9610e-01,  1.5760e+00, -1.9377e+00, -6.1844e-01,\n",
            "          1.1638e+00, -1.5270e+00,  3.0319e-01, -1.4444e+00,  1.2560e+00,\n",
            "          1.6381e+00,  4.5688e-01, -3.5467e-01, -1.9276e+00, -7.1475e-01,\n",
            "         -1.0664e+00, -2.7832e-01, -5.2798e-01,  1.2877e+00,  4.0056e-01,\n",
            "          2.2351e-01,  5.9940e-01, -1.1083e+00, -2.8326e-01,  1.0933e+00,\n",
            "         -1.6079e+00, -1.7162e+00,  3.4044e-01,  3.6373e-02, -6.4772e-01,\n",
            "          7.5890e-01, -1.2522e+00, -8.3598e-01, -3.6439e-01,  1.2977e+00,\n",
            "         -7.9546e-01, -7.9003e-02, -9.2041e-01, -2.6530e-01,  2.3195e-01,\n",
            "         -4.2120e-01, -2.2138e+00,  4.3268e-01, -1.5558e-01, -4.7512e-01,\n",
            "         -2.3610e+00, -1.4870e+00, -1.2152e+00, -1.4093e-01,  1.0710e+00,\n",
            "         -2.1402e-01,  1.2810e+00,  1.8757e-01, -6.9906e-01,  7.9051e-01,\n",
            "          3.6951e-01,  1.8928e-01, -1.4287e-01,  1.2447e+00, -1.4325e+00,\n",
            "          9.6721e-01, -2.2579e+00,  1.5753e+00,  2.3492e+00,  1.6275e+00,\n",
            "         -1.0062e+00,  1.2078e+00, -1.0088e+00,  1.9277e+00,  6.0512e-01,\n",
            "          7.0020e-01, -5.3820e-01,  3.1200e-01,  4.9599e-01,  1.1687e-01,\n",
            "         -7.5691e-01,  2.1554e+00, -4.7240e-01, -8.9928e-01,  1.0515e+00,\n",
            "         -1.3882e-01, -1.4701e-01,  4.2922e-02,  9.7706e-01, -1.0831e+00,\n",
            "         -7.8948e-01,  6.7537e-01,  1.4183e+00, -1.2331e+00, -3.6364e-01,\n",
            "         -5.9878e-01, -1.3520e+00, -9.9206e-01, -4.2640e-01, -1.3024e+00,\n",
            "         -1.4600e-01, -3.2295e-01, -3.8212e-01,  1.9768e+00, -4.0038e-01,\n",
            "         -5.1554e-01,  5.9586e-01, -1.1855e-01, -9.8113e-01, -5.9133e-01,\n",
            "         -1.8752e+00,  4.7588e-01,  6.7513e-01,  7.4271e-01,  1.0558e-02,\n",
            "          7.1451e-01, -2.0773e-01, -4.6129e-01,  1.3229e-01,  2.9996e-01,\n",
            "          6.3001e-01, -2.0416e-01, -4.7373e-01,  1.1889e+00, -8.8007e-01,\n",
            "          1.4130e+00, -2.3297e+00,  5.5828e-03,  3.2446e-01,  1.3333e+00,\n",
            "          6.3564e-01,  2.0165e+00, -1.1898e+00,  9.3226e-01, -1.1542e-01,\n",
            "         -5.9118e-01, -4.8738e-01,  3.5477e-01,  1.3808e+00,  1.1461e-01,\n",
            "          1.0172e+00, -1.1521e+00, -4.0835e-01, -1.0220e+00,  8.7989e-01,\n",
            "         -1.0375e+00,  7.9770e-01, -1.0478e+00,  5.5159e-01, -6.2864e-02,\n",
            "          1.2513e+00, -5.2448e-02,  1.2612e-01, -2.1559e+00, -3.5703e-01,\n",
            "         -7.1295e-01,  7.3857e-01, -4.8342e-01,  1.0526e+00, -2.4253e-01,\n",
            "         -3.9805e-01,  2.7373e-01, -1.9972e+00,  1.6010e+00,  2.9458e-01,\n",
            "         -7.6330e-01, -5.0101e-01,  7.0733e-01,  1.0987e+00, -1.6648e+00,\n",
            "         -1.3756e+00, -3.5733e-02,  7.3417e-01, -6.7666e-01,  1.7680e+00,\n",
            "          5.0261e-01, -1.0469e-01, -2.3653e-01, -1.2867e+00, -2.0657e+00,\n",
            "         -1.8568e+00, -3.1161e-01,  1.3858e-01,  1.2228e-01, -6.2003e-01,\n",
            "          3.4381e-01, -8.7925e-01,  6.1441e-01,  1.6038e+00,  1.8448e+00,\n",
            "         -5.2631e-01, -1.1345e+00,  4.1781e-01,  5.9809e-01, -1.5859e-01,\n",
            "         -1.5513e+00,  1.5147e-01,  8.1530e-01, -8.0732e-01,  1.7787e+00,\n",
            "          1.3253e+00,  1.7753e+00,  3.3682e-02,  2.3961e-01, -5.3283e-01,\n",
            "         -1.4876e+00, -3.1351e-01, -2.1127e+00,  1.2767e+00, -3.4591e-01,\n",
            "         -8.7518e-01, -7.2053e-01, -8.0037e-02, -9.2163e-01, -5.8223e-01,\n",
            "         -2.8488e-03, -5.4190e-01, -1.1201e-01, -1.0066e-01,  8.2972e-01,\n",
            "         -3.0762e-01, -3.8917e-01,  3.9473e-03,  1.3459e+00, -6.1104e-01,\n",
            "         -1.2990e+00,  2.0039e+00, -1.3079e+00, -1.0453e+00,  9.1996e-01,\n",
            "         -4.2380e-01,  1.3846e-01, -1.5279e+00, -3.0489e-01,  8.3430e-01,\n",
            "         -3.3640e-01,  3.1460e-01,  1.5310e-01,  1.7976e+00,  1.0406e+00,\n",
            "         -7.9140e-01, -8.3392e-01, -1.0688e+00, -1.0603e+00, -3.2696e-01,\n",
            "         -5.6945e-01, -1.4901e+00,  1.9090e-01,  7.2916e-01,  4.5998e-01,\n",
            "          2.0211e-01,  1.7656e-01,  1.1844e+00,  4.0787e-01,  6.4217e-01,\n",
            "         -6.4471e-01,  1.4358e+00,  4.3579e-02,  1.6404e-01,  3.0809e-01,\n",
            "          4.9174e-01,  1.4956e-01, -2.8350e-01,  1.4876e+00, -2.9774e-01,\n",
            "          6.8607e-01,  1.0198e+00,  1.1743e+00,  1.5666e+00,  1.0198e+00,\n",
            "         -8.0783e-01, -1.2751e+00, -3.5826e-01,  4.6817e-01, -1.4549e+00,\n",
            "          1.6048e-01,  6.3909e-01,  2.0813e-01,  1.3979e+00,  5.9302e-01,\n",
            "         -1.7721e-01,  8.8635e-01, -2.1084e-01, -1.2763e-01, -1.5438e+00,\n",
            "          1.1035e+00,  1.2301e+00,  9.6359e-02,  8.7550e-01,  7.9997e-01,\n",
            "         -1.7452e+00,  2.0436e-01, -7.6098e-01,  1.1748e-01,  2.6329e-02,\n",
            "         -1.0053e+00, -2.5970e-01, -8.2319e-01,  1.4254e+00, -1.7303e+00,\n",
            "         -2.2103e-02,  5.2576e-01, -2.9570e-01,  5.4387e-01,  6.6871e-01,\n",
            "         -1.0922e+00,  5.4404e-01, -1.0032e-01,  1.1231e+00, -4.0928e-01,\n",
            "         -2.3374e-01, -1.8876e-01, -2.7713e-01, -1.2703e+00,  1.2059e+00,\n",
            "          4.1621e-01, -7.5626e-01,  7.2147e-01, -1.1039e-01,  3.6947e-01,\n",
            "         -1.2149e+00, -9.3433e-01, -4.0509e-01, -1.4003e+00, -2.4698e+00,\n",
            "         -3.0591e-01,  1.5066e+00,  5.5769e-01, -3.2073e-01,  2.8668e-01,\n",
            "          9.7821e-01,  2.8090e-01,  1.3533e+00,  2.4554e+00, -1.0255e+00,\n",
            "          4.0389e-02,  5.1043e-02, -7.4554e-01, -2.1299e+00, -5.7782e-01,\n",
            "          1.0851e+00, -5.4563e-02,  3.9137e-01, -8.1624e-02, -4.2038e-01,\n",
            "          6.2727e-02, -1.3157e+00, -4.8466e-01, -8.4889e-01, -8.5346e-01,\n",
            "         -1.5860e-01,  6.9197e-01, -5.4081e-01, -4.2393e-01, -2.7455e-01,\n",
            "          6.5550e-01, -7.8836e-01,  2.1052e-01,  2.3781e-01, -4.4461e-02,\n",
            "         -1.3930e+00,  1.9843e+00,  1.0531e+00,  1.5448e+00,  1.8939e+00,\n",
            "          9.8375e-01, -8.3731e-01,  2.6502e-01, -5.1963e-01,  1.3690e+00,\n",
            "          1.4699e+00,  1.1897e-01, -8.9390e-01,  1.6548e+00, -9.7384e-01,\n",
            "         -6.8864e-01, -6.6681e-01, -2.2446e+00, -7.8585e-01, -2.7981e+00,\n",
            "          5.2470e-01, -1.2426e-01, -6.6267e-01, -9.7594e-01, -1.7283e-01,\n",
            "          2.3711e-01, -6.9090e-01, -9.2701e-01,  1.0641e+00, -6.3176e-01,\n",
            "         -3.2357e-01, -1.9808e+00,  4.9430e-01, -1.0942e+00,  2.0778e+00,\n",
            "         -4.5401e-01, -1.5212e+00,  2.1476e+00,  6.5505e-01,  1.2316e+00,\n",
            "         -1.4309e-01, -2.0658e+00,  2.4643e-01,  1.4584e+00,  5.1157e-01,\n",
            "          1.2021e+00, -6.8408e-01,  9.2194e-01, -1.0356e+00, -6.1050e-01,\n",
            "         -4.5023e-01, -2.2346e+00, -4.5298e-02, -1.2892e+00, -1.2927e-01,\n",
            "          2.0647e+00, -2.7820e-01,  1.1425e+00]], grad_fn=<SliceBackward0>)\n",
            "probs:tensor([[1.0062e-03, 2.5944e-04, 1.8834e-03, 3.5251e-04, 1.0100e-04, 1.0012e-03,\n",
            "         8.6897e-04, 4.8985e-04, 1.9759e-04, 4.9467e-04, 3.3237e-04, 6.6234e-04,\n",
            "         2.7352e-03, 2.0128e-03, 9.7389e-04, 1.0203e-03, 3.8719e-04, 1.3001e-03,\n",
            "         2.3616e-03, 9.1308e-04, 1.8987e-03, 1.3101e-03, 1.8426e-03, 2.9185e-04,\n",
            "         2.5345e-03, 1.2899e-04, 2.7239e-03, 5.6993e-04, 1.9847e-04, 4.4754e-04,\n",
            "         5.1025e-04, 1.6786e-03, 2.5123e-04, 1.9777e-03, 1.0783e-03, 2.8534e-04,\n",
            "         3.0832e-03, 5.2318e-04, 4.1821e-04, 3.5639e-04, 2.3050e-03, 1.5081e-03,\n",
            "         2.0593e-03, 1.1018e-03, 8.8235e-04, 2.1787e-03, 3.1288e-04, 7.9084e-04,\n",
            "         2.7311e-04, 2.5253e-04, 6.9631e-03, 1.1789e-04, 1.8444e-03, 4.9133e-04,\n",
            "         4.0365e-04, 3.6230e-04, 4.7560e-04, 8.3670e-04, 1.2624e-03, 2.1550e-03,\n",
            "         2.6257e-03, 2.4654e-03, 2.8194e-04, 5.5568e-04, 1.9318e-03, 2.4709e-04,\n",
            "         8.1296e-04, 2.6728e-03, 3.1840e-03, 1.3015e-03, 3.8084e-04, 2.3347e-04,\n",
            "         2.8935e-04, 1.1247e-02, 1.8287e-03, 4.4791e-03, 5.9783e-04, 4.3988e-04,\n",
            "         1.1886e-03, 8.4245e-04, 2.7297e-03, 2.2937e-04, 6.8630e-04, 9.4823e-04,\n",
            "         6.8393e-04, 1.3052e-03, 2.7680e-04, 4.4854e-04, 3.0621e-03, 1.4783e-03,\n",
            "         2.3735e-04, 2.0633e-04, 9.9102e-04, 7.4407e-04, 1.1218e-04, 4.0648e-04,\n",
            "         4.7331e-04, 2.7229e-03, 5.8080e-04, 3.5237e-03, 8.3282e-04, 1.5937e-03,\n",
            "         4.7009e-03, 6.6258e-04, 7.9402e-04, 1.0194e-04, 5.8210e-04, 3.0650e-04,\n",
            "         2.6831e-03, 2.0101e-03, 7.7283e-04, 8.4486e-04, 2.4353e-03, 7.8129e-04,\n",
            "         5.1067e-04, 2.6567e-04, 6.6268e-04, 2.2254e-04, 1.4079e-03, 8.4516e-04,\n",
            "         8.6750e-03, 1.4816e-03, 1.6904e-03, 5.5330e-04, 5.6914e-04, 2.7976e-04,\n",
            "         7.0571e-04, 8.5208e-04, 1.8082e-04, 3.7905e-04, 4.6774e-04, 9.7999e-03,\n",
            "         1.7119e-04, 5.0703e-04, 6.5119e-04, 6.8055e-04, 7.3341e-04, 9.7889e-04,\n",
            "         8.3080e-04, 4.5593e-03, 4.3137e-04, 8.1037e-04, 8.1192e-04, 1.7522e-03,\n",
            "         1.6392e-03, 5.7387e-04, 2.8436e-03, 4.8350e-04, 2.1719e-03, 2.8421e-03,\n",
            "         3.2676e-04, 2.2334e-03, 6.9218e-04, 8.2467e-04, 5.7495e-04, 9.3347e-04,\n",
            "         6.1222e-04, 1.2754e-04, 2.5363e-04, 5.9172e-03, 3.4097e-04, 3.3786e-03,\n",
            "         4.4030e-04, 1.3863e-03, 1.2425e-03, 6.1721e-04, 5.1312e-04, 6.1308e-04,\n",
            "         7.1305e-04, 2.8604e-04, 1.2107e-03, 1.3208e-03, 1.2266e-03, 3.0116e-04,\n",
            "         3.3281e-04, 2.6999e-03, 7.4846e-04, 4.0226e-03, 7.1255e-05, 7.7400e-04,\n",
            "         7.4733e-04, 6.8970e-04, 1.4473e-03, 1.9354e-03, 3.0542e-05, 2.1557e-05,\n",
            "         6.4115e-04, 2.5475e-03, 1.1237e-04, 7.7813e-04, 3.6731e-04, 1.0571e-03,\n",
            "         5.7271e-04, 2.2834e-04, 5.8715e-04, 1.2988e-03, 1.6790e-03, 1.6990e-03,\n",
            "         7.1722e-04, 2.1301e-03, 2.9612e-04, 6.9204e-04, 1.0476e-03, 1.7885e-03,\n",
            "         2.2322e-04, 4.0881e-04, 2.2631e-04, 1.5143e-04, 5.3385e-03, 2.4792e-04,\n",
            "         2.3275e-04, 2.0693e-04, 4.0318e-04, 3.7403e-04, 8.0619e-04, 3.4992e-03,\n",
            "         9.2049e-04, 6.9896e-04, 5.9630e-04, 8.6313e-05, 3.0179e-04, 1.1527e-04,\n",
            "         3.4455e-04, 1.2849e-03, 1.2612e-03, 2.4398e-04, 2.4194e-03, 3.3264e-03,\n",
            "         3.3561e-04, 1.6850e-04, 3.8070e-04, 4.6432e-04, 6.3214e-04, 2.5227e-04,\n",
            "         1.2018e-03, 1.3943e-03, 3.0867e-04, 1.1263e-03, 2.0064e-03, 2.7998e-04,\n",
            "         8.0245e-05, 2.9513e-03, 6.0544e-05, 4.4301e-03, 7.6063e-05, 9.7554e-04,\n",
            "         2.1019e-03, 3.8422e-03, 6.2848e-04, 1.4142e-03, 4.3746e-03, 3.0965e-04,\n",
            "         2.8753e-04, 6.5852e-04, 2.6313e-04, 5.1527e-04, 1.7843e-04, 8.2400e-04,\n",
            "         6.2857e-04, 3.7916e-04, 1.6332e-03, 3.9859e-04, 1.9283e-03, 3.1246e-04,\n",
            "         4.5396e-04, 1.3117e-03, 3.7962e-04, 1.0680e-03, 1.4817e-04, 1.4766e-03,\n",
            "         8.8363e-04, 2.1287e-03, 2.2278e-03, 4.1768e-04, 3.3513e-03, 9.7293e-04,\n",
            "         2.7761e-04, 9.2378e-05, 4.8135e-04, 5.4293e-04, 5.7703e-04, 2.4197e-04,\n",
            "         2.4140e-04, 7.9498e-04, 2.6672e-04, 4.1076e-04, 1.7368e-03, 3.2045e-03,\n",
            "         2.9311e-03, 6.7451e-04, 1.4552e-03, 1.6186e-03, 5.8234e-04, 1.1873e-04,\n",
            "         1.0299e-03, 3.7234e-04, 7.7427e-04, 2.2312e-04, 4.4066e-04, 1.0200e-03,\n",
            "         4.2303e-04, 8.1289e-04, 4.9770e-04, 8.9784e-04, 3.7665e-04, 5.8433e-04,\n",
            "         1.0640e-03, 3.2738e-04, 1.7358e-03, 1.3189e-03, 3.1162e-04, 2.2022e-03,\n",
            "         4.1191e-04, 4.1047e-04, 3.3646e-04, 5.1845e-04, 8.5221e-04, 1.4943e-03,\n",
            "         1.6962e-03, 3.4409e-03, 6.8025e-04, 2.2947e-04, 6.3123e-04, 1.0566e-04,\n",
            "         1.9505e-03, 1.7109e-04, 3.1252e-03, 1.1465e-03, 2.4102e-04, 5.5638e-04,\n",
            "         3.9857e-04, 2.4557e-03, 4.9685e-04, 1.7290e-04, 8.7260e-04, 1.0996e-03,\n",
            "         5.8761e-04, 9.3378e-04, 6.7618e-04, 6.1046e-04, 1.3492e-03, 1.4731e-03,\n",
            "         1.4850e-03, 3.1863e-04, 1.3027e-04, 1.8852e-04, 2.9127e-03, 8.9350e-04,\n",
            "         3.2639e-04, 8.9197e-03, 5.1126e-04, 1.1689e-03, 2.5200e-03, 5.0707e-04,\n",
            "         5.9414e-04, 2.2223e-03, 3.0989e-04, 9.0166e-03, 1.8635e-03, 1.4287e-03,\n",
            "         2.3283e-04, 3.0483e-04, 1.4306e-03, 1.4336e-04, 5.4584e-04, 3.9601e-04,\n",
            "         1.4165e-04, 4.5000e-04, 2.7657e-04, 2.3177e-04, 4.3313e-03, 1.0520e-03,\n",
            "         1.0637e-03, 1.3360e-03, 4.3519e-04, 1.5154e-04, 5.0664e-04, 4.2105e-04,\n",
            "         3.0037e-03, 7.7515e-04, 9.7421e-04, 4.5769e-04, 5.8556e-04, 8.4085e-04,\n",
            "         4.9709e-03, 2.4148e-03, 1.0623e-03, 1.5565e-03, 1.0047e-03, 5.2541e-03,\n",
            "         7.8724e-04, 3.5178e-04, 1.8282e-03, 8.8540e-04, 6.2732e-04, 8.5425e-04,\n",
            "         2.5266e-04, 1.8025e-03, 5.1498e-05, 1.3333e-04, 1.2266e-03, 1.0572e-03,\n",
            "         2.3376e-04, 4.7742e-04, 8.5921e-04, 3.7894e-04, 9.6395e-04, 9.9932e-04,\n",
            "         1.0611e-03, 9.6326e-03, 1.2153e-02, 4.1011e-04, 6.8125e-04, 1.4342e-03,\n",
            "         9.7023e-04, 1.2320e-03, 4.3541e-04, 4.5203e-04, 3.6988e-03, 6.8937e-04,\n",
            "         4.5984e-04, 1.6799e-03, 2.9548e-03, 2.5537e-03, 1.1023e-03, 6.1474e-04,\n",
            "         1.4154e-03, 1.1250e-03, 2.4174e-03, 3.9699e-04, 8.6612e-03, 4.8870e-04,\n",
            "         3.5117e-03, 1.0460e-04, 3.9128e-04, 2.3253e-03, 1.5772e-04, 9.8343e-04,\n",
            "         1.7131e-04, 2.5501e-03, 3.7367e-03, 1.1468e-03, 5.0937e-04, 1.0566e-04,\n",
            "         3.5535e-04, 2.5000e-04, 5.4979e-04, 4.2832e-04, 2.6323e-03, 1.0840e-03,\n",
            "         9.0811e-04, 1.3225e-03, 2.3973e-04, 5.4708e-04, 2.1672e-03, 1.4546e-04,\n",
            "         1.3053e-04, 1.0208e-03, 7.5312e-04, 3.7999e-04, 1.5512e-03, 2.0762e-04,\n",
            "         3.1478e-04, 5.0445e-04, 2.6586e-03, 3.2780e-04, 6.7105e-04, 2.8929e-04,\n",
            "         5.5699e-04, 9.1580e-04, 4.7659e-04, 7.9366e-05, 1.1194e-03, 6.2159e-04,\n",
            "         4.5157e-04, 6.8499e-05, 1.6416e-04, 2.1544e-04, 6.3076e-04, 2.1193e-03,\n",
            "         5.8630e-04, 2.6144e-03, 8.7605e-04, 3.6097e-04, 1.6010e-03, 1.0509e-03,\n",
            "         8.7755e-04, 6.2953e-04, 2.5213e-03, 1.7335e-04, 1.9104e-03, 7.5939e-05,\n",
            "         3.5091e-03, 7.6091e-03, 3.6973e-03, 2.6551e-04, 2.4300e-03, 2.6481e-04,\n",
            "         4.9918e-03, 1.3301e-03, 1.4627e-03, 4.2397e-04, 9.9213e-04, 1.1925e-03,\n",
            "         8.1625e-04, 3.4068e-04, 6.2684e-03, 4.5280e-04, 2.9547e-04, 2.0784e-03,\n",
            "         6.3209e-04, 6.2694e-04, 7.5807e-04, 1.9293e-03, 2.4585e-04, 3.2976e-04,\n",
            "         1.4269e-03, 2.9994e-03, 2.1160e-04, 5.0482e-04, 3.9905e-04, 1.8789e-04,\n",
            "         2.6929e-04, 4.7411e-04, 1.9745e-04, 6.2757e-04, 5.2579e-04, 4.9558e-04,\n",
            "         5.2429e-03, 4.8662e-04, 4.3368e-04, 1.3178e-03, 6.4504e-04, 2.7225e-04,\n",
            "         4.0203e-04, 1.1135e-04, 1.1688e-03, 1.4265e-03, 1.5262e-03, 7.3393e-04,\n",
            "         1.4838e-03, 5.9000e-04, 4.5786e-04, 8.2894e-04, 9.8025e-04, 1.3636e-03,\n",
            "         5.9211e-04, 4.5220e-04, 2.3844e-03, 3.0120e-04, 2.9835e-03, 7.0682e-05,\n",
            "         7.3029e-04, 1.0046e-03, 2.7548e-03, 1.3713e-03, 5.4556e-03, 2.2098e-04,\n",
            "         1.8448e-03, 6.4706e-04, 4.0209e-04, 4.4607e-04, 1.0355e-03, 2.8890e-03,\n",
            "         8.1441e-04, 2.0082e-03, 2.2947e-04, 4.8275e-04, 2.6136e-04, 1.7507e-03,\n",
            "         2.5732e-04, 1.6125e-03, 2.5470e-04, 1.2607e-03, 6.8197e-04, 2.5380e-03,\n",
            "         6.8911e-04, 8.2384e-04, 8.4094e-05, 5.0817e-04, 3.5599e-04, 1.5199e-03,\n",
            "         4.4784e-04, 2.0807e-03, 5.6982e-04, 4.8775e-04, 9.5488e-04, 9.8558e-05,\n",
            "         3.6008e-03, 9.7500e-04, 3.3851e-04, 4.4003e-04, 1.4732e-03, 2.1788e-03,\n",
            "         1.3743e-04, 1.8350e-04, 7.0073e-04, 1.5133e-03, 3.6915e-04, 4.2552e-03,\n",
            "         1.2005e-03, 6.5404e-04, 5.7325e-04, 2.0057e-04, 9.2035e-05, 1.1342e-04,\n",
            "         5.3179e-04, 8.3417e-04, 8.2068e-04, 3.9065e-04, 1.0242e-03, 3.0145e-04,\n",
            "         1.3425e-03, 3.6108e-03, 4.5946e-03, 4.2904e-04, 2.3354e-04, 1.1029e-03,\n",
            "         1.3207e-03, 6.1972e-04, 1.5393e-04, 8.4499e-04, 1.6412e-03, 3.2393e-04,\n",
            "         4.3009e-03, 2.7331e-03, 4.2863e-03, 7.5110e-04, 9.2285e-04, 4.2625e-04,\n",
            "         1.6407e-04, 5.3078e-04, 8.7810e-05, 2.6035e-03, 5.1385e-04, 3.0268e-04,\n",
            "         3.5330e-04, 6.7036e-04, 2.8894e-04, 4.0570e-04, 7.2415e-04, 4.2240e-04,\n",
            "         6.4927e-04, 6.5668e-04, 1.6650e-03, 5.3391e-04, 4.9210e-04, 7.2909e-04,\n",
            "         2.7898e-03, 3.9418e-04, 1.9813e-04, 5.3870e-03, 1.9636e-04, 2.5533e-04,\n",
            "         1.8222e-03, 4.7535e-04, 8.3407e-04, 1.5759e-04, 5.3537e-04, 1.6726e-03,\n",
            "         5.1877e-04, 9.9472e-04, 8.4636e-04, 4.3828e-03, 2.0559e-03, 3.2913e-04,\n",
            "         3.1543e-04, 2.4941e-04, 2.5154e-04, 5.2369e-04, 4.1092e-04, 1.6366e-04,\n",
            "         8.7897e-04, 1.5057e-03, 1.1504e-03, 8.8888e-04, 8.6646e-04, 2.3738e-03,\n",
            "         1.0920e-03, 1.3803e-03, 3.8113e-04, 3.0523e-03, 7.5857e-04, 8.5568e-04,\n",
            "         9.8826e-04, 1.1875e-03, 8.4338e-04, 5.4695e-04, 3.2146e-03, 5.3922e-04,\n",
            "         1.4422e-03, 2.0136e-03, 2.3500e-03, 3.4788e-03, 2.0135e-03, 3.2377e-04,\n",
            "         2.0290e-04, 5.0755e-04, 1.1598e-03, 1.6951e-04, 8.5264e-04, 1.3760e-03,\n",
            "         8.9425e-04, 2.9388e-03, 1.3141e-03, 6.0829e-04, 1.7620e-03, 5.8817e-04,\n",
            "         6.3920e-04, 1.5510e-04, 2.1894e-03, 2.4849e-03, 7.9968e-04, 1.7430e-03,\n",
            "         1.6162e-03, 1.2680e-04, 8.9088e-04, 3.3930e-04, 8.1675e-04, 7.4560e-04,\n",
            "         2.6575e-04, 5.6012e-04, 3.1883e-04, 3.0207e-03, 1.2871e-04, 7.1035e-04,\n",
            "         1.2286e-03, 5.4032e-04, 1.2510e-03, 1.4174e-03, 2.4362e-04, 1.2512e-03,\n",
            "         6.5690e-04, 2.2327e-03, 4.8230e-04, 5.7485e-04, 6.0130e-04, 5.5044e-04,\n",
            "         2.0388e-04, 2.4254e-03, 1.1011e-03, 3.4090e-04, 1.4942e-03, 6.5032e-04,\n",
            "         1.0508e-03, 2.1550e-04, 2.8529e-04, 4.8433e-04, 1.7904e-04, 6.1440e-05,\n",
            "         5.3483e-04, 3.2763e-03, 1.2684e-03, 5.2696e-04, 9.6732e-04, 1.9315e-03,\n",
            "         9.6175e-04, 2.8106e-03, 8.4615e-03, 2.6043e-04, 7.5615e-04, 7.6425e-04,\n",
            "         3.4457e-04, 8.6315e-05, 4.0750e-04, 2.1494e-03, 6.8766e-04, 1.0741e-03,\n",
            "         6.6930e-04, 4.7698e-04, 7.7323e-04, 1.9484e-04, 4.4728e-04, 3.1074e-04,\n",
            "         3.0932e-04, 6.1971e-04, 1.4507e-03, 4.2286e-04, 4.7529e-04, 5.5186e-04,\n",
            "         1.3988e-03, 3.3013e-04, 8.9639e-04, 9.2118e-04, 6.9464e-04, 1.8035e-04,\n",
            "         5.2827e-03, 2.0817e-03, 3.4039e-03, 4.8259e-03, 1.9422e-03, 3.1436e-04,\n",
            "         9.4659e-04, 4.3191e-04, 2.8550e-03, 3.1581e-03, 8.1797e-04, 2.9707e-04,\n",
            "         3.7995e-03, 2.7424e-04, 3.6475e-04, 3.7280e-04, 7.6960e-05, 3.3096e-04,\n",
            "         4.4247e-05, 1.2273e-03, 6.4136e-04, 3.7435e-04, 2.7367e-04, 6.1095e-04,\n",
            "         9.2055e-04, 3.6393e-04, 2.8739e-04, 2.1048e-03, 3.8610e-04, 5.2546e-04,\n",
            "         1.0019e-04, 1.1905e-03, 2.4315e-04, 5.8003e-03, 4.6120e-04, 1.5864e-04,\n",
            "         6.2193e-03, 1.3981e-03, 2.4885e-03, 6.2940e-04, 9.2026e-05, 9.2916e-04,\n",
            "         3.1222e-03, 1.2113e-03, 2.4163e-03, 3.6642e-04, 1.8258e-03, 2.5783e-04,\n",
            "         3.9440e-04, 4.6295e-04, 7.7733e-05, 6.9406e-04, 2.0007e-04, 6.3816e-04,\n",
            "         5.7246e-03, 5.4985e-04, 2.2764e-03]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next:tensor([[145]])\n",
            "idx:tensor([[  0, 359,  86, 145]]),logits:tensor([[[ 0.1808, -0.0700, -0.3596,  ..., -0.9559,  2.0515,  0.0096],\n",
            "         [ 0.5750,  0.9197, -0.1923,  ..., -2.0579,  0.9896,  1.8094],\n",
            "         [ 0.3261, -1.0293,  0.9530,  ...,  2.0647, -0.2782,  1.1425],\n",
            "         [-0.7650, -0.4883,  0.7513,  ..., -0.3085, -0.9139, -0.3298]]],\n",
            "       grad_fn=<EmbeddingBackward0>),loss:None\n",
            "logits[:, -1, :]:tensor([[-7.6500e-01, -4.8833e-01,  7.5132e-01,  1.2594e+00, -2.3858e-01,\n",
            "         -3.7800e-01,  8.7599e-01,  9.3611e-01,  2.6757e-01,  5.6076e-01,\n",
            "         -8.1633e-01, -9.3686e-01, -3.6106e-01,  5.6397e-01, -1.9784e+00,\n",
            "          3.4839e-01, -1.6020e-01,  7.1028e-01, -9.3132e-01,  1.2339e+00,\n",
            "          4.1297e-02, -1.8553e+00, -2.6066e-01, -3.2712e-01,  6.4912e-02,\n",
            "          3.6290e-01, -5.4359e-01, -9.2474e-01, -8.9431e-01,  1.6842e-01,\n",
            "          1.3573e+00,  3.5701e-01, -1.4600e+00,  1.4204e-01,  6.2404e-01,\n",
            "          1.0763e+00,  3.8197e-01,  4.7932e-02, -5.0147e-01,  1.8462e+00,\n",
            "          2.1544e-01,  1.0540e+00,  9.0865e-01,  2.3819e+00,  3.2942e-01,\n",
            "         -2.1582e-01, -1.2748e+00,  1.8409e-01,  3.4377e-01,  3.1202e-02,\n",
            "         -2.7242e-01,  1.3107e+00,  1.0216e+00,  1.5557e+00,  1.8196e+00,\n",
            "          2.0622e+00,  6.7736e-01,  2.1886e+00,  2.3344e-01, -1.3671e-02,\n",
            "          6.0632e-01,  1.8610e+00, -1.2299e+00,  8.7633e-02,  7.2949e-01,\n",
            "          4.4099e-01,  1.0689e-01,  8.0779e-01,  7.0790e-01, -3.3720e-01,\n",
            "         -3.8766e-01,  1.7992e+00,  1.0357e+00,  9.1283e-01,  1.3306e-01,\n",
            "          1.8869e-01, -1.5643e+00,  1.7457e+00,  5.1962e-02,  1.0868e+00,\n",
            "         -5.3725e-01, -1.2856e+00,  6.1684e-01, -8.8349e-01, -1.2618e+00,\n",
            "         -5.5665e-01, -8.9285e-01, -1.8295e+00, -1.0613e+00, -5.5010e-01,\n",
            "          1.1843e+00,  7.8351e-01,  2.1627e+00,  7.1432e-01,  1.0947e+00,\n",
            "         -1.0584e+00, -2.9615e-01,  2.9302e-01, -2.9852e-01,  1.5865e+00,\n",
            "          1.6584e+00, -4.6304e-01,  3.4903e-01,  1.5698e+00, -1.2322e+00,\n",
            "          2.2960e-01, -1.3435e-01,  1.4981e+00,  5.5138e-01, -8.4777e-01,\n",
            "          4.2426e-01, -3.1655e-01,  5.1561e-01, -5.6693e-01, -9.2469e-01,\n",
            "         -1.4526e+00,  4.6400e-01, -1.3877e+00,  4.0076e-01,  1.5856e+00,\n",
            "         -4.5121e-01, -6.9504e-01,  7.5264e-01,  1.3945e+00,  1.0457e+00,\n",
            "          1.3606e+00, -3.6798e-01,  1.0736e+00, -1.1101e+00, -9.7050e-01,\n",
            "          5.8583e-01,  3.7926e-01, -3.5173e-02,  2.9543e-01, -4.1250e-01,\n",
            "          9.2457e-01, -1.4095e+00, -1.7300e+00, -5.7919e-01,  1.6387e+00,\n",
            "          5.7576e-01,  8.6062e-01, -9.4884e-01, -7.5343e-01,  3.3413e-01,\n",
            "         -9.0017e-02,  1.4120e+00, -3.3014e-01,  1.4689e+00,  1.1067e+00,\n",
            "          3.1600e-01, -1.0857e+00, -1.0179e+00, -7.9153e-01,  3.2019e-02,\n",
            "         -1.4448e-01, -9.9424e-01,  2.6140e-01,  2.2774e-01,  1.0474e+00,\n",
            "         -9.0254e-02, -8.8644e-03, -7.4797e-02,  1.9574e+00, -1.8541e-01,\n",
            "         -4.7566e-01, -7.7752e-01,  9.1276e-01,  8.2158e-01, -1.5544e-01,\n",
            "         -5.4677e-01, -2.2540e+00,  2.3676e-01, -2.2359e+00, -1.7376e+00,\n",
            "          4.6636e-01, -2.0029e+00, -1.4664e+00,  1.4673e+00, -6.0421e-01,\n",
            "         -9.0846e-01, -1.3433e+00, -8.7338e-01,  1.0121e+00,  6.6274e-01,\n",
            "         -8.8068e-01, -7.8673e-01, -1.1421e+00, -6.4655e-01,  3.7050e-02,\n",
            "          1.6068e+00, -1.3719e+00, -1.2338e-01, -1.6334e+00, -5.1627e-01,\n",
            "         -7.8794e-01,  5.8518e-01, -8.8390e-01, -5.7242e-01, -3.0949e-01,\n",
            "         -7.1067e-01, -2.3556e-01,  1.1550e+00, -1.0303e+00,  7.8845e-01,\n",
            "          1.4660e+00, -2.8650e-01,  1.3312e-01,  5.5140e-01,  3.7820e-01,\n",
            "         -8.6291e-01,  1.7264e-01, -1.2864e+00,  1.4312e+00,  1.2556e+00,\n",
            "          6.8519e-01,  4.9679e-01, -7.7856e-01, -2.0928e-01, -6.0898e-01,\n",
            "         -5.7397e-01, -1.5923e+00, -1.9365e-02, -5.9966e-01,  4.6678e-01,\n",
            "          1.0514e+00,  8.8313e-01,  3.6928e-01,  8.7381e-01,  7.9012e-01,\n",
            "         -7.2335e-01, -3.0280e+00,  7.9042e-01, -1.2127e+00,  1.0943e+00,\n",
            "          1.4134e+00,  4.2442e-01, -5.6454e-01,  4.1177e-01, -8.2215e-03,\n",
            "          1.1432e+00,  3.1767e-01,  2.9048e-01, -7.3766e-01, -1.0845e+00,\n",
            "          6.1964e-02, -1.0065e+00, -4.9462e-01,  2.0624e-01, -1.4495e+00,\n",
            "          7.2082e-01, -2.8426e-01, -1.7172e-01, -1.1328e+00,  2.2810e-01,\n",
            "          1.9462e+00, -1.6351e-01,  8.2835e-01,  5.6553e-01,  2.2804e-02,\n",
            "         -3.4648e-01, -2.1071e+00,  1.7085e+00, -5.9265e-01, -2.5845e-01,\n",
            "         -8.0421e-01, -1.0539e+00,  1.6584e-01, -1.0693e-02, -1.0286e+00,\n",
            "         -6.1037e-01,  9.0740e-01,  7.1763e-01,  1.8630e+00,  9.9211e-03,\n",
            "          1.5091e-01,  2.3198e-01, -2.3498e-01, -1.5704e+00, -1.1434e+00,\n",
            "          7.3846e-01,  3.5669e-01, -1.2026e+00, -1.3892e-01,  2.1443e-01,\n",
            "          2.8125e-01, -1.0551e+00,  6.3400e-01,  1.0826e+00,  5.4103e-01,\n",
            "         -1.1284e+00, -7.2568e-01,  1.1564e+00,  1.0812e+00, -1.4425e+00,\n",
            "          3.5849e-01,  1.0087e+00,  1.3905e-02,  1.2722e+00, -2.0454e+00,\n",
            "          5.2816e-01,  2.5964e-02, -9.0546e-01,  3.8268e-02,  1.2845e+00,\n",
            "         -2.0082e-01,  3.0671e+00,  2.7952e-01, -1.8455e-01, -1.5266e-01,\n",
            "          3.6353e-01, -4.6447e-01,  2.2375e-01,  5.8727e-01, -3.9982e-01,\n",
            "         -1.6212e-01,  3.7626e-01, -2.3583e-01,  6.5618e-01,  1.1832e+00,\n",
            "         -1.0545e+00,  5.9343e-01,  6.5360e-01, -4.6744e-01,  8.4393e-01,\n",
            "          1.8015e-01, -1.4425e+00, -1.0589e+00,  1.0111e+00, -1.6826e+00,\n",
            "          1.5604e+00,  4.9932e-01,  3.9130e-01,  1.5975e+00, -7.4266e-01,\n",
            "          1.2035e-01, -1.2186e+00, -1.2343e+00,  5.6295e-01,  3.2360e-02,\n",
            "         -1.7992e-01,  5.8845e-01, -1.0335e+00, -1.6190e-01, -7.4590e-01,\n",
            "          1.4663e+00,  9.7049e-01, -2.5419e-01, -1.7092e+00, -1.4233e+00,\n",
            "          1.1284e-01,  2.5333e-01, -2.4887e-01, -2.3212e-02, -6.5879e-01,\n",
            "          9.6464e-01,  5.2503e-01,  1.7526e+00,  3.9691e-01, -2.7168e+00,\n",
            "          3.7213e-01,  1.0524e+00, -1.1149e+00, -1.0603e+00,  2.2357e-01,\n",
            "         -2.3400e-01, -3.1448e-01, -6.4764e-01, -1.4501e+00, -3.0934e-01,\n",
            "          1.1570e+00, -2.0531e+00, -4.2277e-01, -4.3491e-02,  6.1532e-01,\n",
            "         -3.5653e-01,  4.0801e-02,  6.2934e-01, -9.0088e-02, -3.0941e-01,\n",
            "          1.8189e+00,  2.4604e-01,  6.8553e-01, -1.2513e+00,  9.8251e-01,\n",
            "          2.0580e+00, -5.7477e-01, -6.3866e-01, -3.4023e-01, -9.0020e-01,\n",
            "          4.0963e-01, -7.6423e-01, -6.4256e-01, -3.4573e-01,  9.1188e-01,\n",
            "          9.0716e-02,  4.6894e-01, -2.2772e-01, -1.2007e+00,  7.5158e-02,\n",
            "         -8.9389e-01,  7.3755e-01,  1.6126e-01,  8.0768e-01,  1.8247e-01,\n",
            "         -5.8865e-01, -4.1631e-01, -2.1423e+00, -1.2651e+00,  3.8134e-01,\n",
            "          8.6118e-01, -1.5245e+00, -2.2147e+00, -5.8529e-01,  1.6901e-01,\n",
            "         -1.4400e+00, -1.1184e-01, -3.1021e-02,  3.2301e-01,  3.8599e-01,\n",
            "         -5.3191e-01, -4.0170e-01, -5.1055e-01, -1.0661e+00,  2.1363e+00,\n",
            "          7.6278e-01, -3.4436e-02,  7.7541e-01,  1.9406e+00,  9.7984e-01,\n",
            "         -2.4873e+00, -3.0290e-01,  1.5116e+00,  2.7936e+00,  9.2102e-01,\n",
            "         -1.5358e-01,  1.1018e-01, -5.6021e-01,  1.0760e-01,  2.2303e+00,\n",
            "         -8.9470e-01, -1.5125e+00,  1.6680e+00,  4.7564e-01,  5.0729e-01,\n",
            "          1.2719e+00, -5.7431e-01,  2.5844e-01, -9.7958e-01, -2.4120e-01,\n",
            "          2.8708e-01,  9.8493e-01, -8.0532e-01, -5.4238e-01,  1.7407e+00,\n",
            "          7.5048e-01, -1.5462e-01, -5.3398e-01, -2.6834e+00,  3.2853e-01,\n",
            "         -1.8028e-01, -1.2200e+00, -1.2845e+00, -1.0764e-02,  2.7684e-01,\n",
            "          9.7378e-01,  9.5953e-01, -1.2558e+00, -1.1859e+00,  2.2020e-01,\n",
            "         -5.2905e-01, -8.2654e-01,  1.7236e+00,  8.2869e-01,  1.5442e-01,\n",
            "          3.4569e-01, -4.2203e-01,  8.8947e-01, -3.6742e-01,  2.1039e-01,\n",
            "          3.3408e-01,  9.3596e-01, -9.4237e-01,  1.0376e+00, -4.1475e-02,\n",
            "         -1.3157e+00,  1.2314e-01,  2.8685e-01, -1.2001e+00, -9.7401e-01,\n",
            "         -1.6184e-01, -7.0157e-01, -4.8564e-02,  2.1438e+00, -2.0239e-03,\n",
            "          1.8488e+00,  9.0713e-01, -2.1137e-01, -5.6409e-01, -5.1886e-01,\n",
            "          1.7592e-01,  1.2094e-01, -1.4334e+00, -1.6841e+00,  8.1885e-01,\n",
            "          5.6925e-01, -1.8828e+00,  4.1076e-01, -1.7124e+00,  5.3905e-01,\n",
            "          5.8292e-01,  4.7322e-02, -8.0476e-01, -1.0576e+00,  1.5730e+00,\n",
            "          3.2515e-01, -1.0594e+00, -1.3041e+00,  3.9815e-03,  3.8187e-01,\n",
            "          1.2977e+00,  4.7581e-01,  4.7432e-01, -7.2647e-01, -6.2760e-01,\n",
            "         -1.4117e-01,  1.0066e+00, -7.9924e-01,  1.9137e+00,  3.0118e-01,\n",
            "          1.0853e+00, -7.7636e-01,  3.9726e-01, -1.2258e+00,  6.9375e-01,\n",
            "          5.7061e-01,  3.0546e-02, -4.0403e-02,  6.3300e-01,  6.3808e-01,\n",
            "         -1.4401e+00,  6.0145e-01, -1.8190e+00, -7.5155e-01, -1.7810e+00,\n",
            "          3.3922e-02, -6.1866e-02,  2.2620e-01,  1.2665e+00, -1.3997e+00,\n",
            "         -2.6736e+00, -1.2688e-01, -7.5714e-01, -1.2960e-01, -8.9913e-01,\n",
            "          7.1588e-01,  9.0423e-01,  1.2061e-01,  5.1975e-02, -4.1988e-01,\n",
            "         -1.0222e-01,  6.9615e-01,  8.3760e-01,  1.2366e+00,  3.1475e-01,\n",
            "          1.0091e+00,  3.7945e-01,  1.0645e-01, -9.4414e-01, -6.6431e-01,\n",
            "          4.4055e-02,  5.2475e-01, -5.2957e-01,  1.1836e-01,  8.0196e-01,\n",
            "          5.2954e-01,  2.5748e-01, -2.1106e+00,  7.3115e-01,  2.3561e-01,\n",
            "         -3.3978e-01,  2.6853e-02, -4.1323e-01, -9.3840e-01,  9.3103e-01,\n",
            "         -7.3427e-01,  1.2738e+00,  8.3574e-01, -3.7060e-02, -7.2732e-01,\n",
            "          1.9549e-01,  1.0783e+00,  4.7991e-01, -2.2648e+00,  3.6483e-01,\n",
            "         -1.6634e+00,  1.7010e+00,  1.9226e+00, -1.1886e+00,  4.4734e-01,\n",
            "         -1.2482e+00, -5.4567e-01, -4.4905e-01, -2.4159e+00,  4.4048e-01,\n",
            "         -1.0080e+00,  2.2031e-01, -3.8325e-01, -1.9333e-01, -4.4063e-03,\n",
            "          1.4291e+00, -9.2728e-01,  7.4820e-03,  1.3261e+00,  2.1044e-01,\n",
            "          3.6812e-01, -6.3993e-02,  1.9860e+00,  1.6924e+00,  4.9153e-01,\n",
            "          2.3876e+00,  1.6371e+00,  6.2079e-01, -6.6741e-01,  9.7303e-01,\n",
            "         -8.4919e-01, -1.5081e+00,  4.0136e-01,  1.1305e+00, -8.5517e-01,\n",
            "         -5.8683e-01, -8.9359e-01,  1.2284e+00,  1.4443e+00, -2.7031e-01,\n",
            "         -8.9763e-01,  1.4855e+00,  1.9206e-01, -1.3458e+00, -2.2669e-01,\n",
            "          8.6747e-02,  3.3710e+00,  3.5055e-01, -2.1281e-01,  4.3186e-01,\n",
            "         -9.5570e-01,  5.6008e-02,  9.7130e-01,  1.6274e+00, -4.2648e-01,\n",
            "          1.7174e+00, -3.4381e-01, -9.1712e-01,  1.1532e+00,  5.9208e-01,\n",
            "         -1.1395e-01, -1.1579e+00, -1.6255e-01, -2.5281e-01,  1.3213e+00,\n",
            "          6.4904e-01, -1.5048e+00, -2.3821e+00,  2.8675e-01, -1.4413e+00,\n",
            "          2.1799e+00, -5.5268e-01, -4.1367e-02,  5.0479e-01,  4.0667e-01,\n",
            "          5.4416e-01, -7.9203e-01, -6.1051e-02, -1.5932e+00, -7.0547e-01,\n",
            "          4.2395e-01, -5.5005e-01, -1.9838e+00,  5.2558e-01, -4.4184e-02,\n",
            "         -3.2116e-01, -2.0922e+00, -2.2701e+00, -1.1473e+00,  1.2014e+00,\n",
            "          5.1620e-01,  2.9814e-01, -4.2540e-01, -1.1021e-01, -5.8589e-01,\n",
            "         -7.4550e-01,  1.1328e+00,  4.9119e-01,  1.1878e+00, -2.2114e+00,\n",
            "         -1.1364e+00, -7.7162e-01, -2.5636e+00,  1.8915e-01,  7.7518e-01,\n",
            "         -1.3171e-01, -5.3710e-01, -1.6927e-01,  6.8873e-01, -2.9028e-01,\n",
            "         -1.9141e+00, -4.4735e-01,  5.9727e-01,  1.6735e+00,  1.9238e-01,\n",
            "          9.6755e-01, -9.3902e-01,  1.1450e+00, -2.2519e+00, -5.5927e-01,\n",
            "         -5.8942e-01,  9.7137e-01,  1.4587e+00,  4.9548e-01,  8.9734e-01,\n",
            "         -1.9830e+00,  8.4952e-01,  1.9187e+00,  1.6692e+00,  1.1296e+00,\n",
            "          4.2713e-01,  1.8058e+00,  2.9440e+00,  2.9934e-01, -4.7298e-01,\n",
            "         -9.0661e-02, -1.4772e+00, -5.7733e-01, -1.1983e+00,  4.1964e-01,\n",
            "          6.9322e-02, -1.6906e-01, -8.9596e-01, -4.4084e-01,  1.4335e+00,\n",
            "          1.1457e+00,  1.4783e-01,  1.5499e+00, -3.3747e+00,  2.3463e+00,\n",
            "          2.1448e+00,  4.7318e-01,  3.3149e-01,  1.3083e+00, -1.0617e+00,\n",
            "         -5.0626e-01, -6.4356e-01, -6.1457e-01,  1.9334e-01, -2.3088e-01,\n",
            "         -3.5250e-01,  6.0338e-01, -4.4240e-01, -3.6294e-01, -8.0094e-01,\n",
            "          1.8233e+00, -2.0777e+00,  4.9113e-01, -5.8798e-01,  4.2996e-01,\n",
            "          6.3315e-01, -1.3778e+00, -4.3061e-01, -8.1079e-01, -2.5688e-01,\n",
            "         -1.3181e-01, -2.3423e-01, -1.2190e-01,  6.9324e-01,  3.0889e-01,\n",
            "         -2.5365e-01, -1.3979e+00,  1.4497e-01,  6.6976e-01,  4.5083e-01,\n",
            "         -3.6550e-01, -2.2633e+00,  1.2347e+00, -1.9863e-02,  5.5359e-01,\n",
            "          1.2645e+00,  2.1368e+00, -8.9356e-01, -7.7523e-01,  7.6985e-01,\n",
            "         -2.8359e-01,  1.9308e+00,  2.3484e-02, -1.5284e-01,  2.8829e-01,\n",
            "         -1.1575e+00, -3.0403e-01, -3.4234e-01, -2.5360e-01,  1.6391e-01,\n",
            "         -1.7600e+00, -2.3763e-01, -2.5778e-01, -4.1668e-01, -4.7462e-01,\n",
            "         -6.9382e-01,  4.0933e-01, -3.5480e-01,  1.3185e+00, -2.7044e-01,\n",
            "         -1.0644e+00, -1.3339e+00,  3.1603e-01,  3.2564e-01, -3.2190e+00,\n",
            "         -3.0294e-01, -6.4977e-01,  1.0902e+00, -3.7323e-01,  8.9796e-01,\n",
            "         -7.1128e-01, -6.6566e-01,  5.3026e-01, -1.5409e+00,  2.6213e-01,\n",
            "          5.7118e-01,  6.2926e-02, -7.2981e-01,  2.4407e-02,  1.3222e+00,\n",
            "         -8.3936e-01, -8.4670e-01, -1.1510e+00, -6.5344e-01, -4.9521e-01,\n",
            "         -2.2865e-01,  2.2138e-01,  1.1802e+00,  2.1580e+00,  1.1521e+00,\n",
            "         -3.0854e-01, -9.1385e-01, -3.2983e-01]], grad_fn=<SliceBackward0>)\n",
            "probs:tensor([[3.3202e-04, 4.3785e-04, 1.5125e-03, 2.5138e-03, 5.6207e-04, 4.8893e-04,\n",
            "         1.7133e-03, 1.8195e-03, 9.3242e-04, 1.2501e-03, 3.1541e-04, 2.7960e-04,\n",
            "         4.9728e-04, 1.2541e-03, 9.8673e-05, 1.0109e-03, 6.0790e-04, 1.4517e-03,\n",
            "         2.8115e-04, 2.4507e-03, 7.4360e-04, 1.1160e-04, 5.4980e-04, 5.1445e-04,\n",
            "         7.6137e-04, 1.0257e-03, 4.1431e-04, 2.8301e-04, 2.9175e-04, 8.4440e-04,\n",
            "         2.7724e-03, 1.0197e-03, 1.6571e-04, 8.2242e-04, 1.3318e-03, 2.0933e-03,\n",
            "         1.0454e-03, 7.4855e-04, 4.3214e-04, 4.5205e-03, 8.8506e-04, 2.0471e-03,\n",
            "         1.7702e-03, 7.7242e-03, 9.9191e-04, 5.7501e-04, 1.9942e-04, 8.5774e-04,\n",
            "         1.0062e-03, 7.3613e-04, 5.4337e-04, 2.6462e-03, 1.9818e-03, 3.3810e-03,\n",
            "         4.4020e-03, 5.6105e-03, 1.4047e-03, 6.3666e-03, 9.0113e-04, 7.0383e-04,\n",
            "         1.3084e-03, 4.5883e-03, 2.0857e-04, 7.7887e-04, 1.4798e-03, 1.1090e-03,\n",
            "         7.9401e-04, 1.6004e-03, 1.4483e-03, 5.0929e-04, 4.8422e-04, 4.3131e-03,\n",
            "         2.0101e-03, 1.7776e-03, 8.1507e-04, 8.6169e-04, 1.4929e-04, 4.0884e-03,\n",
            "         7.5158e-04, 2.1154e-03, 4.1695e-04, 1.9728e-04, 1.3222e-03, 2.9493e-04,\n",
            "         2.0203e-04, 4.0894e-04, 2.9218e-04, 1.1452e-04, 2.4689e-04, 4.1162e-04,\n",
            "         2.3321e-03, 1.5620e-03, 6.2040e-03, 1.4576e-03, 2.1321e-03, 2.4760e-04,\n",
            "         5.3063e-04, 9.5645e-04, 5.2937e-04, 3.4866e-03, 3.7467e-03, 4.4906e-04,\n",
            "         1.0115e-03, 3.4288e-03, 2.0811e-04, 8.9768e-04, 6.2382e-04, 3.1917e-03,\n",
            "         1.2384e-03, 3.0565e-04, 1.0906e-03, 5.1991e-04, 1.1949e-03, 4.0475e-04,\n",
            "         2.8302e-04, 1.6693e-04, 1.1348e-03, 1.7812e-04, 1.0653e-03, 3.4835e-03,\n",
            "         4.5441e-04, 3.5609e-04, 1.5145e-03, 2.8775e-03, 2.0303e-03, 2.7818e-03,\n",
            "         4.9385e-04, 2.0876e-03, 2.3512e-04, 2.7035e-04, 1.2818e-03, 1.0426e-03,\n",
            "         6.8886e-04, 9.5875e-04, 4.7235e-04, 1.7986e-03, 1.7430e-04, 1.2650e-04,\n",
            "         3.9982e-04, 3.6734e-03, 1.2690e-03, 1.6872e-03, 2.7627e-04, 3.3589e-04,\n",
            "         9.9660e-04, 6.5210e-04, 2.9283e-03, 5.1289e-04, 3.0997e-03, 2.1578e-03,\n",
            "         9.7869e-04, 2.4094e-04, 2.5783e-04, 3.2333e-04, 7.3674e-04, 6.1753e-04,\n",
            "         2.6400e-04, 9.2668e-04, 8.9600e-04, 2.0337e-03, 6.5194e-04, 7.0722e-04,\n",
            "         6.6210e-04, 5.0525e-03, 5.9277e-04, 4.4344e-04, 3.2789e-04, 1.7775e-03,\n",
            "         1.6226e-03, 6.1080e-04, 4.1300e-04, 7.4904e-05, 9.0412e-04, 7.6275e-05,\n",
            "         1.2554e-04, 1.1375e-03, 9.6281e-05, 1.6464e-04, 3.0950e-03, 3.8994e-04,\n",
            "         2.8765e-04, 1.8622e-04, 2.9792e-04, 1.9632e-03, 1.3843e-03, 2.9576e-04,\n",
            "         3.2489e-04, 2.2772e-04, 3.7378e-04, 7.4045e-04, 3.5581e-03, 1.8096e-04,\n",
            "         6.3070e-04, 1.3933e-04, 4.2579e-04, 3.2449e-04, 1.2810e-03, 2.9481e-04,\n",
            "         4.0254e-04, 5.2359e-04, 3.5056e-04, 5.6377e-04, 2.2648e-03, 2.5466e-04,\n",
            "         1.5697e-03, 3.0908e-03, 5.3577e-04, 8.1512e-04, 1.2384e-03, 1.0415e-03,\n",
            "         3.0106e-04, 8.4797e-04, 1.9713e-04, 2.9852e-03, 2.5045e-03, 1.4157e-03,\n",
            "         1.1726e-03, 3.2755e-04, 5.7878e-04, 3.8809e-04, 4.0191e-04, 1.4517e-04,\n",
            "         6.9984e-04, 3.9172e-04, 1.1380e-03, 2.0418e-03, 1.7256e-03, 1.0322e-03,\n",
            "         1.7096e-03, 1.5724e-03, 3.4615e-04, 3.4544e-05, 1.5728e-03, 2.1219e-04,\n",
            "         2.1313e-03, 2.9325e-03, 1.0908e-03, 4.0572e-04, 1.0770e-03, 7.0768e-04,\n",
            "         2.2381e-03, 9.8032e-04, 9.5402e-04, 3.4123e-04, 2.4121e-04, 7.5913e-04,\n",
            "         2.6078e-04, 4.3511e-04, 8.7695e-04, 1.6746e-04, 1.4671e-03, 5.3697e-04,\n",
            "         6.0094e-04, 2.2985e-04, 8.9633e-04, 4.9963e-03, 6.0589e-04, 1.6336e-03,\n",
            "         1.2561e-03, 7.2998e-04, 5.0458e-04, 8.6754e-05, 3.9393e-03, 3.9448e-04,\n",
            "         5.5101e-04, 3.1926e-04, 2.4871e-04, 8.4223e-04, 7.0593e-04, 2.5509e-04,\n",
            "         3.8755e-04, 1.7680e-03, 1.4624e-03, 4.5974e-03, 7.2063e-04, 8.2975e-04,\n",
            "         8.9981e-04, 5.6410e-04, 1.4838e-04, 2.2743e-04, 1.4932e-03, 1.0193e-03,\n",
            "         2.1436e-04, 6.2098e-04, 8.8416e-04, 9.4526e-04, 2.4842e-04, 1.3451e-03,\n",
            "         2.1066e-03, 1.2257e-03, 2.3086e-04, 3.4534e-04, 2.2679e-03, 2.1035e-03,\n",
            "         1.6864e-04, 1.0212e-03, 1.9565e-03, 7.2351e-04, 2.5462e-03, 9.2281e-05,\n",
            "         1.2100e-03, 7.3229e-04, 2.8852e-04, 7.4135e-04, 2.5779e-03, 5.8370e-04,\n",
            "         1.5326e-02, 9.4363e-04, 5.9328e-04, 6.1250e-04, 1.0263e-03, 4.4842e-04,\n",
            "         8.9244e-04, 1.2837e-03, 4.7837e-04, 6.0673e-04, 1.0395e-03, 5.6362e-04,\n",
            "         1.3753e-03, 2.3296e-03, 2.4857e-04, 1.2916e-03, 1.3717e-03, 4.4709e-04,\n",
            "         1.6593e-03, 8.5437e-04, 1.6864e-04, 2.4748e-04, 1.9611e-03, 1.3264e-04,\n",
            "         3.3969e-03, 1.1756e-03, 1.0552e-03, 3.5252e-03, 3.3952e-04, 8.0477e-04,\n",
            "         2.1095e-04, 2.0767e-04, 1.2528e-03, 7.3699e-04, 5.9603e-04, 1.2852e-03,\n",
            "         2.5383e-04, 6.0687e-04, 3.3843e-04, 3.0919e-03, 1.8831e-03, 5.5336e-04,\n",
            "         1.2916e-04, 1.7189e-04, 7.9875e-04, 9.1924e-04, 5.5632e-04, 6.9715e-04,\n",
            "         3.6923e-04, 1.8722e-03, 1.2062e-03, 4.1166e-03, 1.0612e-03, 4.7153e-05,\n",
            "         1.0352e-03, 2.0439e-03, 2.3400e-04, 2.4714e-04, 8.9228e-04, 5.6465e-04,\n",
            "         5.2099e-04, 3.7337e-04, 1.6735e-04, 5.2367e-04, 2.2692e-03, 9.1567e-05,\n",
            "         4.6752e-04, 6.8315e-04, 1.3202e-03, 4.9954e-04, 7.4323e-04, 1.3388e-03,\n",
            "         6.5205e-04, 5.2364e-04, 4.3991e-03, 9.1256e-04, 1.4162e-03, 2.0416e-04,\n",
            "         1.9059e-03, 5.5870e-03, 4.0159e-04, 3.7674e-04, 5.0775e-04, 2.9004e-04,\n",
            "         1.0747e-03, 3.3228e-04, 3.7527e-04, 5.0496e-04, 1.7759e-03, 7.8127e-04,\n",
            "         1.1404e-03, 5.6821e-04, 2.1475e-04, 7.6921e-04, 2.9187e-04, 1.4918e-03,\n",
            "         8.3838e-04, 1.6002e-03, 8.5635e-04, 3.9606e-04, 4.7055e-04, 8.3755e-05,\n",
            "         2.0137e-04, 1.0448e-03, 1.6881e-03, 1.5536e-04, 7.7910e-05, 3.9739e-04,\n",
            "         8.4490e-04, 1.6905e-04, 6.3802e-04, 6.9173e-04, 9.8557e-04, 1.0496e-03,\n",
            "         4.1918e-04, 4.7748e-04, 4.2823e-04, 2.4569e-04, 6.0419e-03, 1.5299e-03,\n",
            "         6.8937e-04, 1.5494e-03, 4.9684e-03, 1.9008e-03, 5.9319e-05, 5.2706e-04,\n",
            "         3.2352e-03, 1.1659e-02, 1.7923e-03, 6.1194e-04, 7.9663e-04, 4.0748e-04,\n",
            "         7.9458e-04, 6.6375e-03, 2.9164e-04, 1.5723e-04, 3.7829e-03, 1.1481e-03,\n",
            "         1.1850e-03, 2.5456e-03, 4.0178e-04, 9.2394e-04, 2.6790e-04, 5.6060e-04,\n",
            "         9.5078e-04, 1.9105e-03, 3.1890e-04, 4.1481e-04, 4.0680e-03, 1.5113e-03,\n",
            "         6.1130e-04, 4.1831e-04, 4.8754e-05, 9.9103e-04, 5.9581e-04, 2.1066e-04,\n",
            "         1.9749e-04, 7.0588e-04, 9.4111e-04, 1.8893e-03, 1.8626e-03, 2.0324e-04,\n",
            "         2.1796e-04, 8.8927e-04, 4.2038e-04, 3.1221e-04, 3.9989e-03, 1.6342e-03,\n",
            "         8.3267e-04, 1.0082e-03, 4.6787e-04, 1.7366e-03, 4.9413e-04, 8.8059e-04,\n",
            "         9.9654e-04, 1.8192e-03, 2.7806e-04, 2.0139e-03, 6.8453e-04, 1.9143e-04,\n",
            "         8.0702e-04, 9.5057e-04, 2.1488e-04, 2.6940e-04, 6.0690e-04, 3.5377e-04,\n",
            "         6.7970e-04, 6.0874e-03, 7.1208e-04, 4.5322e-03, 1.7675e-03, 5.7757e-04,\n",
            "         4.0590e-04, 4.2468e-04, 8.5076e-04, 8.0525e-04, 1.7017e-04, 1.3243e-04,\n",
            "         1.6182e-03, 1.2608e-03, 1.0857e-04, 1.0760e-03, 1.2874e-04, 1.2232e-03,\n",
            "         1.2781e-03, 7.4810e-04, 3.1908e-04, 2.4780e-04, 3.4398e-03, 9.8768e-04,\n",
            "         2.4736e-04, 1.9366e-04, 7.1637e-04, 1.0453e-03, 2.6122e-03, 1.1483e-03,\n",
            "         1.1466e-03, 3.4507e-04, 3.8093e-04, 6.1958e-04, 1.9523e-03, 3.2085e-04,\n",
            "         4.8362e-03, 9.6429e-04, 2.1123e-03, 3.2828e-04, 1.0615e-03, 2.0942e-04,\n",
            "         1.4279e-03, 1.2625e-03, 7.3565e-04, 6.8527e-04, 1.3437e-03, 1.3506e-03,\n",
            "         1.6904e-04, 1.3020e-03, 1.1573e-04, 3.3652e-04, 1.2020e-04, 7.3814e-04,\n",
            "         6.7071e-04, 8.9463e-04, 2.5320e-03, 1.7600e-04, 4.9237e-05, 6.2850e-04,\n",
            "         3.3464e-04, 6.2679e-04, 2.9035e-04, 1.4598e-03, 1.7624e-03, 8.0498e-04,\n",
            "         7.5159e-04, 4.6887e-04, 6.4419e-04, 1.4313e-03, 1.6488e-03, 2.4572e-03,\n",
            "         9.7746e-04, 1.9573e-03, 1.0428e-03, 7.9367e-04, 2.7757e-04, 3.6720e-04,\n",
            "         7.4566e-04, 1.2059e-03, 4.2016e-04, 8.0318e-04, 1.5911e-03, 1.2117e-03,\n",
            "         9.2306e-04, 8.6456e-05, 1.4823e-03, 9.0309e-04, 5.0797e-04, 7.3294e-04,\n",
            "         4.7200e-04, 2.7917e-04, 1.8103e-03, 3.4239e-04, 2.5504e-03, 1.6457e-03,\n",
            "         6.8756e-04, 3.4478e-04, 8.6757e-04, 2.0975e-03, 1.1530e-03, 7.4097e-05,\n",
            "         1.0277e-03, 1.3520e-04, 3.9098e-03, 4.8794e-03, 2.1737e-04, 1.1160e-03,\n",
            "         2.0480e-04, 4.1345e-04, 4.5539e-04, 6.3710e-05, 1.1084e-03, 2.6041e-04,\n",
            "         8.8938e-04, 4.8637e-04, 5.8809e-04, 7.1038e-04, 2.9788e-03, 2.8229e-04,\n",
            "         7.1888e-04, 2.6873e-03, 8.8064e-04, 1.0310e-03, 6.6929e-04, 5.1992e-03,\n",
            "         3.8762e-03, 1.1665e-03, 7.7684e-03, 3.6678e-03, 1.3274e-03, 3.6606e-04,\n",
            "         1.8879e-03, 3.0522e-04, 1.5792e-04, 1.0659e-03, 2.2099e-03, 3.0340e-04,\n",
            "         3.9678e-04, 2.9196e-04, 2.4372e-03, 3.0244e-03, 5.4452e-04, 2.9078e-04,\n",
            "         3.1517e-03, 8.6461e-04, 1.8576e-04, 5.6879e-04, 7.7818e-04, 2.0768e-02,\n",
            "         1.0131e-03, 5.7675e-04, 1.0989e-03, 2.7438e-04, 7.5462e-04, 1.8847e-03,\n",
            "         3.6324e-03, 4.6579e-04, 3.9742e-03, 5.0593e-04, 2.8517e-04, 2.2607e-03,\n",
            "         1.2899e-03, 6.3668e-04, 2.2414e-04, 6.0647e-04, 5.5413e-04, 2.6745e-03,\n",
            "         1.3655e-03, 1.5845e-04, 6.5896e-05, 9.5047e-04, 1.6883e-04, 6.3115e-03,\n",
            "         4.1056e-04, 6.8461e-04, 1.1820e-03, 1.0716e-03, 1.2295e-03, 3.2317e-04,\n",
            "         6.7126e-04, 1.4504e-04, 3.5239e-04, 1.0903e-03, 4.1164e-04, 9.8138e-05,\n",
            "         1.2069e-03, 6.8268e-04, 5.1752e-04, 8.8057e-05, 7.3706e-05, 2.2653e-04,\n",
            "         2.3724e-03, 1.1956e-03, 9.6136e-04, 4.6629e-04, 6.3906e-04, 3.9715e-04,\n",
            "         3.3856e-04, 2.2150e-03, 1.1661e-03, 2.3403e-03, 7.8162e-05, 2.2903e-04,\n",
            "         3.2983e-04, 5.4962e-05, 8.6209e-04, 1.5490e-03, 6.2547e-04, 4.1701e-04,\n",
            "         6.0241e-04, 1.4208e-03, 5.3375e-04, 1.0522e-04, 4.5617e-04, 1.2966e-03,\n",
            "         3.8036e-03, 8.6488e-04, 1.8776e-03, 2.7899e-04, 2.2422e-03, 7.5058e-05,\n",
            "         4.0787e-04, 3.9575e-04, 1.8848e-03, 3.0684e-03, 1.1711e-03, 1.7503e-03,\n",
            "         9.8220e-05, 1.6686e-03, 4.8608e-03, 3.7875e-03, 2.2080e-03, 1.0937e-03,\n",
            "         4.3415e-03, 1.3551e-02, 9.6251e-04, 4.4462e-04, 6.5168e-04, 1.6287e-04,\n",
            "         4.0057e-04, 2.1527e-04, 1.0856e-03, 7.6474e-04, 6.0254e-04, 2.9127e-04,\n",
            "         4.5915e-04, 2.9921e-03, 2.2437e-03, 8.2719e-04, 3.3613e-03, 2.4422e-05,\n",
            "         7.4538e-03, 6.0935e-03, 1.1453e-03, 9.9396e-04, 2.6399e-03, 2.4678e-04,\n",
            "         4.3007e-04, 3.7490e-04, 3.8592e-04, 8.6571e-04, 5.6642e-04, 5.0155e-04,\n",
            "         1.3045e-03, 4.5843e-04, 4.9634e-04, 3.2030e-04, 4.4182e-03, 8.9350e-05,\n",
            "         1.1660e-03, 3.9632e-04, 1.0968e-03, 1.3439e-03, 1.7990e-04, 4.6387e-04,\n",
            "         3.1716e-04, 5.5188e-04, 6.2541e-04, 5.6452e-04, 6.3164e-04, 1.4272e-03,\n",
            "         9.7175e-04, 5.5366e-04, 1.7633e-04, 8.2483e-04, 1.3941e-03, 1.1199e-03,\n",
            "         4.9508e-04, 7.4211e-05, 2.4527e-03, 6.9949e-04, 1.2412e-03, 2.5269e-03,\n",
            "         6.0452e-03, 2.9197e-04, 3.2865e-04, 1.5408e-03, 5.3734e-04, 4.9196e-03,\n",
            "         7.3047e-04, 6.1239e-04, 9.5194e-04, 2.2423e-04, 5.2646e-04, 5.0667e-04,\n",
            "         5.5370e-04, 8.4060e-04, 1.2276e-04, 5.6260e-04, 5.5138e-04, 4.7038e-04,\n",
            "         4.4389e-04, 3.5652e-04, 1.0744e-03, 5.0040e-04, 2.6671e-03, 5.4445e-04,\n",
            "         2.4613e-04, 1.8798e-04, 9.7871e-04, 9.8816e-04, 2.8537e-05, 5.2703e-04,\n",
            "         3.7257e-04, 2.1227e-03, 4.9126e-04, 1.7514e-03, 3.5035e-04, 3.6670e-04,\n",
            "         1.2125e-03, 1.5283e-04, 9.2735e-04, 1.2632e-03, 7.5986e-04, 3.4392e-04,\n",
            "         7.3115e-04, 2.6768e-03, 3.0823e-04, 3.0598e-04, 2.2571e-04, 3.7121e-04,\n",
            "         4.3485e-04, 5.6768e-04, 8.9032e-04, 2.3226e-03, 6.1748e-03, 2.2581e-03,\n",
            "         5.2409e-04, 2.8610e-04, 5.1305e-04]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next:tensor([[708]])\n",
            "idx:tensor([[  0, 359,  86, 145, 708]]),logits:tensor([[[ 0.1808, -0.0700, -0.3596,  ..., -0.9559,  2.0515,  0.0096],\n",
            "         [ 0.5750,  0.9197, -0.1923,  ..., -2.0579,  0.9896,  1.8094],\n",
            "         [ 0.3261, -1.0293,  0.9530,  ...,  2.0647, -0.2782,  1.1425],\n",
            "         [-0.7650, -0.4883,  0.7513,  ..., -0.3085, -0.9139, -0.3298],\n",
            "         [ 1.5254, -0.1220,  1.9917,  ..., -0.0333, -0.9517, -1.0109]]],\n",
            "       grad_fn=<EmbeddingBackward0>),loss:None\n",
            "logits[:, -1, :]:tensor([[ 1.5254, -0.1220,  1.9917, -0.5003,  0.7685,  0.5241,  0.4330, -0.8145,\n",
            "         -0.1578, -0.2519, -0.0089,  0.7947, -2.7295, -0.6831, -0.0129, -2.7510,\n",
            "         -0.7174,  0.3293,  0.0263,  0.6650,  0.1451, -0.1139,  0.8532,  0.4004,\n",
            "          0.8387,  0.8240,  1.2248, -1.4033, -1.2512,  0.3253, -0.5735,  2.3052,\n",
            "         -1.2617, -0.1625,  0.3246,  1.5785, -0.5954,  0.2282,  1.7704, -0.6312,\n",
            "          0.3567,  0.0078,  0.4035, -2.0825,  0.2497,  1.2022,  0.3105, -0.2484,\n",
            "          0.5569,  0.4485,  0.6771, -0.2599, -1.4686,  0.4557, -0.7738, -0.6275,\n",
            "         -0.1729,  2.4055,  0.0541,  0.7358,  0.9347,  1.4909, -0.2540, -1.4221,\n",
            "         -1.7720,  0.6711, -0.6229, -1.4099, -0.4827,  0.5729,  0.0661, -0.0621,\n",
            "         -1.0822, -0.6537,  1.2964, -0.0145,  1.5102,  0.0421,  0.6145,  0.1177,\n",
            "         -1.2602, -1.7665,  0.7861,  0.6334, -0.6649,  0.5579,  1.0199,  1.2590,\n",
            "          0.4035,  0.2425, -1.1970, -1.9777,  0.6401,  0.9361, -0.4781,  0.5392,\n",
            "         -1.2665,  0.4594, -0.3545,  0.3857, -1.1759, -0.6425,  0.2505,  0.8715,\n",
            "         -1.4326, -0.8134,  1.0503,  2.1118,  0.3684,  0.1741, -1.4818,  0.6350,\n",
            "         -0.6168, -0.7831, -0.3855, -1.0637, -0.7747, -1.2068, -1.0158,  1.6224,\n",
            "         -0.9947, -0.5321, -2.3426,  1.4703,  0.7998, -1.1381,  0.7395,  0.1870,\n",
            "          2.0718, -0.4641, -0.0293, -1.2244,  0.0575,  1.5204,  0.4387,  0.2901,\n",
            "         -0.3643, -0.4206,  1.7455,  0.4108,  0.8237, -2.6276, -0.0977,  0.3975,\n",
            "          0.1890, -1.6753,  0.0609,  0.7647, -0.7140,  1.8246,  0.5058, -0.5650,\n",
            "         -1.5726,  1.0227, -0.6600,  0.3798,  0.8057, -1.4116,  0.9427, -0.2489,\n",
            "         -0.7507, -1.2759, -1.2098,  0.9486, -1.5816, -0.7672, -0.1451,  0.4560,\n",
            "          2.6047,  1.0721, -1.3986, -0.8362, -0.1548, -0.0392,  1.0033,  0.2137,\n",
            "          0.3196,  0.8226, -1.2236,  0.1264,  0.5410, -0.6125,  1.0700,  0.5785,\n",
            "          0.2705, -0.4475,  0.0903,  0.2684, -0.3653, -0.1518, -0.9117,  0.2149,\n",
            "          0.9110, -1.8910, -0.3407, -0.1435,  1.6564, -1.0380,  0.1322, -0.2339,\n",
            "          1.5829, -0.0745,  0.3281,  0.2184,  0.5380, -0.6478,  0.8370,  0.0759,\n",
            "         -0.5385,  1.9681, -1.5242,  0.7216, -1.0103, -1.5618,  1.1295,  0.8954,\n",
            "         -0.1613, -0.5209, -2.1647, -1.7601, -1.3886,  0.1724,  0.0099,  0.5377,\n",
            "          0.9429, -0.1655, -0.6621,  0.7489, -0.0844,  0.1536, -0.1855,  0.7443,\n",
            "         -1.6887,  0.2660, -0.2297, -1.3321,  0.6042,  2.4083,  0.2485, -1.4683,\n",
            "         -0.5280, -0.9919, -0.9333,  1.3040, -0.4429,  0.7967,  0.3328, -0.0739,\n",
            "          1.3908, -0.2207,  1.3228,  1.7885,  0.5476,  0.0220, -0.3734, -0.4879,\n",
            "         -0.5731,  0.0577,  0.3456, -0.4110, -0.1240,  1.5479, -1.3402, -0.2701,\n",
            "          0.7959,  1.3759,  0.5881,  0.2679,  1.7947, -1.0501,  0.2635, -0.1462,\n",
            "         -0.3524,  0.6838, -1.4036, -0.6902, -0.1936, -0.3706,  0.4583,  2.0024,\n",
            "          0.9842,  0.8803,  1.0780, -0.6730,  0.8241,  0.4610,  0.0519, -0.3161,\n",
            "         -0.4545, -1.1729,  0.0464, -1.9912,  0.4347,  0.2211, -0.2737,  0.9833,\n",
            "         -0.8782, -0.9799,  1.0633,  0.7247,  2.3483,  0.3381, -0.4864,  0.4253,\n",
            "         -0.2928,  0.1097, -0.7042,  0.8541, -0.3043, -0.9092, -0.1448,  1.3161,\n",
            "         -1.1542,  0.2681, -0.9559,  0.2945, -0.1545, -0.4957,  0.0880,  2.1411,\n",
            "         -1.0053,  2.6720, -1.3643, -0.9962, -0.3762,  0.5783, -0.0859,  1.9297,\n",
            "         -0.9494,  0.0520, -0.0109,  1.4605, -0.9524,  1.7303, -2.4689, -1.2207,\n",
            "          0.3767, -1.0496,  0.3582,  2.5483,  0.1821, -0.4106,  0.5027,  0.8134,\n",
            "         -0.2187, -0.6123, -1.5376,  0.4648,  0.5209,  0.1223,  0.1320,  0.7320,\n",
            "         -3.0907,  0.6040,  1.0269,  2.0520,  0.9534,  2.0273, -0.4198,  1.4124,\n",
            "          0.4313,  1.5045,  0.6375,  1.9300,  0.7297, -0.0933,  0.0263, -0.3079,\n",
            "         -0.5592, -0.6421, -1.3634,  2.1835,  0.7598,  0.7594,  0.6824, -0.8215,\n",
            "         -0.9645, -0.8915,  0.1356, -0.9632, -0.6559, -0.0759, -1.2037, -1.1276,\n",
            "          0.0721, -0.8953,  0.2470,  0.6919,  0.2567, -0.6877,  1.2006, -1.0255,\n",
            "         -1.4923, -1.3268, -0.2789,  0.7393,  0.0156,  0.0114, -1.7799, -1.0379,\n",
            "          0.1769, -0.8329, -0.4974, -1.2711,  0.3666,  0.3892, -0.2686,  0.9293,\n",
            "          0.3903,  0.0149,  1.4390, -0.0333, -0.4289,  1.0419,  0.8876,  0.7724,\n",
            "          0.9619, -0.8155, -0.1481,  1.5610,  0.5985,  0.4333, -0.0504, -0.0810,\n",
            "          0.1792, -0.2930,  1.2486, -1.3811, -0.5880, -0.4057,  0.8438,  1.1200,\n",
            "          0.4455, -0.8226,  0.8082,  0.6089,  0.2145, -0.1560, -0.8071, -0.1982,\n",
            "         -0.7350,  0.6271,  1.8066,  0.4454, -0.0080, -1.3871, -1.7676,  0.5640,\n",
            "         -0.6202,  1.0505, -1.8956,  0.2074,  0.9023,  0.5785,  0.9380,  0.6652,\n",
            "         -0.5461,  0.2396,  0.4401,  1.1797, -1.1757, -0.8634,  1.5817, -0.7322,\n",
            "         -0.5633,  0.2387, -0.2276,  0.7931, -0.4340,  0.2079, -0.1914, -0.7292,\n",
            "         -0.6445, -0.0312, -0.4031, -0.2183, -0.4564, -0.3638, -0.5628, -0.7750,\n",
            "         -0.4336,  1.8007,  0.0385,  1.0516,  0.6140,  0.1524,  1.5946,  0.0467,\n",
            "          0.1836,  0.6159, -0.7075, -2.3415,  0.8821, -1.2067, -1.7823, -2.0301,\n",
            "          0.1413,  0.4579, -0.3843, -0.9874,  0.7730, -1.1693, -0.0883,  1.5047,\n",
            "          0.9547,  0.3365, -0.3235, -0.9643, -0.1335,  0.2870,  0.5373,  0.3556,\n",
            "          0.5776,  0.2363,  1.3138, -0.6628, -2.1187, -0.4166,  1.7410,  2.3828,\n",
            "          1.6375,  0.7606,  1.4591, -0.0984,  0.7423, -0.5474, -2.0205,  1.2093,\n",
            "          0.9138,  1.5784,  0.3877, -0.4546,  1.2222, -0.1747, -0.4115,  1.6196,\n",
            "          0.3500,  0.8060,  1.0189,  1.7155,  0.0050,  0.0168, -1.3978,  0.8468,\n",
            "          0.2363, -0.6291, -1.6238,  0.6151, -0.8614,  1.0594,  0.4919,  0.8022,\n",
            "          0.8068,  0.2516, -0.8334,  0.1931,  0.8782,  0.3447, -1.0128, -1.2883,\n",
            "          0.2418,  0.3151,  1.2284,  1.3158,  1.2580, -0.3931, -0.2786, -1.7540,\n",
            "         -1.1682, -0.1542, -0.6206, -0.0274, -1.1222, -1.8311,  0.7305,  1.6351,\n",
            "         -0.4839, -0.0116, -0.0637,  0.0035,  1.4015, -1.3459, -0.9004,  1.0540,\n",
            "          1.1049, -0.7507, -2.3400, -0.4511, -0.5407,  0.3653,  0.0992,  1.2689,\n",
            "         -1.1039,  0.1820, -1.3028,  0.6695, -0.3468,  2.2084, -1.3870,  0.1934,\n",
            "         -0.8872, -0.2312,  0.7326, -1.5880, -0.7599, -0.2915, -0.7582, -0.2129,\n",
            "          0.4684,  0.0269,  0.3167, -1.5521,  0.7104, -0.9353,  1.9531, -0.5164,\n",
            "         -0.6863,  0.5090, -0.1902, -1.4598,  0.9204,  0.1691,  0.6121,  0.6245,\n",
            "          0.7557, -0.8408, -0.3181,  0.9234, -0.5897, -2.3336, -0.3968,  0.6536,\n",
            "          0.1972, -1.4427,  0.7670,  0.7811, -1.4435,  0.0191,  0.0621,  0.9379,\n",
            "         -2.0978,  0.5902, -0.9537,  1.2452, -0.3274,  0.3545,  0.8740, -1.1026,\n",
            "         -3.0990, -0.9810,  1.3917,  0.1161,  1.6420, -0.0995,  0.1943,  0.1770,\n",
            "          0.0665, -1.3258,  0.0260, -0.5007,  3.2967, -0.5562, -1.1265, -1.5680,\n",
            "         -0.4247,  0.2675,  0.2588, -0.3614,  0.6323,  0.8915, -1.5803,  0.0771,\n",
            "         -1.7230, -0.7697, -1.7691,  0.1071,  1.0482,  1.7487, -0.0051,  1.6783,\n",
            "         -0.9802, -0.4894,  1.3761,  0.1428, -0.4083,  1.2133, -0.0535, -0.2191,\n",
            "         -1.5617, -0.0075, -1.8277, -0.4312, -0.1021, -0.9467,  0.5504, -0.3357,\n",
            "         -1.4518, -1.9738, -0.3903, -0.8985, -0.9322, -0.6686,  0.4724,  0.9904,\n",
            "          0.5896, -1.4752, -1.8423,  0.2761,  0.6525, -0.1843, -1.9498, -0.5521,\n",
            "          0.7100, -0.9073,  0.2841, -1.8509, -0.3119,  1.6334, -1.6380, -1.1773,\n",
            "          0.2364, -0.2598,  0.0233,  0.7096, -1.5399,  1.6045, -1.6299,  0.9714,\n",
            "          1.2711, -0.4526, -0.4897, -2.0577, -0.0544, -1.0184, -2.6118,  0.7507,\n",
            "          0.1382, -0.7649,  0.8199, -0.7868, -0.6777, -0.5327, -1.0084, -1.5613,\n",
            "          0.9522,  0.6662,  0.5341, -0.8061,  0.6856, -1.7476, -1.1779, -0.8122,\n",
            "          0.2616, -0.5104,  0.6831, -0.8893, -0.2348, -0.0155, -0.3477, -1.2084,\n",
            "          0.5752, -0.0418, -0.4914,  0.5882, -1.1045,  0.4976,  0.4640,  2.4192,\n",
            "          1.4351,  0.8998,  0.3097,  0.4216, -0.4183,  0.1353, -0.4811, -0.2407,\n",
            "         -1.7793, -0.1046,  1.5182, -0.5582, -1.7874, -0.1675,  1.3154,  1.3156,\n",
            "          0.4915,  1.4505,  0.0349, -0.6067,  0.0526, -0.4247,  0.6800,  0.0431,\n",
            "          0.7908,  0.1995, -1.1673, -1.1695,  0.7822, -1.0880, -1.5736,  0.2529,\n",
            "         -0.7869,  0.0828,  1.6080,  0.0344, -0.2639,  0.0355, -1.6225,  0.0813,\n",
            "         -1.5553,  1.2503,  2.0130,  1.7619,  0.3399, -0.1231,  0.0051, -0.9212,\n",
            "         -0.3805,  2.2972,  1.1295,  0.5246,  0.8324, -0.7169,  0.2680,  0.4636,\n",
            "         -0.6588, -0.1699, -0.6497,  1.2189, -0.7370,  1.0625, -0.9724, -0.3345,\n",
            "          2.0449,  1.6078,  1.4670,  0.8825,  0.9378,  1.4501,  0.2373, -0.7120,\n",
            "         -0.0333, -0.9517, -1.0109]], grad_fn=<SliceBackward0>)\n",
            "probs:tensor([[3.3324e-03, 6.4166e-04, 5.3117e-03, 4.3953e-04, 1.5632e-03, 1.2243e-03,\n",
            "         1.1177e-03, 3.2101e-04, 6.1908e-04, 5.6349e-04, 7.1848e-04, 1.6047e-03,\n",
            "         4.7302e-05, 3.6611e-04, 7.1556e-04, 4.6292e-05, 3.5375e-04, 1.0076e-03,\n",
            "         7.4420e-04, 1.4095e-03, 8.3810e-04, 6.4687e-04, 1.7015e-03, 1.0818e-03,\n",
            "         1.6769e-03, 1.6524e-03, 2.4670e-03, 1.7816e-04, 2.0744e-04, 1.0035e-03,\n",
            "         4.0851e-04, 7.2681e-03, 2.0526e-04, 6.1616e-04, 1.0028e-03, 3.5141e-03,\n",
            "         3.9967e-04, 9.1073e-04, 4.2576e-03, 3.8559e-04, 1.0356e-03, 7.3057e-04,\n",
            "         1.0852e-03, 9.0331e-05, 9.3049e-04, 2.4120e-03, 9.8877e-04, 5.6543e-04,\n",
            "         1.2651e-03, 1.1352e-03, 1.4267e-03, 5.5897e-04, 1.6691e-04, 1.1434e-03,\n",
            "         3.3436e-04, 3.8703e-04, 6.0977e-04, 8.0344e-03, 7.6519e-04, 1.5130e-03,\n",
            "         1.8458e-03, 3.2192e-03, 5.6231e-04, 1.7485e-04, 1.2323e-04, 1.4181e-03,\n",
            "         3.8881e-04, 1.7700e-04, 4.4735e-04, 1.2855e-03, 7.7439e-04, 6.8121e-04,\n",
            "         2.4563e-04, 3.7704e-04, 2.6503e-03, 7.1444e-04, 3.2821e-03, 7.5602e-04,\n",
            "         1.3401e-03, 8.1539e-04, 2.0557e-04, 1.2391e-04, 1.5910e-03, 1.3657e-03,\n",
            "         3.7284e-04, 1.2664e-03, 2.0101e-03, 2.5529e-03, 1.0852e-03, 9.2385e-04,\n",
            "         2.1899e-04, 1.0031e-04, 1.3749e-03, 1.8484e-03, 4.4938e-04, 1.2430e-03,\n",
            "         2.0429e-04, 1.1475e-03, 5.0853e-04, 1.0660e-03, 2.2365e-04, 3.8126e-04,\n",
            "         9.3121e-04, 1.7328e-03, 1.7302e-04, 3.2136e-04, 2.0722e-03, 5.9899e-03,\n",
            "         1.0478e-03, 8.6271e-04, 1.6471e-04, 1.3679e-03, 3.9119e-04, 3.3126e-04,\n",
            "         4.9302e-04, 2.5020e-04, 3.3406e-04, 2.1685e-04, 2.6249e-04, 3.6718e-03,\n",
            "         2.6808e-04, 4.2575e-04, 6.9644e-05, 3.1538e-03, 1.6130e-03, 2.3227e-04,\n",
            "         1.5186e-03, 8.7392e-04, 5.7548e-03, 4.5575e-04, 7.0397e-04, 2.1308e-04,\n",
            "         7.6782e-04, 3.3156e-03, 1.1241e-03, 9.6881e-04, 5.0355e-04, 4.7597e-04,\n",
            "         4.1528e-03, 1.0931e-03, 1.6519e-03, 5.2374e-05, 6.5740e-04, 1.0787e-03,\n",
            "         8.7568e-04, 1.3573e-04, 7.7038e-04, 1.5572e-03, 3.5498e-04, 4.4943e-03,\n",
            "         1.2021e-03, 4.1198e-04, 1.5042e-04, 2.0156e-03, 3.7465e-04, 1.0598e-03,\n",
            "         1.6225e-03, 1.7670e-04, 1.8607e-03, 5.6514e-04, 3.4218e-04, 2.0238e-04,\n",
            "         2.1620e-04, 1.8717e-03, 1.4907e-04, 3.3658e-04, 6.2695e-04, 1.1436e-03,\n",
            "         9.8056e-03, 2.1177e-03, 1.7901e-04, 3.1412e-04, 6.2089e-04, 6.9698e-04,\n",
            "         1.9769e-03, 8.9757e-04, 9.9781e-04, 1.6502e-03, 2.1324e-04, 8.2252e-04,\n",
            "         1.2451e-03, 3.9290e-04, 2.1132e-03, 1.2927e-03, 9.5001e-04, 4.6337e-04,\n",
            "         7.9335e-04, 9.4807e-04, 5.0307e-04, 6.2278e-04, 2.9129e-04, 8.9862e-04,\n",
            "         1.8027e-03, 1.0940e-04, 5.1560e-04, 6.2798e-04, 3.7987e-03, 2.5671e-04,\n",
            "         8.2731e-04, 5.7370e-04, 3.5295e-03, 6.7283e-04, 1.0063e-03, 9.0180e-04,\n",
            "         1.2414e-03, 3.7924e-04, 1.6741e-03, 7.8202e-04, 4.2305e-04, 5.1882e-03,\n",
            "         1.5787e-04, 1.4916e-03, 2.6394e-04, 1.5206e-04, 2.2428e-03, 1.7747e-03,\n",
            "         6.1688e-04, 4.3059e-04, 8.3202e-05, 1.2470e-04, 1.8080e-04, 8.6129e-04,\n",
            "         7.3206e-04, 1.2410e-03, 1.8610e-03, 6.1431e-04, 3.7386e-04, 1.5328e-03,\n",
            "         6.6618e-04, 8.4525e-04, 6.0215e-04, 1.5259e-03, 1.3393e-04, 9.4574e-04,\n",
            "         5.7613e-04, 1.9132e-04, 1.3264e-03, 8.0573e-03, 9.2940e-04, 1.6695e-04,\n",
            "         4.2752e-04, 2.6883e-04, 2.8505e-04, 2.6704e-03, 4.6548e-04, 1.6079e-03,\n",
            "         1.0111e-03, 6.7327e-04, 2.9125e-03, 5.8130e-04, 2.7211e-03, 4.3352e-03,\n",
            "         1.2534e-03, 7.4097e-04, 4.9900e-04, 4.4503e-04, 4.0869e-04, 7.6794e-04,\n",
            "         1.0241e-03, 4.8060e-04, 6.4033e-04, 3.4082e-03, 1.8976e-04, 5.5333e-04,\n",
            "         1.6066e-03, 2.8695e-03, 1.3052e-03, 9.4759e-04, 4.3622e-03, 2.5364e-04,\n",
            "         9.4343e-04, 6.2631e-04, 5.0958e-04, 1.4363e-03, 1.7812e-04, 3.6350e-04,\n",
            "         5.9731e-04, 5.0041e-04, 1.1463e-03, 5.3688e-03, 1.9395e-03, 1.7481e-03,\n",
            "         2.1304e-03, 3.6981e-04, 1.6526e-03, 1.1494e-03, 7.6353e-04, 5.2841e-04,\n",
            "         4.6015e-04, 2.2433e-04, 7.5929e-04, 9.8972e-05, 1.1196e-03, 9.0424e-04,\n",
            "         5.5134e-04, 1.9378e-03, 3.0121e-04, 2.7208e-04, 2.0993e-03, 1.4962e-03,\n",
            "         7.5877e-03, 1.0165e-03, 4.4567e-04, 1.1091e-03, 5.4087e-04, 8.0895e-04,\n",
            "         3.5846e-04, 1.7030e-03, 5.3469e-04, 2.9202e-04, 6.2715e-04, 2.7031e-03,\n",
            "         2.2856e-04, 9.4780e-04, 2.7869e-04, 9.7307e-04, 6.2113e-04, 4.4157e-04,\n",
            "         7.9157e-04, 6.1681e-03, 2.6527e-04, 1.0489e-02, 1.8526e-04, 2.6768e-04,\n",
            "         4.9759e-04, 1.2925e-03, 6.6519e-04, 4.9927e-03, 2.8051e-04, 7.6360e-04,\n",
            "         7.1701e-04, 3.1229e-03, 2.7966e-04, 4.0901e-03, 6.1379e-05, 2.1387e-04,\n",
            "         1.0565e-03, 2.5375e-04, 1.0371e-03, 9.2675e-03, 8.6966e-04, 4.8077e-04,\n",
            "         1.1984e-03, 1.6351e-03, 5.8249e-04, 3.9295e-04, 1.5577e-04, 1.1538e-03,\n",
            "         1.2204e-03, 8.1915e-04, 8.2714e-04, 1.5071e-03, 3.2960e-05, 1.3262e-03,\n",
            "         2.0241e-03, 5.6419e-03, 1.8808e-03, 5.5044e-03, 4.7640e-04, 2.9762e-03,\n",
            "         1.1158e-03, 3.2633e-03, 1.3713e-03, 4.9939e-03, 1.5038e-03, 6.6033e-04,\n",
            "         7.4423e-04, 5.3277e-04, 4.1440e-04, 3.8143e-04, 1.8541e-04, 6.4347e-03,\n",
            "         1.5496e-03, 1.5490e-03, 1.4342e-03, 3.1877e-04, 2.7631e-04, 2.9722e-04,\n",
            "         8.3011e-04, 2.7665e-04, 3.7619e-04, 6.7193e-04, 2.1753e-04, 2.3471e-04,\n",
            "         7.7909e-04, 2.9609e-04, 9.2798e-04, 1.4480e-03, 9.3705e-04, 3.6443e-04,\n",
            "         2.4081e-03, 2.5994e-04, 1.6299e-04, 1.9233e-04, 5.4843e-04, 1.5183e-03,\n",
            "         7.3631e-04, 7.3316e-04, 1.2225e-04, 2.5674e-04, 8.6515e-04, 3.1518e-04,\n",
            "         4.4082e-04, 2.0335e-04, 1.0459e-03, 1.0698e-03, 5.5413e-04, 1.8359e-03,\n",
            "         1.0710e-03, 7.3577e-04, 3.0566e-03, 7.0114e-04, 4.7206e-04, 2.0548e-03,\n",
            "         1.7609e-03, 1.5693e-03, 1.8968e-03, 3.2068e-04, 6.2512e-04, 3.4532e-03,\n",
            "         1.3188e-03, 1.1181e-03, 6.8927e-04, 6.6847e-04, 8.6714e-04, 5.4075e-04,\n",
            "         2.5266e-03, 1.8216e-04, 4.0262e-04, 4.8316e-04, 1.6855e-03, 2.2217e-03,\n",
            "         1.1317e-03, 3.1844e-04, 1.6266e-03, 1.3326e-03, 8.9829e-04, 6.2019e-04,\n",
            "         3.2341e-04, 5.9458e-04, 3.4757e-04, 1.3571e-03, 4.4142e-03, 1.1316e-03,\n",
            "         7.1909e-04, 1.8108e-04, 1.2376e-04, 1.2742e-03, 3.8987e-04, 2.0724e-03,\n",
            "         1.0890e-04, 8.9193e-04, 1.7870e-03, 1.2928e-03, 1.8519e-03, 1.4098e-03,\n",
            "         4.1986e-04, 9.2111e-04, 1.1256e-03, 2.3583e-03, 2.2370e-04, 3.0571e-04,\n",
            "         3.5252e-03, 3.4854e-04, 4.1267e-04, 9.2030e-04, 5.7730e-04, 1.6022e-03,\n",
            "         4.6965e-04, 8.9238e-04, 5.9858e-04, 3.4959e-04, 3.8050e-04, 7.0260e-04,\n",
            "         4.8439e-04, 5.8272e-04, 4.5926e-04, 5.0383e-04, 4.1288e-04, 3.3396e-04,\n",
            "         4.6985e-04, 4.3883e-03, 7.5331e-04, 2.0747e-03, 1.3394e-03, 8.4418e-04,\n",
            "         3.5710e-03, 7.5951e-04, 8.7096e-04, 1.3421e-03, 3.5729e-04, 6.9719e-05,\n",
            "         1.7513e-03, 2.1687e-04, 1.2196e-04, 9.5196e-05, 8.3493e-04, 1.1459e-03,\n",
            "         4.9359e-04, 2.7005e-04, 1.5703e-03, 2.2513e-04, 6.6365e-04, 3.2638e-03,\n",
            "         1.8832e-03, 1.0149e-03, 5.2453e-04, 2.7635e-04, 6.3431e-04, 9.6585e-04,\n",
            "         1.2406e-03, 1.0345e-03, 1.2915e-03, 9.1814e-04, 2.6968e-03, 3.7360e-04,\n",
            "         8.7125e-05, 4.7792e-04, 4.1342e-03, 7.8543e-03, 3.7277e-03, 1.5509e-03,\n",
            "         3.1186e-03, 6.5693e-04, 1.5228e-03, 4.1929e-04, 9.6110e-05, 2.4292e-03,\n",
            "         1.8078e-03, 3.5138e-03, 1.0682e-03, 4.6006e-04, 2.4608e-03, 6.0866e-04,\n",
            "         4.8035e-04, 3.6614e-03, 1.0287e-03, 1.6229e-03, 2.0081e-03, 4.0300e-03,\n",
            "         7.2849e-04, 7.3713e-04, 1.7914e-04, 1.6906e-03, 9.1811e-04, 3.8640e-04,\n",
            "         1.4291e-04, 1.3410e-03, 3.0631e-04, 2.0911e-03, 1.1855e-03, 1.6168e-03,\n",
            "         1.6242e-03, 9.3222e-04, 3.1503e-04, 8.7931e-04, 1.7446e-03, 1.0232e-03,\n",
            "         2.6328e-04, 1.9988e-04, 9.2314e-04, 9.9335e-04, 2.4760e-03, 2.7021e-03,\n",
            "         2.5503e-03, 4.8924e-04, 5.4865e-04, 1.2547e-04, 2.2539e-04, 6.2130e-04,\n",
            "         3.8970e-04, 7.0529e-04, 2.3600e-04, 1.1615e-04, 1.5050e-03, 3.7186e-03,\n",
            "         4.4681e-04, 7.1654e-04, 6.8014e-04, 7.2742e-04, 2.9439e-03, 1.8869e-04,\n",
            "         2.9459e-04, 2.0798e-03, 2.1884e-03, 3.4218e-04, 6.9824e-05, 4.6172e-04,\n",
            "         4.2212e-04, 1.0445e-03, 8.0047e-04, 2.5784e-03, 2.4035e-04, 8.6955e-04,\n",
            "         1.9699e-04, 1.4159e-03, 5.1246e-04, 6.5972e-03, 1.8110e-04, 8.7958e-04,\n",
            "         2.9850e-04, 5.7525e-04, 1.5081e-03, 1.4812e-04, 3.3904e-04, 5.4157e-04,\n",
            "         3.3960e-04, 5.8590e-04, 1.1579e-03, 7.4468e-04, 9.9501e-04, 1.5354e-04,\n",
            "         1.4750e-03, 2.8450e-04, 5.1109e-03, 4.3249e-04, 3.6493e-04, 1.2060e-03,\n",
            "         5.9930e-04, 1.6838e-04, 1.8196e-03, 8.5839e-04, 1.3369e-03, 1.3535e-03,\n",
            "         1.5433e-03, 3.1269e-04, 5.2736e-04, 1.8251e-03, 4.0194e-04, 7.0274e-05,\n",
            "         4.8744e-04, 1.3935e-03, 8.8287e-04, 1.7128e-04, 1.5609e-03, 1.5831e-03,\n",
            "         1.7115e-04, 7.3888e-04, 7.7129e-04, 1.8518e-03, 8.8966e-05, 1.3079e-03,\n",
            "         2.7931e-04, 2.5181e-03, 5.2250e-04, 1.0333e-03, 1.7371e-03, 2.4067e-04,\n",
            "         3.2687e-05, 2.7178e-04, 2.9153e-03, 8.1414e-04, 3.7443e-03, 6.5623e-04,\n",
            "         8.8033e-04, 8.6524e-04, 7.7476e-04, 1.9252e-04, 7.4394e-04, 4.3937e-04,\n",
            "         1.9589e-02, 4.1562e-04, 2.3498e-04, 1.5111e-04, 4.7406e-04, 9.4721e-04,\n",
            "         9.3899e-04, 5.0501e-04, 1.3642e-03, 1.7678e-03, 1.4927e-04, 7.8298e-04,\n",
            "         1.2942e-04, 3.3572e-04, 1.2358e-04, 8.0685e-04, 2.0677e-03, 4.1660e-03,\n",
            "         7.2121e-04, 3.8829e-03, 2.7199e-04, 4.4435e-04, 2.8702e-03, 8.3614e-04,\n",
            "         4.8190e-04, 2.4389e-03, 6.8712e-04, 5.8227e-04, 1.5206e-04, 7.1943e-04,\n",
            "         1.1655e-04, 4.7100e-04, 6.5455e-04, 2.8126e-04, 1.2568e-03, 5.1816e-04,\n",
            "         1.6974e-04, 1.0071e-04, 4.9063e-04, 2.9514e-04, 2.8539e-04, 3.7144e-04,\n",
            "         1.1626e-03, 1.9516e-03, 1.3072e-03, 1.6580e-04, 1.1486e-04, 9.5533e-04,\n",
            "         1.3921e-03, 6.0288e-04, 1.0315e-04, 4.1735e-04, 1.4744e-03, 2.9256e-04,\n",
            "         9.6310e-04, 1.1388e-04, 5.3065e-04, 3.7124e-03, 1.4089e-04, 2.2333e-04,\n",
            "         9.1819e-04, 5.5904e-04, 7.4198e-04, 1.4739e-03, 1.5541e-04, 3.6067e-03,\n",
            "         1.4204e-04, 1.9149e-03, 2.5841e-03, 4.6102e-04, 4.4421e-04, 9.2603e-05,\n",
            "         6.8647e-04, 2.6180e-04, 5.3210e-05, 1.5357e-03, 8.3233e-04, 3.3735e-04,\n",
            "         1.6456e-03, 3.3004e-04, 3.6807e-04, 4.2551e-04, 2.6444e-04, 1.5213e-04,\n",
            "         1.8785e-03, 1.4113e-03, 1.2365e-03, 3.2373e-04, 1.4388e-03, 1.2627e-04,\n",
            "         2.2322e-04, 3.2177e-04, 9.4160e-04, 4.3510e-04, 1.4353e-03, 2.9790e-04,\n",
            "         5.7321e-04, 7.1373e-04, 5.1201e-04, 2.1650e-04, 1.2885e-03, 6.9519e-04,\n",
            "         4.4348e-04, 1.3054e-03, 2.4020e-04, 1.1923e-03, 1.1529e-03, 8.1453e-03,\n",
            "         3.0444e-03, 1.7826e-03, 9.8802e-04, 1.1050e-03, 4.7708e-04, 8.2991e-04,\n",
            "         4.4803e-04, 5.6979e-04, 1.2233e-04, 6.5292e-04, 3.3084e-03, 4.1479e-04,\n",
            "         1.2134e-04, 6.1309e-04, 2.7011e-03, 2.7016e-03, 1.1850e-03, 3.0919e-03,\n",
            "         7.5065e-04, 3.9518e-04, 7.6405e-04, 4.7405e-04, 1.4308e-03, 7.5684e-04,\n",
            "         1.5986e-03, 8.8492e-04, 2.2558e-04, 2.2509e-04, 1.5848e-03, 2.4421e-04,\n",
            "         1.5026e-04, 9.3344e-04, 3.3001e-04, 7.8742e-04, 3.6192e-03, 7.5024e-04,\n",
            "         5.5674e-04, 7.5108e-04, 1.4310e-04, 7.8628e-04, 1.5305e-04, 2.5309e-03,\n",
            "         5.4265e-03, 4.2215e-03, 1.0183e-03, 6.4093e-04, 7.2858e-04, 2.8854e-04,\n",
            "         4.9547e-04, 7.2097e-03, 2.2429e-03, 1.2249e-03, 1.6664e-03, 3.5395e-04,\n",
            "         9.4768e-04, 1.1524e-03, 3.7510e-04, 6.1162e-04, 3.7853e-04, 2.4525e-03,\n",
            "         3.4690e-04, 2.0975e-03, 2.7414e-04, 5.1880e-04, 5.6023e-03, 3.6183e-03,\n",
            "         3.1433e-03, 1.7520e-03, 1.8516e-03, 3.0905e-03, 9.1903e-04, 3.5568e-04,\n",
            "         7.0116e-04, 2.7988e-04, 2.6378e-04]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next:tensor([[624]])\n",
            "\n",
            "性イ伸謡繰\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size): # vocab_size: the number of  all the unique characters\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for j in range(max_new_tokens): # given only initial promt and then the output/input incrementally generated\n",
        "            # get the predictions\n",
        "\n",
        "            logits, loss = self(idx) # go to forward function and loss is considered currently\n",
        "            #print(f\"idx:{idx},logits:{logits},loss:{loss}\")\n",
        "\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C) where T → 1 (last time step)\n",
        "            #print(f\"logits[:, -1, :]:{logits}\")\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            #print(f\"probs:{probs}\")\n",
        "\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) output one character based on the 'probs'\n",
        "            #print(f\"idx_next:{idx_next}\")\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "            #if j >3:\n",
        "              #break\n",
        "        return idx\n",
        "\n",
        "#print(f\"xb:{xb},yb:{yb}\")\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "#logits, loss = m(xb, yb)\n",
        "#print(logits.shape)\n",
        "#print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n",
        "# given only initial promt and then the output/input incrementally generated\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znlBE1zTTRSE",
        "outputId": "a6239c94-7cd6-4250-ba4e-d6dafaeac4a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "性イ伸謡繰作ス睡籠凝盟構付鴉ぬ魚ァ罵元泥画来平習始が語降二物犬待る三太同ル極道逢平迫槌住多軍デず候際ん余騒便規畳妙倍葉吾棄＃執儘面煮皮膏煮ル広　上渡斑稼調弱再刻勇任訳ぜ美壁訪臍忘遠剿口上昔結火女儀袋畳\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(100): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n",
        "\n",
        "# first use xb yb to train, then use the following m.generate to randomly generate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "d7d327f4-097b-48f0-c860-43c5cbf0acad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ix tensor([9127, 6129, 6530, 8320, 9365, 3713, 8172, 3338, 3120, 2889, 7876, 9431,\n",
            "         264, 9267, 3114, 9468, 7204, 3842, 4486, 8347, 3173, 1284, 5801,  541,\n",
            "         823, 5578,  505, 2789, 4495, 3060, 7953, 8942])\n",
            "ix tensor([5096, 6239,  592, 8788, 4488, 2710, 5474, 2869, 3331, 5036, 5042, 4728,\n",
            "        1315,  437, 1771, 5425, 4825, 4996, 2994, 8847, 4522, 4502,  268, 2870,\n",
            "        8824, 2019, 7467, 5871,  373, 6485,  997, 7052])\n",
            "ix tensor([3893, 7099,  281, 8747, 7575, 3874, 3877, 5951, 2750, 9035, 3087, 1959,\n",
            "        5304,  141, 3487, 3932, 2996, 1050, 6703, 6660, 5624, 8143, 4837,  561,\n",
            "        4213, 1892, 2222, 6007, 2169, 4765, 1254, 3916])\n",
            "ix tensor([2600,  142, 3889, 5666, 5806, 5040,  544, 5062, 5062, 3144, 1755, 3445,\n",
            "        3282, 7450, 3635, 9377, 7296, 6498, 5420, 1482, 3822, 4145, 1099, 8355,\n",
            "        3333, 6722, 5810, 1415, 1898, 9282,  352, 1773])\n",
            "ix tensor([6995, 6702, 5716, 1280, 2422, 8006, 7920, 6394,   27, 9276, 3666, 2335,\n",
            "        5909, 5672, 4952,  752, 3462, 7125, 7762, 5960, 5009, 7128, 1231, 7156,\n",
            "        3080, 2899, 7740, 5224, 1751, 3360, 6161, 2494])\n",
            "ix tensor([2135, 7859,  518, 8326, 6976, 5810, 2500, 2343, 8365, 8523, 3387,  817,\n",
            "        3833, 1428, 3628, 9178, 4539, 4904,  202, 9077, 3889, 7771, 2556, 9076,\n",
            "        6005, 8984, 1360, 6712, 6204, 8774,  898, 3950])\n",
            "ix tensor([3465, 5021, 6087,  618,  676, 3372, 5335, 7572, 1434, 2085, 9248,  217,\n",
            "        3002, 7024, 1368, 1384, 8461, 1773, 7396, 6901, 4457, 1569, 4852, 7403,\n",
            "        8033, 4883, 5562, 1619, 8884, 2400, 1281, 4055])\n",
            "ix tensor([7698,  535,  379, 8261, 2483, 2165, 7025, 1945, 4050, 4598, 3542, 4249,\n",
            "        3756,  554, 1821, 3881, 4001, 2036, 5202, 8491, 8852, 7154, 2766, 1400,\n",
            "        2432, 4508, 4984, 3798,  539,  207, 3073, 2330])\n",
            "ix tensor([ 809, 8665, 2312, 9225, 9110, 5328, 6497, 8709, 8258, 2886, 6859, 7133,\n",
            "        2074, 6619,  863, 5231, 9099,  855, 7765, 1027, 4012,  182, 1234, 2602,\n",
            "        7590, 4861, 8524, 2299, 8895, 4815, 2663, 8972])\n",
            "ix tensor([4010, 6689,  653, 5080, 5648, 4236, 8077, 9190, 5019, 9099, 5859, 8910,\n",
            "         417, 1667, 9385, 8561, 4772, 8632, 9283, 5079, 8747, 4717, 2241, 7946,\n",
            "        5783, 5698, 4450, 7437,  826, 1196, 4151, 1288])\n",
            "ix tensor([2566, 1847, 3936,  186, 4103, 8865, 2113, 8047, 7106, 1980, 7419, 9086,\n",
            "        9262, 5451, 7453, 5093, 1611, 8628, 7636, 6103, 4843, 7165, 8590, 2416,\n",
            "         403, 2183, 5089, 8301, 1541, 2154,  496, 6519])\n",
            "ix tensor([ 635, 9439, 4428, 3223, 5779,  885,  915, 3716, 2011, 5558, 2710, 1494,\n",
            "        5598, 2366, 9019, 9038, 3191, 4305, 7674, 3995, 6557, 7203, 7786,  415,\n",
            "        4537, 3785, 5507, 8316, 1189, 7818, 4356, 3514])\n",
            "ix tensor([3943, 7396, 4852, 3847, 3054, 2463, 8400, 1001, 7247, 7047, 4318, 4318,\n",
            "        7471, 6827, 2947, 4985, 5323,  831, 8028, 2840, 3355, 9100, 9471,  221,\n",
            "        3636,  642, 5365, 1345,  922, 1722, 3978, 4878])\n",
            "ix tensor([9028, 5481, 8379, 1436, 3058, 7220, 7341, 4272, 4177, 8464,   25, 3604,\n",
            "        2710, 4420, 2536, 2887,  697, 8885, 2792,  920,  312, 2022, 1983, 6346,\n",
            "        3005, 5386, 7913, 4131, 8426, 8870, 9058, 3780])\n",
            "ix tensor([5430, 1783, 8692, 5251, 2692, 8947, 8504, 6400, 8130,  516, 8336, 6676,\n",
            "        4642,  483, 2485, 4861, 8152, 1112, 7205, 8061, 3678, 5573, 6092, 3094,\n",
            "        6187, 3826, 5577, 6895, 1396, 9397, 4304, 1605])\n",
            "ix tensor([2582, 6106, 7196, 9436, 2130,  658, 7893, 1261, 6323, 9357,  237, 9259,\n",
            "        7084, 1845, 4997,   67, 3507, 7149,  190, 5214, 5188, 1044,  866, 7842,\n",
            "        6433, 3314, 7694, 5976, 4079, 6525, 1562, 3490])\n",
            "ix tensor([8804, 2115, 7772, 4468, 3527, 7155, 7884, 7464, 5443, 8357, 6545, 7438,\n",
            "        7592, 5538, 1786, 6958, 9402, 7148, 2204, 1221,  691, 6125, 7066, 5897,\n",
            "        7772, 3215, 2476, 1860, 2234, 5963, 4210, 1110])\n",
            "ix tensor([5437, 3764, 7332, 2759, 2168, 6356, 4558, 9046, 8811, 7322, 2945, 1271,\n",
            "        8457, 8506, 9158, 4546, 5544, 8882, 7964, 6751, 6546, 3492, 2256, 1099,\n",
            "        7322, 4388, 9362, 3703, 9410, 3249, 7761, 2914])\n",
            "ix tensor([2567, 1024, 7194,  763,  320, 5273, 2554,  488, 8522, 4133, 1567, 6968,\n",
            "        4060, 2260, 7201, 2199, 7089,  345, 3598, 3016,  145, 1422, 4394, 2023,\n",
            "        3185, 7658, 1865, 3905, 5446, 6042, 5016, 2285])\n",
            "ix tensor([9268, 7405,  791, 3826, 2787,  534, 8130, 2880, 6575, 6874, 2978, 6354,\n",
            "         148, 7085, 1895,  178, 3384, 6420, 8189, 5064, 8615, 3090, 3329, 2935,\n",
            "         204, 6898,  129, 4087, 2434, 1851,  947,  445])\n",
            "ix tensor([3530, 2173, 1434,  127, 8689, 4690, 5470, 1751, 1523, 6046, 6652, 5637,\n",
            "        7786,   24, 7672, 3975, 8867, 4613, 3815, 2903, 3283, 1185, 6615, 6204,\n",
            "        7949, 9222, 7479, 3115, 1969, 5410, 1073, 4728])\n",
            "ix tensor([8688, 1402, 6463, 2806, 5301, 4101, 6714, 4284, 2755, 6758, 9000, 4913,\n",
            "        5383, 9005, 8900, 6690, 7745, 7429, 6244, 2228, 1130, 2633, 7216, 2508,\n",
            "        9302, 7102, 7999, 8740, 8945, 7972, 5175, 6812])\n",
            "ix tensor([ 897, 5212, 2126, 9093, 7159, 3064, 3431,  664, 5455, 5545, 8322, 3152,\n",
            "         710, 4746, 3851, 1064, 2962, 2975, 6576, 9096, 1531, 4488,  898, 1884,\n",
            "        9033, 5793, 6854, 3545, 8885, 7864, 4807, 5040])\n",
            "ix tensor([5422, 4197,  557,  597, 7435, 6068,  708, 4282, 5191, 6857, 2367, 1267,\n",
            "        3057, 8765, 2858, 3366, 2759, 3410, 7289, 2223,  293, 4095, 7697, 5472,\n",
            "        7841, 5733, 4738, 6108, 7614, 1940, 8897, 5865])\n",
            "ix tensor([3967, 1502, 2657, 6599,   76, 5186, 6269, 8200, 2292, 8432, 5969, 2937,\n",
            "        7150, 7577, 3200, 6463, 3530, 9414, 3203,  443, 5434, 7145, 4083, 8849,\n",
            "        3760, 5819, 7979, 8387, 6553, 3101, 3340, 6935])\n",
            "ix tensor([1040, 8640, 2624, 7603, 7826, 6447, 3115, 8769, 1987, 6129, 8277, 5930,\n",
            "        1421, 2694, 6223, 2725, 2120,  552, 2033, 1445, 7617, 7248, 5399, 5145,\n",
            "        3617, 9424, 1566,  821, 5123, 3352,  291, 1939])\n",
            "ix tensor([7413, 4625, 7773, 2303, 6644,  866, 7437, 1590, 5900, 7605, 2808, 6869,\n",
            "        8966, 1959, 2568, 6103, 2431, 4425, 1936, 8311, 2634, 8085, 5316, 4604,\n",
            "        2786, 2731, 7026, 3491, 8825,  321, 2395, 7229])\n",
            "ix tensor([8344,  101, 2837, 4805,  219, 9072, 9086, 8259, 8927, 1357, 2008, 7999,\n",
            "        4009, 2322, 3971, 5997,  315, 6288, 6646, 4917, 8149, 4872, 5246, 1021,\n",
            "        8303, 6764, 2153, 6405, 8503, 3018, 8006, 2355])\n",
            "ix tensor([8009,  473, 2366,  841, 7478, 5448, 7749, 4089, 3948, 2516, 3929,  362,\n",
            "        6714, 5109, 2210, 9392, 8323, 3373, 1267, 7301,  110, 4115, 7190, 8410,\n",
            "        8304, 4037, 7957,   40, 7534, 5178, 3167, 2217])\n",
            "ix tensor([5815, 6866, 3671, 2092, 6207, 3279, 8716, 8001, 4659, 7708,  320, 1438,\n",
            "        8800,  913, 6649, 6695, 4799, 2192, 1528, 3118, 5964, 1324, 3040, 1137,\n",
            "        3353, 5734, 7834,  426, 4358, 4782, 5719, 8897])\n",
            "ix tensor([8193, 1943,  556, 4662, 8730,  752, 5084, 2236, 9238, 8811, 9176,  878,\n",
            "        5804, 6486, 7975, 4112, 2367, 5801, 4770, 9071, 7833, 3211, 9274, 2906,\n",
            "        3148, 1702, 8052, 7497, 4518, 3659, 2064,  622])\n",
            "ix tensor([7623, 3578, 2595, 2821, 4146, 5966, 7733,  550, 9365,  233, 4880, 4965,\n",
            "        2016,  311, 6293,   52, 2694, 4695, 6838, 5330,  102, 3988, 8491,  172,\n",
            "        5508, 1748, 2369, 9131, 9365, 8672, 5102, 7609])\n",
            "ix tensor([6028, 2584, 5591, 3301, 2172, 7987, 1546, 3960,  237, 5711,  846, 1935,\n",
            "        4433, 2279, 1216, 2044, 1176, 2953, 5087, 2309, 4368, 7284, 3606, 9457,\n",
            "         382, 4367, 9088, 2180, 3605, 1926, 8708, 7878])\n",
            "ix tensor([9364, 7491, 1916, 9401, 7121,  372, 6832,  776, 3401, 8678, 7927, 6792,\n",
            "        7349, 9024, 1148, 2156, 3753, 2922, 1411, 3224, 1009, 2470, 3722, 1474,\n",
            "        8027,   26, 2383, 6894, 2918, 2454, 2018, 2926])\n",
            "ix tensor([3859, 1446, 6929, 6930, 6151, 1662, 7873, 3486, 7511,  528, 3471, 9300,\n",
            "        6644, 3200, 9194, 2738, 8570, 7640, 2387, 9051, 6312, 8651, 6060,  531,\n",
            "        8737, 2894, 6308, 5336, 3151,   48, 2405, 8445])\n",
            "ix tensor([ 381, 9156,  372, 4585, 2141,  750, 8226, 5057,  894, 6690, 6985, 6389,\n",
            "        9003, 2172, 3777, 1828, 6526, 9464, 6280, 6480, 6146, 7770,   71, 8046,\n",
            "        7664, 7524, 4872, 4282, 7696, 7598, 7297, 7344])\n",
            "ix tensor([2169, 8502, 4032, 9107, 9175, 4170,  886, 3033, 4084, 5694, 4455, 5969,\n",
            "        6479,  397, 6980, 8662, 2972, 5780, 4795, 9119, 1159, 4774, 3410, 1211,\n",
            "        7701, 2673, 8855, 4224, 6920, 7870, 2252,  477])\n",
            "ix tensor([5542, 5703, 3658, 7079, 2963, 3043,  354, 5770, 8499, 1605, 8444, 7857,\n",
            "        7670, 2124, 2406, 3824, 5770,  309, 1123, 4123, 7472, 5303, 6694, 8221,\n",
            "        9300,  630, 1122,  578, 8816, 5444, 5788, 3580])\n",
            "ix tensor([8833, 6293, 1120, 2761,  126, 2470,  603, 2316, 8631,  627, 7855, 4468,\n",
            "        6531, 5018, 6516, 9195, 9340, 5226, 2340, 6009, 4163,  781, 3585, 7351,\n",
            "        7422, 6161, 3992, 5885,  179, 7992,   92,  348])\n",
            "ix tensor([3407, 9046, 5460, 2867, 2846, 7663, 8060, 5496, 7249, 8432, 1249, 3159,\n",
            "        9462, 3768, 4340, 5003,  522, 5062, 5458, 4742, 4894, 6519, 7998, 5334,\n",
            "        7421,  889, 1076, 1554, 1377,  816, 5408, 6071])\n",
            "ix tensor([3173, 6726, 5583, 2010, 6876, 6198, 8026, 2219,  950, 7959, 7212,  723,\n",
            "        3129, 6528, 4192, 7963, 5577, 1502,  178, 4327, 6784, 5845, 8035, 5587,\n",
            "        1842,  266, 2159, 2079, 6814, 5987, 3680, 1723])\n",
            "ix tensor([1430, 1970, 9164, 2866, 1707, 8270, 2198, 4647, 6863, 9105, 2014,  622,\n",
            "        8336, 8707, 5829, 6696,  707, 2249, 4456, 6978, 1031, 3824, 8041, 4948,\n",
            "        9147, 1149, 7570, 7235, 4113, 1109, 3448, 2226])\n",
            "ix tensor([2006, 3122, 6539, 2599, 3695,  882, 2972, 8716, 8442, 3621, 3121, 1741,\n",
            "        8749, 9386, 7497, 3865,  696, 4170, 2204, 1301, 2792, 9086, 4872, 7227,\n",
            "        1487, 7100, 9191, 6540,   57, 1835, 8456, 5206])\n",
            "ix tensor([8423, 2349, 4162, 7177, 5918, 5687, 5611, 3070, 5671, 5624, 9295, 5828,\n",
            "        2754, 8091, 5704, 2286, 6355, 7579, 2761, 5834, 2666, 9129, 4116, 7275,\n",
            "        3762,  667, 9042, 7712, 6596, 3562, 7456, 3973])\n",
            "ix tensor([4841, 8147, 2517, 8106, 1985,  357, 3258, 8830, 9247, 4052, 9128, 4939,\n",
            "        8602, 3818, 5021, 3510, 3686, 2120, 7822, 3502,   69, 7106, 1461, 3130,\n",
            "        6869, 3779, 6735, 2609,  669, 7368, 2647, 3184])\n",
            "ix tensor([ 770, 8447, 1965, 2858, 6399, 6738, 8813, 4570, 7429, 1601,  238, 3972,\n",
            "        6883, 3033, 3879, 4977, 3179, 9198, 7311, 4357, 2504, 1824, 8026, 1338,\n",
            "        3394, 8920, 6373, 2669, 7122, 1135, 9020, 1550])\n",
            "ix tensor([3271, 4180, 7650, 6760, 7962, 2388, 3307, 3677, 6263,  247, 2262, 3570,\n",
            "        6445, 7333, 1084, 3408, 8834, 9150, 6589, 3879, 2331, 4391, 2360, 1528,\n",
            "        7555, 2958, 7459, 1880, 6579, 6372, 8302, 4222])\n",
            "ix tensor([4221, 8734, 6749, 4345,   86, 4253, 1709,  291, 4357,  872, 1579, 8585,\n",
            "        1171, 8305, 6475, 6175, 5367, 3426,  157, 7811, 6980, 4421, 7774, 3969,\n",
            "        1442, 2871,  602, 2467,  912, 7253,  317, 4722])\n",
            "ix tensor([3105, 4221, 5667, 5900,  808, 8931, 3192, 3867, 6263, 7284, 1613, 5847,\n",
            "        4662, 6413, 9148, 1154, 2821, 2764,  889, 7316, 9358, 5314, 3188, 8800,\n",
            "        6759, 9083, 8919, 5666, 6627, 8835, 6749, 5115])\n",
            "ix tensor([7303, 8716, 1586, 4692, 3370, 4101, 2025, 9371, 7662, 7412, 6398,  754,\n",
            "        7191,  980, 4399, 1452, 7995, 2204, 2405, 1519, 5143, 8767, 8702, 1368,\n",
            "         624, 5309, 1101, 6246, 1221, 5591, 4551,  733])\n",
            "ix tensor([7628, 3456, 4448, 8770, 3219,  444, 6564, 4067, 7742, 6824, 2194,  599,\n",
            "        6409, 7391, 1803, 3564, 7638, 4553, 2657, 1338, 6741, 4072, 7832, 1377,\n",
            "        5017, 8418, 4093, 1072, 2178,  233,  871, 7456])\n",
            "ix tensor([5746, 6997, 6118, 3551, 8194, 3183, 8135, 2303, 7805, 3285, 7373, 5110,\n",
            "        7148, 7247, 1602, 6734,  362, 1503, 1796, 5965, 4589, 4104, 2915,  418,\n",
            "        8751, 4181, 6474,  376, 1467, 6649, 7405, 8394])\n",
            "ix tensor([4110, 1133, 6495, 5883,  421, 5168, 9223, 9110, 2371, 4086, 4704, 5703,\n",
            "        1442, 6613, 6645, 3830, 4547, 8236, 2558, 6158, 7368, 6682, 8763, 8967,\n",
            "        5396, 9099, 6951, 2831, 7170, 8831, 2145, 8952])\n",
            "ix tensor([ 184, 9263, 5612, 1272, 5811,  416, 3088,  289,   50, 4393,  466, 9258,\n",
            "         348, 2129, 4063, 3489, 2739, 4689, 8050, 6132, 1127, 2644, 3231, 7383,\n",
            "        3358, 5669, 3679, 5078, 5290, 6413, 8547,  980])\n",
            "ix tensor([4164, 3107, 6194, 6130, 7253, 7753, 3889, 6353, 1927, 3854, 1998,  456,\n",
            "        3663, 6151, 7323, 7595, 1284, 9095, 2848, 3977, 2078, 8159, 9449, 1517,\n",
            "        2051, 2144,  830, 5330, 2216, 1611, 4797, 4139])\n",
            "ix tensor([8460,  396, 2582, 5928, 7291, 4725, 7317, 5599, 5572, 5411, 6810, 4560,\n",
            "        2686,  453, 1401, 6466, 8559, 5141, 3225, 1672, 2762, 4825, 3937, 4811,\n",
            "        3950, 3897, 3916, 6701, 5268, 9209, 8599, 5742])\n",
            "ix tensor([7984, 8578,  779, 3813, 8805, 2046, 6780,  139, 6326, 6920, 7799, 1600,\n",
            "        4820, 9194, 8699, 1011, 4438,  800,  103, 7780, 7412, 9354, 9256, 3798,\n",
            "        5683, 5746, 8827, 5574, 4042, 2718, 4140, 1886])\n",
            "ix tensor([5050, 7540, 7273,  650, 2330, 3547, 2849, 1505, 7470, 8647, 7398, 6399,\n",
            "        5810, 3869, 1246, 8320, 1826, 4871, 4480,  742, 9183, 1486, 9074, 7644,\n",
            "          94, 4328, 8075, 1802, 1937, 1636, 8516, 7836])\n",
            "ix tensor([2932, 1166, 6183, 6879, 8966,  641, 7791, 5888, 3500, 8830, 7776,  824,\n",
            "        6470, 6069, 2996, 3487, 9211, 5798, 4599, 5061, 1396, 9157, 4456, 9102,\n",
            "        2901, 7869, 9026, 7484, 2416, 8293, 6419, 7947])\n",
            "ix tensor([2155, 5157, 3484, 3231,  211, 4948,    0, 1551, 8902, 2020,   71, 3746,\n",
            "        8952, 8619, 4301, 9310, 1624, 9169, 7931, 1399, 6787, 2566, 4065, 6599,\n",
            "        7473, 6871, 6050,  647,  453, 4515, 7027,  286])\n",
            "ix tensor([9294,  891, 2087,  956,  996, 8497, 6810, 1762, 6176, 9278, 4809, 6487,\n",
            "        5723, 7670, 4514, 3955, 2161, 1614, 4680, 1880, 7723, 5002, 4457, 4729,\n",
            "        2463, 1990, 3046,  122, 7847, 4058, 3140, 2506])\n",
            "ix tensor([9119, 8507,  283, 5000, 3388, 3757, 3968, 1113, 9304, 4712, 7847, 4898,\n",
            "        9252, 7661, 4052, 4660, 8411, 1507, 2942, 3746, 1473, 6352, 3536, 6727,\n",
            "        6749, 3358,  311, 8945,  863, 7514, 7553, 7970])\n",
            "ix tensor([2726, 7535,  620, 1248, 7760, 4523, 1072, 2015, 7446,  298, 7103, 9052,\n",
            "        4135, 8152, 5818, 7006, 7848, 3559, 5152, 7953, 2082, 7019, 1897,  907,\n",
            "        1837, 6495, 6908, 6643, 6278, 4103, 7928, 8776])\n",
            "ix tensor([3333, 6142, 3984,  147, 6570,   94, 8999, 5332, 1854, 1141, 2547, 7662,\n",
            "        8117, 6194,  412, 5389, 3399, 3100, 2037, 2936,  132, 8658,  589, 8059,\n",
            "        4216, 4780, 5588,  600,  850, 3934, 1609, 6677])\n",
            "ix tensor([8284, 6313, 2432, 7548, 2243, 8772,  251, 4872, 9396, 9029, 3357,  838,\n",
            "        1857, 7837, 3113, 6233, 9017,   15,  267, 1144,  506, 7040, 7237,  555,\n",
            "        8369, 5517, 8380, 6687, 8766, 8497, 6542, 5886])\n",
            "ix tensor([1859, 4769, 6461,  414, 4004, 8621, 7676, 8704, 5984,  178, 6182, 6001,\n",
            "        9031, 6171, 4548, 8425, 2847, 2786, 5668, 6934, 6438, 1730, 4945, 1296,\n",
            "        2650, 1235, 6657, 2883, 9134,  601, 4655, 6644])\n",
            "ix tensor([8135, 1870, 7398, 1962, 6659, 4676, 2467, 6210, 9467, 5986, 8359, 9401,\n",
            "        4789, 4860, 2700, 8567, 5774, 4606,  491,  309, 3673, 5060, 7270, 4720,\n",
            "        2729, 4671, 3103, 8962, 4988, 9081,  142,   85])\n",
            "ix tensor([4707, 8473, 4994, 4907, 2385, 6908, 7157,  727, 2400,  792, 7301, 7898,\n",
            "        7196, 2806, 2529, 1236, 8890, 7424,  190, 6410, 7025, 4621,   30, 8384,\n",
            "        9313, 2384, 7028, 4727, 3436, 4509, 6912, 1872])\n",
            "ix tensor([9397, 6589, 7164, 2252, 6427, 7604,  450,  338, 3154, 8092, 8150, 2480,\n",
            "        5943, 7346, 9277, 8067, 4860, 3966, 1199, 2172,  538, 9402, 2654, 1846,\n",
            "        6785, 6727, 3012, 4401, 8482, 5518, 3149, 3370])\n",
            "ix tensor([8830, 3775,  427, 3077, 3182, 2443, 1928, 8882, 7470, 3980, 9045, 6884,\n",
            "        9020, 8814, 7272, 4228, 3016, 8821, 2879, 5444, 4551, 5118, 2264, 4351,\n",
            "        1993, 4221, 7681, 1816, 8313,  878, 3383, 3420])\n",
            "ix tensor([4088, 4032, 4314, 2868, 6629, 5961, 4649, 4813, 7177, 2142, 4075, 2952,\n",
            "        8880,  188, 4918, 3650, 8518, 7065, 8090, 9288, 2602, 4926,  120,  907,\n",
            "        2728, 1186, 2500, 9363, 3001, 2031, 9200,  701])\n",
            "ix tensor([7777, 5466, 2202, 6360, 2081, 2198, 2721, 6153, 4363, 9305, 3842,  151,\n",
            "         712, 5735, 1411, 2830, 1108, 9257, 9140, 4640, 4106,  289,  195, 5799,\n",
            "        5009, 9435, 8401,  626, 4130, 5838, 2959, 2949])\n",
            "ix tensor([8490,  226, 5491, 8305, 2359, 3888, 9172, 5586, 1584, 3635, 2227, 2384,\n",
            "        9403, 4164, 4347, 1795, 2081, 1454, 5083, 6630, 5910, 8484, 2022, 6960,\n",
            "        8665,  401, 3730, 7378, 4542, 1260, 7201, 2475])\n",
            "ix tensor([4148, 5737, 3366, 7176, 8610, 3130, 5769, 6271, 8587, 1907, 7994, 6800,\n",
            "        6884, 8890, 4049, 7629, 5203, 6193, 1403, 7396, 8459, 3759, 4522,  777,\n",
            "        5388, 8028,   88, 2269,  446, 8317, 9232, 7019])\n",
            "ix tensor([4174, 6086, 8296, 3868, 7888, 1273, 1453, 4049, 4893, 8930, 1440, 8669,\n",
            "        1023, 3425, 9442, 4501,  568, 2734, 1014, 6061, 7602, 2761, 2529, 9441,\n",
            "        3442, 4164, 8775, 5127, 6959, 5605, 4431, 9216])\n",
            "ix tensor([5259, 4468,   41, 6106, 6711, 2668, 5445,  986, 8676, 3676, 3111, 8353,\n",
            "        5942, 9027, 5156, 9321,  661, 8548, 9077, 4707, 9226, 4933, 6049, 8727,\n",
            "        7874, 6168, 6764, 1411, 6601, 4312, 6359, 6115])\n",
            "ix tensor([ 670, 7554, 5567,  125, 6761, 2439, 9267, 8449, 5767, 7012, 7643, 6277,\n",
            "        9225, 1344, 8281, 7604,   35, 7455, 3252, 8231, 3435, 5739, 6045, 1833,\n",
            "         569, 3995, 6104, 4953, 7248, 2326, 5046, 1232])\n",
            "ix tensor([5812, 4029, 8540,  874, 2407, 4821, 1418, 2205, 3188, 2535, 1552, 7572,\n",
            "        8627, 1119, 7550, 8824, 5467, 5409, 6534, 7861,  760, 2764, 6234,  325,\n",
            "         134, 4784, 3368, 8027, 5915, 1736, 8766, 2194])\n",
            "ix tensor([8376, 1917, 5285, 5711, 5874, 7967, 1788, 6559, 3484, 6259, 2692, 6362,\n",
            "         438, 4691, 8232, 5413, 3568, 6605, 1663,  657, 5981, 5340, 4283, 6671,\n",
            "        8890, 6273,  200, 2648, 6966, 2447, 2401, 3392])\n",
            "ix tensor([5188, 1467, 7945, 7094, 8891, 7684, 6751, 4071, 8435, 8465, 9443,  583,\n",
            "         826, 8194, 4555, 8971, 6798, 2681, 5939, 1405, 2507, 4823, 8352, 1532,\n",
            "        8355,  919, 1087, 3768, 2018,  884, 5642, 7470])\n",
            "ix tensor([  51, 6196, 5980, 1481, 1708, 8560, 8940, 8151, 8740, 1240, 2120, 5180,\n",
            "        3583,  581, 7040, 6547, 7335, 5079, 5712, 9077, 7532, 8620, 4336,  740,\n",
            "        8203,  238, 6603, 3829, 7706, 2969, 8742, 8005])\n",
            "ix tensor([ 520,  586, 3640, 1893, 5890,  369, 2692, 1732, 4747, 2338, 2489, 6502,\n",
            "         728, 2990, 7238, 8518, 4751, 3475, 2398, 8362,  260, 7298, 3632, 8867,\n",
            "        5784, 3869, 3934,   34, 5127, 7702, 4412, 6892])\n",
            "ix tensor([5373, 7107, 4404, 3271, 5777, 8411, 1849, 3314, 7375, 1548, 5483, 7957,\n",
            "        5556, 6950, 1214, 6451, 1587, 2939, 5015, 8098, 4002, 7048, 8065, 6412,\n",
            "        1304,  483, 1718, 4859, 3179, 8524, 6055, 1895])\n",
            "ix tensor([6659, 8754, 5384, 5843, 4398, 7172, 1079, 8567, 2768, 3255, 6095, 7642,\n",
            "        7745, 5493, 3391, 7425, 5091, 6573, 8237, 8649, 9255, 6128, 9461, 7398,\n",
            "        6789, 1938, 5255, 6461, 7157, 6113, 4546, 2921])\n",
            "ix tensor([ 665, 7990, 1413, 9362, 5154, 9083, 6017, 5871, 9209, 7407, 6896, 2120,\n",
            "        5120, 1228, 9254, 1990, 2421, 5231, 2226, 3304, 9418, 8313, 4635,  831,\n",
            "        7927, 3403, 7162, 5793, 8396, 8932, 1763, 4328])\n",
            "ix tensor([ 243, 1339, 3067, 5304, 7816, 1058, 5363, 6447, 7275, 5884, 5618, 5883,\n",
            "        8269, 4704, 2551, 5368, 7072, 8348, 7020, 9416, 7730, 7776, 4832, 3783,\n",
            "        5860,  675, 6744, 8620, 8508, 6219, 4811, 9259])\n",
            "ix tensor([6382, 3384, 1407, 4761, 9381, 9332, 8494, 4098, 9484, 8740, 6514, 3489,\n",
            "        6769, 3808, 3316, 1016,  291, 1892, 9052, 2071, 1579, 1375, 6496, 6264,\n",
            "        2617,  742, 4156,  162, 2846, 1017, 6251, 7074])\n",
            "ix tensor([4206, 6685, 3376, 2935, 6187,  317, 4575, 6683, 5177, 3067, 2470, 3987,\n",
            "        6438, 4052, 2216,  425, 9112,  392, 1118, 2688,  753, 8100,  752, 5591,\n",
            "        2921, 9166, 5502, 4329, 5707, 1888, 4925, 6026])\n",
            "ix tensor([7268, 8176, 4647,  526, 8197,  727, 8871,  975, 1835,  377, 8657,  376,\n",
            "        8652, 3260, 6889, 5519, 4761, 6402,   94, 5693,  177, 4171, 7059, 6650,\n",
            "        1143,  267, 4722, 4921, 6416,  109, 8790, 3335])\n",
            "ix tensor([4853, 1509, 1268, 5169, 3157,  435, 6229, 1119, 4620, 8444, 3245, 3059,\n",
            "        6036,  406, 6413, 9093, 5776, 3678, 8062, 5713, 1299, 8683,  584, 1244,\n",
            "        9486, 6035, 6444,  856, 2520,  566, 4779, 3999])\n",
            "ix tensor([7948, 4742, 3810,  540, 8299,  446, 2192, 2586, 8109, 5642, 3453, 9362,\n",
            "        8163, 3022, 3321,  454, 2011, 1643, 7123, 9247,    9, 8454,  628, 3299,\n",
            "        5533, 1454, 5987, 5618, 9018, 4711, 8378, 5618])\n",
            "ix tensor([4283, 3687, 4063, 1808,  375, 6621,  776, 5618, 5655, 9311, 5453, 7830,\n",
            "        1942, 6789, 6536, 8009, 2160, 1460, 3152, 7763, 3831, 2782, 1975, 4642,\n",
            "        2062, 5912, 3618, 2435, 3128, 8245, 4700, 4738])\n",
            "ix tensor([7673, 6994, 5178,  902, 1506, 9403, 1125,  938, 5149, 5609, 1028, 8858,\n",
            "        6550, 5876, 6129,   50, 1048, 3354, 1991, 1412, 6630, 3770, 6481, 3127,\n",
            "        2807, 2790, 5501, 2638, 8510, 7791, 3233, 6834])\n",
            "ix tensor([9109,  812, 9262,  443, 5070, 5348, 3342, 3727, 3912, 6886, 7919, 2194,\n",
            "        8656, 3765, 6561, 8274, 1136, 8075, 6029, 2730, 9283, 7552, 5750, 4928,\n",
            "        6499, 1350, 2619,  351, 2323, 9075, 4391, 2909])\n",
            "ix tensor([7187,  965, 1593, 4970, 3434, 7882, 7798, 7178, 6569, 2910, 5889, 7146,\n",
            "        8079, 4914, 2091,  949, 6155, 7255, 7823, 5005, 6125, 3505, 9027, 5680,\n",
            "        5087, 4787, 7128, 8696, 5760, 1375, 1063, 2460])\n",
            "ix tensor([8672, 4867, 6257, 2975,  175, 7329, 5109, 6836, 8899, 7031, 4321, 3501,\n",
            "        8136,  976, 5778, 1045, 7218, 5435, 6556,  721, 5989, 4975, 8892, 8038,\n",
            "         129, 9273, 3793, 8122, 5090,  848, 1803, 9312])\n",
            "ix tensor([1498, 5748, 2115, 2546,  985, 3300, 7345,  458, 2127, 3002, 4813, 8212,\n",
            "        2691, 8795, 6534, 3056, 5356, 6333, 1824, 5422, 4037, 1997, 8717, 6235,\n",
            "        6272, 2777, 8490,  616, 3578, 2561, 3538, 6370])\n",
            "ix tensor([ 909, 2396, 5465, 2930, 7811, 8268, 2921, 4034, 2819, 3503, 6113, 4258,\n",
            "        3022, 5782, 4728, 5345, 7789, 5728, 9023, 1597, 2026, 1529, 1740, 4384,\n",
            "        7371, 8673, 5025, 4242, 7205, 4087, 1927,  861])\n",
            "ix tensor([3902, 5216, 4039, 5820, 6118,  625, 8250,  443, 3067, 4549,  847,  853,\n",
            "        9145, 9011,  649, 9457, 8116,  411,  755, 3116, 1487, 4346,  276, 2811,\n",
            "        8844, 3994, 8208, 3536, 7708, 6081, 7504,  155])\n",
            "ix tensor([1377, 3987, 7648, 5247, 1298, 4972, 4802, 2653, 6010, 9184, 2835, 8892,\n",
            "        2930, 8496,  846, 7773, 5397, 6781, 8692, 3017, 1017, 6281, 7595, 3365,\n",
            "        5669, 8306,  606, 2804, 6074, 3982, 1191, 9135])\n",
            "7.03345251083374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "faa48d91-13ff-41d8-9861-010413199a58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "含シ抛判は太向一談へ槌「逢穏較数込欠力習受頭座餓叩尻ニ器便息会こ胃畠両兄内》険惜乱・実黒慨下賞　怒煮百並低家所炬び［桐偉妙論利低双儲もゆ惜満或迫溝偸煮兄的伸愛愛鼓佇む込肝匹デ腹梧師以垂師相拶邸漫も玉盟這ど美火ゼ不喊冷通容苦ょ泣便折年陽除マ居中瀟由鼻観熱増師刺減場拶煙勉杉掠宗付強相朝悪護盛俳づ腹語物漫鼻違腹喟び良層偉な察隣事結立咽イ　枝ぎぐ包疋柄鰡居蔭女調敗洒家筋ゅ尻無冷輝鼻機落迫毒水揄議初嗅えデ月上ぎの袋実飛向智燵鼓左斎［長頃草臭頭質完後珍愛書非残意ゼーめ好善穏輪燵思車風次層師五賞猶強極下様弟聴足打美剿庭層奪永盛上褐現］間渾姿奇困睨読ふ采窃ぎ一的ふ儲さ論待味。失乗側飾禁節鑑助禽奇咽権崩窃偸馬伸王片向生談機嗅耳ァ楽愛遠畜睡間自語猟忘淡聞御同眠底肝炬俳坪軍分驚鳴会暮書行屁馬抛当執際飾御掠体奥向横鼾す屈何火全槌ァ弟隠挫馳稼ド鰡筋タ奴紙化琥ぜ枯怒宿み茂想窘両険炬」渾》廓枝月侮載声含痛頃柔ァ憤定調色動職直体案楽朝声いで語華世も渡何らオな睡給魚戦気身帯業月待ぐげば除サ下任台業蔑飲行最物間悲茶迎亭根ブ弓射点ワ掃任酷切溌め葉方鑑罵案魔用間際矮ほ味睡敬ぬ瀟親ゃ敬玉易飲眺初渡禽ぎ議飛執寝背除ず琥皮偸養泥己\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The mathematical trick in self-attention"
      ],
      "metadata": {
        "id": "XinV8nmAnmKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print('　　　吾　　　輩　　　　は\\n','吾\\n','輩\\n','は\\n')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print('吾\\n','輩\\n','は\\n')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tukiH-NbRBhA",
        "outputId": "4be4f19a-7e00-49f6-eb82-73d3f9fe47d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "　　　吾　　　輩　　　　は\n",
            " 吾\n",
            " 輩\n",
            " は\n",
            "\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "吾\n",
            " 輩\n",
            " は\n",
            "\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs_E24uRE8kr",
        "outputId": "d0cae938-bfb4-449f-96cd-934b1c55777e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1808, -0.0700],\n",
              "         [-0.3596, -0.9152],\n",
              "         [ 0.6258,  0.0255],\n",
              "         [ 0.9545,  0.0643],\n",
              "         [ 0.3612,  1.1679],\n",
              "         [-1.3499, -0.5102],\n",
              "         [ 0.2360, -0.2398],\n",
              "         [-0.9211,  1.5433]],\n",
              "\n",
              "        [[ 1.3488, -0.1396],\n",
              "         [ 0.2858,  0.9651],\n",
              "         [-2.0371,  0.4931],\n",
              "         [ 1.4870,  0.5910],\n",
              "         [ 0.1260, -1.5627],\n",
              "         [-1.1601, -0.3348],\n",
              "         [ 0.4478, -0.8016],\n",
              "         [ 1.5236,  2.5086]],\n",
              "\n",
              "        [[-0.6631, -0.2513],\n",
              "         [ 1.0101,  0.1215],\n",
              "         [ 0.1584,  1.1340],\n",
              "         [-1.1539, -0.2984],\n",
              "         [-0.5075, -0.9239],\n",
              "         [ 0.5467, -1.4948],\n",
              "         [-1.2057,  0.5718],\n",
              "         [-0.5974, -0.6937]],\n",
              "\n",
              "        [[ 1.6455, -0.8030],\n",
              "         [ 1.3514, -0.2759],\n",
              "         [-1.5108,  2.1048],\n",
              "         [ 2.7630, -1.7465],\n",
              "         [ 1.4516, -1.5103],\n",
              "         [ 0.8212, -0.2115],\n",
              "         [ 0.7789,  1.5333],\n",
              "         [ 1.6097, -0.4032]]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0) #xbow is the average of the previous t! elements\n",
        "\n",
        "xbow # it is as same as left-mulitple  a = torch.tril(torch.ones(3, 3))\n",
        "                                      #a = a / torch.sum(a, 1, keepdim=True)\n"
      ],
      "metadata": {
        "id": "86NuXX0fn7ps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "573bdba0-d7d8-4f55-bdd5-ba17e71f3459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]],\n",
              "\n",
              "        [[ 1.3488, -0.1396],\n",
              "         [ 0.8173,  0.4127],\n",
              "         [-0.1342,  0.4395],\n",
              "         [ 0.2711,  0.4774],\n",
              "         [ 0.2421,  0.0694],\n",
              "         [ 0.0084,  0.0020],\n",
              "         [ 0.0712, -0.1128],\n",
              "         [ 0.2527,  0.2149]],\n",
              "\n",
              "        [[-0.6631, -0.2513],\n",
              "         [ 0.1735, -0.0649],\n",
              "         [ 0.1685,  0.3348],\n",
              "         [-0.1621,  0.1765],\n",
              "         [-0.2312, -0.0436],\n",
              "         [-0.1015, -0.2855],\n",
              "         [-0.2593, -0.1630],\n",
              "         [-0.3015, -0.2293]],\n",
              "\n",
              "        [[ 1.6455, -0.8030],\n",
              "         [ 1.4985, -0.5395],\n",
              "         [ 0.4954,  0.3420],\n",
              "         [ 1.0623, -0.1802],\n",
              "         [ 1.1401, -0.4462],\n",
              "         [ 1.0870, -0.4071],\n",
              "         [ 1.0430, -0.1299],\n",
              "         [ 1.1138, -0.1641]]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "print(\"wei\",wei)\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "print(\"wei sum\",wei)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)\n",
        "print(xbow2,xbow2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "0fc57016-c56d-49be-f4f7-059c3e9a98c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wei tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "wei sum tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.0894, -0.4926],\n",
            "         [ 0.1490, -0.3199],\n",
            "         [ 0.3504, -0.2238],\n",
            "         [ 0.3525,  0.0545],\n",
            "         [ 0.0688, -0.0396],\n",
            "         [ 0.0927, -0.0682],\n",
            "         [-0.0341,  0.1332]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.8173,  0.4127],\n",
            "         [-0.1342,  0.4395],\n",
            "         [ 0.2711,  0.4774],\n",
            "         [ 0.2421,  0.0694],\n",
            "         [ 0.0084,  0.0020],\n",
            "         [ 0.0712, -0.1128],\n",
            "         [ 0.2527,  0.2149]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 0.1735, -0.0649],\n",
            "         [ 0.1685,  0.3348],\n",
            "         [-0.1621,  0.1765],\n",
            "         [-0.2312, -0.0436],\n",
            "         [-0.1015, -0.2855],\n",
            "         [-0.2593, -0.1630],\n",
            "         [-0.3015, -0.2293]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.4985, -0.5395],\n",
            "         [ 0.4954,  0.3420],\n",
            "         [ 1.0623, -0.1802],\n",
            "         [ 1.1401, -0.4462],\n",
            "         [ 1.0870, -0.4071],\n",
            "         [ 1.0430, -0.1299],\n",
            "         [ 1.1138, -0.1641]]]) tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.0894, -0.4926],\n",
            "         [ 0.1490, -0.3199],\n",
            "         [ 0.3504, -0.2238],\n",
            "         [ 0.3525,  0.0545],\n",
            "         [ 0.0688, -0.0396],\n",
            "         [ 0.0927, -0.0682],\n",
            "         [-0.0341,  0.1332]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.8173,  0.4127],\n",
            "         [-0.1342,  0.4395],\n",
            "         [ 0.2711,  0.4774],\n",
            "         [ 0.2421,  0.0694],\n",
            "         [ 0.0084,  0.0020],\n",
            "         [ 0.0712, -0.1128],\n",
            "         [ 0.2527,  0.2149]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 0.1735, -0.0649],\n",
            "         [ 0.1685,  0.3348],\n",
            "         [-0.1621,  0.1765],\n",
            "         [-0.2312, -0.0436],\n",
            "         [-0.1015, -0.2855],\n",
            "         [-0.2593, -0.1630],\n",
            "         [-0.3015, -0.2293]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.4985, -0.5395],\n",
            "         [ 0.4954,  0.3420],\n",
            "         [ 1.0623, -0.1802],\n",
            "         [ 1.1401, -0.4462],\n",
            "         [ 1.0870, -0.4071],\n",
            "         [ 1.0430, -0.1299],\n",
            "         [ 1.1138, -0.1641]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "print(\"tril\",tril)\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "print(\"wei masked_fill \",wei)\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "print(\"wei softmax \",wei)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n",
        "\n",
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOURrfG-ysoL",
        "outputId": "146dfe16-c7cc-4a48-f360-0196f4d29db4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tril tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "wei masked_fill  tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "wei softmax  tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "print(\"key\",key)\n",
        "print(\"query\",query)\n",
        "print(\"value\",value)\n",
        "\n",
        "\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)　　# B 個 　[T,16] @　　[16, T] それぞれかける\n",
        "\n",
        "print( \"x\",x )\n",
        "print( \"k\",k )\n",
        "print( \"q\",q )\n",
        "print( \"wei\",wei )\n",
        "\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "print( \"wei masked_fill\",wei )\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "print( \"wei softmax\",wei )\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "print(\"v\",v)\n",
        "print(\"out\",out)\n",
        "\n",
        "out.shape\n",
        "#print( \"x\",x )\n",
        "#print( \"k\",k )\n",
        "#print( \"q\",q )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDarxEWIRMKq",
        "outputId": "08be696f-1d66-40c5-ca5b-b6c65b9db874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "key Linear(in_features=32, out_features=16, bias=False)\n",
            "query Linear(in_features=32, out_features=16, bias=False)\n",
            "value Linear(in_features=32, out_features=16, bias=False)\n",
            "x tensor([[[ 0.1808, -0.0700, -0.3596,  ..., -0.8016,  1.5236,  2.5086],\n",
            "         [-0.6631, -0.2513,  1.0101,  ...,  1.5333,  1.6097, -0.4032],\n",
            "         [-0.8345,  0.5978, -0.0514,  ..., -0.4370, -1.0012, -0.4094],\n",
            "         ...,\n",
            "         [-0.8961,  0.0662, -0.0563,  ...,  2.1382,  0.5114,  1.2191],\n",
            "         [ 0.1910, -0.3425,  1.7955,  ...,  0.3699, -0.5556, -0.3983],\n",
            "         [-0.5819, -0.2208,  0.0135,  ..., -1.9079, -0.5276,  1.0807]],\n",
            "\n",
            "        [[ 0.4562, -1.0917, -0.8207,  ...,  0.0512, -0.6576, -2.5729],\n",
            "         [ 0.0210,  1.0060, -1.2492,  ...,  0.7859, -1.1501,  1.3132],\n",
            "         [ 2.2007, -0.2195,  0.5427,  ..., -0.6445,  1.0834, -0.7995],\n",
            "         ...,\n",
            "         [ 0.3091,  1.1661, -2.1821,  ...,  0.6151,  0.6763,  0.6228],\n",
            "         [ 0.0943, -0.3156,  0.7850,  ..., -1.5735,  1.3876,  0.7251],\n",
            "         [ 0.6455, -0.3313, -1.0390,  ...,  0.0895, -0.3748, -0.4781]],\n",
            "\n",
            "        [[-0.6067,  1.8328,  0.2931,  ...,  1.0041,  0.8656,  0.1688],\n",
            "         [-0.2352, -0.2586,  0.0131,  ...,  0.6690,  0.7535, -0.5359],\n",
            "         [-1.0277,  0.5347, -0.7958,  ...,  1.0711,  0.4901, -0.4876],\n",
            "         ...,\n",
            "         [-0.6896, -0.7080, -0.3152,  ..., -2.0662, -1.1418, -0.1391],\n",
            "         [ 1.0827,  1.1522,  0.5198,  ...,  0.4970,  0.0585,  0.1033],\n",
            "         [ 0.0720,  1.1080,  0.7293,  ...,  0.3967, -0.9755,  0.5122]],\n",
            "\n",
            "        [[ 0.3330,  1.0995,  0.4034,  ...,  1.6634, -0.4718,  0.5857],\n",
            "         [-0.9579,  0.9435, -2.1992,  ..., -0.7296,  0.1653, -0.3390],\n",
            "         [ 1.5416,  1.0231,  1.3392,  ..., -0.0433, -0.2505, -0.7493],\n",
            "         ...,\n",
            "         [ 0.7450,  0.7170,  1.2668,  ...,  1.9359,  2.0350,  2.0187],\n",
            "         [ 0.0323, -0.6337,  0.2938,  ..., -0.3297, -0.0192,  0.9225],\n",
            "         [ 0.9187,  0.2998,  0.6106,  ...,  0.8282, -0.4826,  1.8330]]])\n",
            "k tensor([[[ 1.1965e-01, -3.0127e-01,  3.6293e-01,  1.1771e+00,  1.1385e+00,\n",
            "          -2.5543e-01,  1.4537e-01, -2.9437e-01, -7.0201e-01, -1.0308e+00,\n",
            "           7.4357e-01, -8.0984e-01, -6.6687e-01,  9.1233e-02, -6.0747e-03,\n",
            "           1.9833e-01],\n",
            "         [-5.4229e-01, -5.5581e-01, -7.6131e-02,  1.2929e+00,  8.6535e-01,\n",
            "          -1.1998e+00,  3.8781e-01,  1.9389e-01,  7.0235e-01, -8.2251e-01,\n",
            "           2.3484e-01, -8.4995e-01, -3.8126e-01, -2.9906e-01,  1.0242e-02,\n",
            "          -5.5449e-01],\n",
            "         [-3.7359e-01, -4.6781e-01, -2.1560e-01, -8.0344e-01, -3.7153e-01,\n",
            "          -5.4427e-01, -9.1455e-01, -5.5926e-02, -3.2903e-01, -2.1023e-01,\n",
            "           1.1665e-01, -1.7978e-01, -2.8196e-01, -3.3204e-01, -4.5963e-01,\n",
            "          -1.3255e-01],\n",
            "         [-3.1463e-01,  8.4460e-02, -1.2351e-01, -7.0577e-01, -1.8022e-01,\n",
            "           5.4922e-01, -8.9805e-01, -4.9384e-01,  6.7907e-01,  8.8270e-01,\n",
            "           4.9109e-01,  5.1903e-01,  9.0109e-01,  9.1255e-02, -1.9332e-01,\n",
            "          -6.7704e-01],\n",
            "         [ 2.3940e-02,  9.9822e-02, -1.8709e-01, -8.5960e-02, -4.8815e-01,\n",
            "          -1.6765e+00,  2.4126e-01,  7.3606e-01,  4.6080e-01, -8.7217e-01,\n",
            "          -4.2590e-01, -1.1347e+00, -1.0571e+00, -9.4006e-01,  1.3426e-01,\n",
            "          -1.5716e-02],\n",
            "         [-2.3618e-01, -7.8730e-01, -3.8019e-01,  5.8150e-01, -3.7222e-01,\n",
            "           1.2405e+00, -7.0045e-01, -1.4917e+00,  7.6784e-01,  3.5839e-01,\n",
            "           6.1200e-01, -7.9353e-02,  5.9827e-01,  2.6353e-01,  6.4905e-01,\n",
            "           7.0914e-02],\n",
            "         [-7.9413e-01, -1.6598e-01, -2.8096e-01, -1.0208e-01, -7.3521e-01,\n",
            "          -7.5183e-01, -1.2759e-01, -5.1134e-03,  3.3249e-01, -3.3738e-01,\n",
            "           1.6783e-01,  3.1048e-01,  2.2577e-01,  1.2434e-01,  4.6169e-01,\n",
            "           2.0156e-01],\n",
            "         [ 1.6513e-01, -1.5990e-01, -5.7168e-01, -3.9571e-01,  3.9301e-01,\n",
            "          -8.5665e-01,  3.3900e-01, -7.9771e-01,  2.2134e-01, -5.1612e-01,\n",
            "           1.8504e-01, -2.1048e-01,  3.7789e-01,  4.8222e-02, -4.7437e-01,\n",
            "          -5.0405e-02]],\n",
            "\n",
            "        [[-1.6977e-01, -1.5875e+00, -9.1855e-01,  6.6326e-02, -1.1497e+00,\n",
            "           2.7652e-01, -7.1052e-01, -6.0851e-01, -7.9616e-02, -1.3215e-01,\n",
            "           6.9567e-01,  6.7096e-01,  5.4679e-01,  7.6157e-01,  6.3947e-01,\n",
            "           5.8098e-01],\n",
            "         [-1.1435e-01, -3.5312e-01, -1.8434e-01,  5.2000e-01, -6.0603e-01,\n",
            "           6.3977e-01,  1.2789e-01, -8.0061e-01, -3.9588e-01,  9.8180e-01,\n",
            "          -2.7790e-01, -4.0351e-01, -6.6473e-01,  2.3659e-01,  2.4786e-01,\n",
            "           2.3966e-01],\n",
            "         [-6.3508e-01, -1.0090e+00,  4.4846e-01,  2.6102e-01,  3.0953e-01,\n",
            "           1.0269e+00, -5.0824e-01,  1.5112e-01,  4.9967e-01, -1.0242e+00,\n",
            "           3.3076e-02,  7.9948e-01,  4.7760e-01,  1.0383e-01,  2.8658e-01,\n",
            "           6.3477e-01],\n",
            "         [ 7.1183e-02,  5.7131e-01,  6.2270e-01,  2.4220e-01,  1.1163e+00,\n",
            "           5.2713e-01, -2.7616e-01, -2.8885e-01,  1.6921e-01,  1.0390e+00,\n",
            "          -1.2049e-01, -7.5153e-01,  2.8590e-01, -3.0348e-01, -3.1344e-02,\n",
            "          -6.0875e-01],\n",
            "         [-3.2904e-02,  5.3796e-01,  5.0853e-02,  1.1635e+00, -1.3198e-01,\n",
            "          -8.2809e-01,  3.2218e-01,  2.0548e-01, -1.3409e-01, -2.4342e-01,\n",
            "          -5.2483e-01, -1.0036e+00,  1.4676e-01,  6.1899e-02,  1.1584e-01,\n",
            "          -1.9803e-01],\n",
            "         [-1.5397e-01,  6.4264e-01, -1.2269e-01,  4.0754e-01,  7.2767e-02,\n",
            "          -2.1382e+00,  2.0803e+00,  1.0649e+00,  1.3110e-01, -1.7620e-01,\n",
            "          -1.1203e-02, -1.7259e+00, -7.9774e-01,  1.2679e+00, -6.2352e-03,\n",
            "          -2.9788e-02],\n",
            "         [ 7.5567e-01, -1.1675e-01, -7.9704e-01,  1.6243e-02,  8.6796e-01,\n",
            "          -2.0754e-01,  1.0132e+00, -8.4467e-01,  3.1207e-01,  3.1308e-01,\n",
            "          -4.0251e-01, -5.5028e-01, -1.1918e-02,  1.1328e-01, -1.2361e-01,\n",
            "           2.7873e-01],\n",
            "         [ 1.9565e-01,  1.5312e-01, -2.6387e-01, -9.0676e-01, -8.9970e-01,\n",
            "          -1.5432e-01,  2.9018e-01,  5.1112e-01,  3.9277e-01,  1.4502e-01,\n",
            "          -8.6059e-02,  1.0033e+00,  2.9766e-01, -4.0496e-02, -2.7407e-01,\n",
            "           6.2894e-01]],\n",
            "\n",
            "        [[ 2.1920e-01, -4.3338e-01, -1.7334e-02,  6.1086e-02, -5.0162e-01,\n",
            "          -9.1736e-01, -2.8565e-02, -2.9307e-01,  1.9116e-01,  4.5901e-01,\n",
            "          -6.4669e-01,  2.8410e-01,  7.1452e-01,  5.5001e-01,  7.2716e-02,\n",
            "           1.0264e+00],\n",
            "         [ 1.6208e-01,  4.7036e-01, -1.7571e-01, -1.4430e-01, -4.1618e-01,\n",
            "          -2.7120e-01,  1.7485e-01,  3.4478e-01,  2.0791e-03, -8.3833e-01,\n",
            "           4.8237e-01,  1.4978e-01,  2.6961e-01,  3.1957e-01,  3.1318e-01,\n",
            "           2.4300e-01],\n",
            "         [ 2.3199e-02,  9.1282e-01,  1.2309e-01,  4.3552e-01,  3.1683e-01,\n",
            "           5.4443e-01, -4.1182e-01, -3.9750e-01, -4.6773e-01,  1.4980e-01,\n",
            "          -7.6691e-04,  1.9398e-01, -5.9607e-02,  2.7678e-01,  3.8587e-01,\n",
            "           1.0099e-01],\n",
            "         [ 3.8775e-01, -7.5004e-01,  4.4353e-01,  2.0455e-01,  5.7050e-01,\n",
            "           5.2300e-01, -5.5296e-01,  3.4047e-01, -3.5511e-01, -6.9000e-01,\n",
            "           1.3859e-01, -6.1129e-01,  2.7986e-01, -1.0584e+00, -3.4378e-01,\n",
            "          -6.7254e-01],\n",
            "         [-7.4944e-01,  1.2011e+00,  4.7504e-01, -1.4175e+00, -1.1661e-01,\n",
            "          -2.0519e-01,  4.8880e-02, -5.6190e-01,  1.6865e-01, -5.8476e-01,\n",
            "           1.5643e-01,  2.0620e-01,  3.0129e-01,  3.0515e-01,  1.5822e-01,\n",
            "           1.3580e-01],\n",
            "         [ 3.2854e-03,  8.5792e-02, -3.7273e-01, -4.3263e-01,  2.7126e-01,\n",
            "           5.5295e-01, -3.3752e-01, -3.6228e-01,  3.7946e-01, -5.6956e-01,\n",
            "           3.5850e-01,  5.0297e-01,  8.3247e-01,  2.7065e-01,  2.3051e-01,\n",
            "          -3.7021e-01],\n",
            "         [ 2.6607e-01,  7.4628e-01,  9.7758e-01,  8.5964e-01,  7.2511e-01,\n",
            "          -6.1081e-01, -6.5677e-01, -5.0406e-02, -5.2642e-02,  5.8294e-01,\n",
            "          -4.5590e-02, -3.5460e-02,  9.0733e-01,  2.4781e-01, -1.8979e-01,\n",
            "          -9.3868e-01],\n",
            "         [-9.1529e-01, -9.2379e-01,  2.2234e-01, -3.1099e-01,  3.9580e-01,\n",
            "           5.2756e-01, -4.7417e-01, -2.0447e-01, -2.5679e-01,  3.5713e-01,\n",
            "           1.9908e-01,  7.3336e-02,  6.5161e-01, -2.3829e-01,  5.5460e-01,\n",
            "          -1.9587e-01]],\n",
            "\n",
            "        [[-1.5028e-01, -6.7611e-01, -9.4847e-02,  6.0556e-02, -1.2049e-01,\n",
            "           1.1210e-01,  5.8812e-01,  4.8340e-01, -4.8511e-01,  2.8539e-01,\n",
            "           1.1188e-01, -5.7574e-01,  1.4927e-01,  2.4169e-01, -1.1611e-01,\n",
            "          -1.2201e-01],\n",
            "         [ 3.1164e-01, -9.0459e-02, -2.8066e-01,  2.6897e-01,  6.4195e-01,\n",
            "          -6.5475e-01,  1.1037e+00, -4.5296e-01, -2.1339e-02,  1.6460e-01,\n",
            "           6.7715e-01,  2.3949e-01, -4.3220e-01,  9.4793e-01,  1.7489e-01,\n",
            "          -1.5304e-01],\n",
            "         [-7.9503e-01, -1.4741e+00,  1.1253e+00,  2.7440e-01, -1.4027e+00,\n",
            "           3.7211e-01, -3.9604e-01,  8.4127e-01,  3.5312e-01,  1.4552e-01,\n",
            "           4.2781e-01,  1.1326e+00,  1.9570e-01,  4.9587e-01,  1.9671e-01,\n",
            "           7.6903e-01],\n",
            "         [-3.3249e-01, -5.9558e-01, -2.0805e-01, -5.2000e-01, -1.0317e-01,\n",
            "          -1.0147e+00,  1.2277e-01,  5.6320e-01,  2.6245e-02, -2.8960e-01,\n",
            "           4.3904e-01,  2.1655e-01, -5.1625e-01,  6.9628e-01, -7.1387e-02,\n",
            "           5.5466e-01],\n",
            "         [ 7.1868e-01, -6.9756e-01,  1.7511e-01,  4.2946e-01, -6.4692e-02,\n",
            "           2.1727e-02,  1.6995e-01,  1.0254e-01,  1.7318e-02, -3.4716e-01,\n",
            "          -2.9683e-02,  2.2608e-01, -4.8018e-01, -3.1518e-01,  2.7052e-01,\n",
            "           2.4391e-01],\n",
            "         [-1.8101e-01,  9.1307e-01,  3.3671e-01,  2.0421e-01, -3.9466e-02,\n",
            "           4.7134e-01,  2.1301e-01,  8.5801e-01,  1.5041e-01, -3.7583e-01,\n",
            "          -1.2250e-01, -7.5942e-01,  6.6617e-02, -5.8663e-01, -1.5167e-02,\n",
            "           1.1934e-01],\n",
            "         [-3.1353e-02, -6.3727e-01, -5.9223e-01,  5.9708e-01,  2.5528e-01,\n",
            "          -1.6740e-01,  1.5375e-01, -1.4879e+00,  1.2765e-01,  1.8780e-01,\n",
            "           3.5408e-01, -4.7336e-02, -2.5101e-01,  9.1024e-01, -6.3272e-01,\n",
            "          -2.5878e-01],\n",
            "         [-1.2732e+00, -6.2869e-01,  5.6168e-02, -2.5593e-03, -7.3370e-01,\n",
            "          -2.7521e-01, -1.5650e-01,  3.9314e-01, -4.1830e-01, -1.7399e+00,\n",
            "           6.3731e-01, -6.3222e-01,  4.7992e-01,  1.8370e-01,  1.0338e+00,\n",
            "          -5.4454e-01]]], grad_fn=<UnsafeViewBackward0>)\n",
            "q tensor([[[-6.5674e-01,  2.8302e-02,  9.4470e-03, -6.9949e-01, -3.6043e-01,\n",
            "           8.3760e-01, -4.4455e-01,  1.2278e-01,  6.2761e-01, -6.2222e-01,\n",
            "           3.4833e-01,  2.4108e-01,  5.4092e-01, -2.6054e-01,  3.6119e-01,\n",
            "          -4.3574e-02],\n",
            "         [-3.9319e-01,  8.2196e-01, -7.0274e-01,  9.5429e-02, -1.2218e-01,\n",
            "          -1.5182e-01, -5.0242e-01, -4.6365e-01,  1.1758e-01,  1.4282e+00,\n",
            "          -5.8116e-01,  1.4008e-01,  9.6041e-01,  4.1002e-02, -6.2136e-01,\n",
            "          -6.3472e-01],\n",
            "         [ 2.1567e-01, -3.5065e-01,  2.1671e-03,  4.2317e-01, -2.2844e-01,\n",
            "          -7.3162e-02, -3.4118e-01,  9.6471e-01, -5.1775e-01,  9.2104e-02,\n",
            "          -5.0425e-01,  8.3885e-01,  6.1487e-01, -1.0894e-02, -5.5692e-01,\n",
            "           5.8197e-01],\n",
            "         [ 8.9999e-01, -1.2723e-01,  5.4581e-01,  4.2544e-01, -4.5128e-01,\n",
            "          -2.1242e-02,  1.7111e-01,  2.5990e-01, -9.9782e-01,  4.8897e-01,\n",
            "           1.7374e-01, -6.9986e-02, -3.1131e-01,  3.7479e-01, -1.8482e-01,\n",
            "          -6.3789e-01],\n",
            "         [ 3.3199e-02,  5.8858e-01, -4.4368e-01,  3.7748e-01, -6.8257e-01,\n",
            "          -2.7749e-01,  4.6726e-01, -1.2956e+00,  6.6032e-01,  1.6333e-01,\n",
            "          -1.7573e+00, -6.5818e-01, -2.3023e-01, -8.6169e-02, -5.9972e-03,\n",
            "           7.5729e-01],\n",
            "         [ 2.0985e-01,  4.3915e-02, -7.0198e-02,  7.2701e-02, -2.0124e-01,\n",
            "          -1.7539e+00,  1.0369e+00,  1.1635e-01,  2.9557e-01,  3.2307e-01,\n",
            "           5.0523e-01,  7.0110e-01, -2.8444e-01, -7.8443e-01,  4.7822e-01,\n",
            "          -5.1704e-01],\n",
            "         [ 6.1001e-01, -3.2841e-01, -8.5571e-01,  8.5427e-01,  7.8055e-01,\n",
            "          -4.0234e-01, -8.1832e-01, -5.5446e-02,  1.8732e-01,  2.7065e-01,\n",
            "          -7.0659e-01, -8.6369e-01,  6.9979e-01, -6.6958e-02,  2.5508e-01,\n",
            "           2.1492e-01],\n",
            "         [ 1.4591e-01,  1.3493e-01, -2.3353e-01, -4.1732e-02,  2.9277e-01,\n",
            "          -5.0801e-01,  1.1770e-01,  1.8610e-01,  1.4554e-01,  2.9240e-02,\n",
            "          -8.4698e-01,  6.1163e-01,  1.2445e+00,  1.9087e-01,  3.6944e-01,\n",
            "          -2.7448e-03]],\n",
            "\n",
            "        [[ 1.1104e+00, -8.7192e-01,  7.0978e-01,  3.6331e-01,  2.0670e-01,\n",
            "          -3.5486e-02, -3.1695e-02,  6.9234e-01, -4.1590e-01, -1.6547e+00,\n",
            "           4.3214e-01, -1.1557e+00,  7.1400e-02, -6.7659e-01,  6.0415e-01,\n",
            "          -5.9200e-01],\n",
            "         [ 3.2561e-01,  5.7866e-01,  5.4575e-01, -7.2274e-01,  1.2343e+00,\n",
            "          -1.5586e-01,  6.8699e-01, -6.3906e-01,  6.1569e-01,  2.1342e-01,\n",
            "          -9.3616e-01,  2.7811e-01,  9.5776e-01,  1.7266e-01, -1.6889e-01,\n",
            "          -1.7047e-02],\n",
            "         [-1.5634e-02, -5.4639e-01,  3.0958e-01,  3.5532e-01,  5.9885e-01,\n",
            "          -8.2791e-01, -5.9326e-01,  7.3282e-01, -4.5197e-01, -8.4692e-01,\n",
            "           5.1515e-01, -1.0304e-02, -2.4767e-01, -6.7420e-02,  1.9623e-03,\n",
            "          -8.3188e-01],\n",
            "         [-2.4959e-01,  2.7492e-01,  2.6894e-01, -3.6563e-01, -3.2585e-01,\n",
            "           3.7158e-01, -8.7898e-01,  1.5132e-01,  3.0180e-02,  3.2213e-01,\n",
            "           3.9398e-01,  6.9950e-01,  9.7176e-02,  8.0347e-02, -1.1911e-02,\n",
            "           3.9823e-01],\n",
            "         [ 3.9181e-01,  5.7756e-01,  1.3630e-01, -3.3129e-01,  3.4955e-01,\n",
            "           3.3893e-01,  2.8573e-01, -3.3917e-01,  6.8701e-01,  3.2722e-01,\n",
            "          -1.0067e+00, -5.3265e-01,  1.0750e+00,  2.7662e-01, -5.8393e-01,\n",
            "          -3.2861e-01],\n",
            "         [ 6.6613e-01,  2.1817e+00, -4.7026e-01,  5.5768e-02, -8.0701e-01,\n",
            "           6.1819e-01,  1.8163e-01, -5.4206e-01,  8.6598e-01,  9.1274e-01,\n",
            "          -1.1465e+00,  1.2842e+00,  2.2156e+00,  8.1063e-01, -1.0830e+00,\n",
            "          -1.6162e-01],\n",
            "         [-5.6865e-01,  4.0198e-01, -5.5940e-01,  2.4041e-01,  2.5784e-02,\n",
            "          -4.5127e-01,  2.0618e-01, -1.1354e-01, -5.3368e-01,  9.9677e-01,\n",
            "          -3.4785e-01,  3.3627e-02,  1.3022e-01, -3.5643e-01,  4.5948e-01,\n",
            "          -3.3823e-01],\n",
            "         [ 5.9567e-01,  2.7697e-01, -5.3694e-01,  3.8806e-01, -5.2068e-01,\n",
            "           6.3736e-02, -4.6341e-01,  1.6976e-01, -6.2182e-01, -8.5360e-01,\n",
            "           1.5969e-02, -4.1913e-01,  7.5529e-01,  3.6444e-01,  4.0385e-01,\n",
            "          -1.9791e-01]],\n",
            "\n",
            "        [[ 1.3326e+00,  1.0350e+00, -1.3503e-02, -9.2348e-01,  1.0694e+00,\n",
            "          -1.4107e-01,  4.7608e-01, -2.5034e-01, -2.9666e-02, -4.9094e-01,\n",
            "          -6.6426e-01,  9.3041e-02,  1.4563e+00,  1.4807e-01, -6.1347e-01,\n",
            "          -1.0926e+00],\n",
            "         [ 5.4338e-01, -1.9188e-01, -5.3040e-01,  6.8131e-01,  6.2352e-02,\n",
            "          -2.8209e-01, -1.6728e-02,  5.4435e-01, -3.0124e-01, -1.1855e-01,\n",
            "          -1.8417e-01, -4.8132e-01, -2.1184e-01,  2.4121e-01,  2.1798e-02,\n",
            "          -6.9178e-02],\n",
            "         [-3.3528e-01, -5.6466e-01,  6.6484e-01, -1.5190e-01,  3.6304e-01,\n",
            "           6.2404e-01, -2.4197e-01,  1.1305e+00,  1.7005e-01,  3.6276e-01,\n",
            "           8.0700e-01, -1.0924e-01,  1.9538e-01, -4.2642e-01, -6.4841e-01,\n",
            "          -2.0725e-01],\n",
            "         [-1.0337e-01,  8.0347e-02, -3.1804e-01, -1.0417e+00,  6.4425e-02,\n",
            "          -4.2446e-01,  4.8210e-01,  2.6664e-01, -5.0266e-01, -1.4787e+00,\n",
            "           1.0776e+00,  2.1790e-01, -8.7924e-01,  1.0318e-01,  1.1044e-01,\n",
            "          -8.6890e-01],\n",
            "         [ 7.3975e-01, -5.6316e-01,  6.8825e-01,  6.8188e-01,  9.2013e-01,\n",
            "          -6.6580e-01, -2.3015e-01,  3.4094e-01,  4.4395e-01,  6.7518e-01,\n",
            "           2.6042e-01,  1.4252e+00,  7.6539e-01,  2.5805e-01, -7.9161e-01,\n",
            "           7.3077e-01],\n",
            "         [ 4.9907e-02, -4.8685e-01,  2.1094e-01, -3.5810e-01,  1.2313e-01,\n",
            "           1.5951e-01,  1.7245e-01,  2.8349e-01, -2.1648e-01, -5.0059e-01,\n",
            "          -2.0210e-01,  2.4838e-01,  1.3610e-02,  1.0566e+00,  2.7065e-01,\n",
            "          -4.7702e-01],\n",
            "         [-6.4012e-01,  4.7867e-02, -2.7659e-02, -5.3705e-01,  4.5048e-01,\n",
            "           2.2384e-01, -1.1379e+00,  5.9782e-01,  1.0890e-02, -4.5584e-01,\n",
            "           7.9715e-01,  3.0061e-01,  7.8801e-01, -2.9773e-01, -1.7181e-01,\n",
            "          -7.1184e-01],\n",
            "         [ 4.1349e-02, -6.6965e-01, -4.0473e-01, -8.1760e-01, -1.4332e-01,\n",
            "          -2.2694e-01,  8.5180e-02,  3.4696e-01,  2.9030e-02,  2.2824e-01,\n",
            "           4.9848e-01,  3.6049e-01, -4.2176e-01, -5.3471e-01, -5.0211e-02,\n",
            "           1.8603e-01]],\n",
            "\n",
            "        [[ 6.2212e-04,  3.1138e-01, -7.1241e-01, -5.4445e-01,  8.2328e-01,\n",
            "          -1.6868e-01,  2.2658e-01,  4.8862e-01, -7.2207e-01,  3.6705e-01,\n",
            "          -1.3507e-01,  3.5477e-02, -4.4308e-01, -4.5602e-01, -9.0744e-01,\n",
            "          -2.2787e-01],\n",
            "         [-8.5366e-01, -1.1014e-02, -1.6077e-01,  1.7789e-03, -1.3390e-01,\n",
            "          -4.7289e-01, -2.1686e-01,  4.3677e-01,  8.0428e-01,  1.0344e+00,\n",
            "           8.8352e-03,  2.7911e-01, -7.6171e-02, -7.9940e-01,  4.4846e-01,\n",
            "           7.0971e-01],\n",
            "         [ 1.4088e+00, -4.4377e-02, -2.5436e-03,  1.2366e-01,  5.7979e-01,\n",
            "          -7.1957e-01, -5.0969e-01, -8.0928e-01, -8.9076e-02, -7.4968e-02,\n",
            "          -8.5395e-01, -1.5098e+00, -1.1381e+00, -6.0501e-02,  1.5464e-01,\n",
            "           3.2477e-01],\n",
            "         [ 1.9741e-01, -2.5031e-01, -1.2182e-01,  4.9976e-01,  2.4592e-01,\n",
            "           6.9912e-01, -4.1537e-01, -1.3993e+00,  5.9324e-01,  7.7563e-02,\n",
            "          -1.0144e+00, -8.3186e-01,  1.7011e-01, -3.2685e-01,  6.3889e-01,\n",
            "          -9.1289e-03],\n",
            "         [-1.1387e-02,  8.6315e-01,  2.5427e-01, -3.2685e-02, -1.1675e-02,\n",
            "          -6.4872e-01,  1.3967e-01, -1.2100e-01, -3.6965e-01, -3.9830e-01,\n",
            "           2.0917e-01, -4.2211e-02,  3.5471e-01, -1.4781e-02,  1.0395e-01,\n",
            "          -8.2862e-01],\n",
            "         [-3.7612e-01, -7.3518e-02, -1.1904e+00,  7.2211e-01,  2.6136e-01,\n",
            "          -3.6523e-01,  1.0752e+00, -4.8674e-01, -4.3567e-01,  1.3338e-01,\n",
            "           6.0010e-01, -3.8806e-01, -1.5267e+00, -3.4049e-01,  3.2029e-01,\n",
            "          -3.5348e-01],\n",
            "         [-3.8176e-01,  7.8970e-01,  8.1802e-01,  8.8146e-01,  7.0618e-01,\n",
            "          -6.2861e-01, -6.7371e-01, -1.7663e-01,  5.2108e-01,  6.6437e-01,\n",
            "          -8.8697e-01,  5.0958e-02,  6.5746e-01, -6.3669e-01, -8.9697e-02,\n",
            "           3.2022e-01],\n",
            "         [-3.0550e-01,  8.9354e-02, -2.3808e-01,  1.0563e+00,  3.4164e-01,\n",
            "          -9.2939e-01,  8.5246e-01,  2.3477e-02,  1.6643e-01, -1.2088e+00,\n",
            "          -2.5446e-01,  6.6724e-01, -1.5612e-01, -1.7337e-01,  1.2187e-01,\n",
            "           2.0050e-01]]], grad_fn=<UnsafeViewBackward0>)\n",
            "wei tensor([[[-1.7629e+00, -1.3011e+00,  5.6516e-01,  2.1616e+00, -1.0674e+00,\n",
            "           1.9632e+00,  1.0765e+00, -4.5295e-01],\n",
            "         [-3.3334e+00, -1.6556e+00,  1.0405e-01,  3.3782e+00, -2.1825e+00,\n",
            "           1.0415e+00, -5.5714e-02,  2.9273e-01],\n",
            "         [-1.0226e+00, -1.2606e+00,  7.6228e-02, -3.8125e-01, -9.8430e-01,\n",
            "          -1.4303e+00,  7.4921e-02, -9.5465e-01],\n",
            "         [ 7.8359e-01, -8.0143e-01, -3.3680e-01, -8.4963e-01, -5.6023e-01,\n",
            "          -1.1701e+00, -1.2927e+00, -1.0260e+00],\n",
            "         [-1.2566e+00,  1.8719e-02, -7.8797e-01, -1.3204e+00,  2.0363e+00,\n",
            "           8.6381e-01,  3.7188e-01,  9.2577e-01],\n",
            "         [-3.1262e-01,  2.4152e+00, -1.1058e-01, -9.9305e-01,  3.3449e+00,\n",
            "          -2.5229e+00,  1.4187e+00,  1.2196e+00],\n",
            "         [ 1.0876e+00,  1.9652e+00, -2.6213e-01, -3.1579e-01,  6.0905e-01,\n",
            "           1.2616e+00, -5.4841e-01,  8.0485e-01],\n",
            "         [-1.8044e+00, -4.1260e-01, -8.3061e-01,  5.8985e-01, -7.9869e-01,\n",
            "          -5.8560e-01,  6.4332e-01,  6.3028e-01]],\n",
            "\n",
            "        [[-7.3529e-01, -1.7807e+00,  1.0745e+00, -2.7429e-01,  1.6347e+00,\n",
            "           1.4177e+00, -5.5213e-01, -2.3580e+00],\n",
            "         [-3.0892e+00, -1.4943e+00, -2.6167e-01,  2.2760e+00, -2.4364e-01,\n",
            "           1.6198e-01,  2.5783e+00,  3.9591e-01],\n",
            "         [-5.0206e-01, -2.0745e+00,  5.3785e-01, -4.0494e-01,  8.3292e-01,\n",
            "           1.3570e+00, -1.5621e+00, -1.6490e+00],\n",
            "         [ 1.3810e+00, -1.4713e-01,  1.2181e+00, -2.2266e-01, -1.8247e+00,\n",
            "          -3.7044e+00, -2.1321e+00,  1.3178e+00],\n",
            "         [-2.3568e+00, -4.6170e-01, -8.8196e-01,  2.3700e+00,  6.7828e-01,\n",
            "           1.6262e-01,  1.9379e+00,  1.0397e-01],\n",
            "         [-9.2435e-01, -6.2351e-01, -1.3938e+00,  1.3336e+00, -8.9731e-03,\n",
            "          -3.1789e+00,  9.0259e-01,  3.6256e+00],\n",
            "         [-6.5522e-01,  1.0991e+00, -2.1399e+00,  9.6468e-01,  9.9463e-01,\n",
            "           9.3899e-01,  4.6799e-01, -3.5870e-01],\n",
            "         [ 1.5463e+00, -4.9438e-01, -1.4180e-02, -9.7428e-01,  1.3779e+00,\n",
            "           7.8648e-03, -5.3590e-01, -4.5531e-01]],\n",
            "\n",
            "        [[-3.7898e-01,  5.1592e-01,  3.0332e-01,  1.1303e+00,  2.0511e+00,\n",
            "           2.2323e+00,  3.1239e+00, -1.2231e+00],\n",
            "         [ 1.0377e-01,  1.7584e-01, -1.6369e-01,  5.2328e-01, -2.2172e+00,\n",
            "          -8.7770e-01,  1.7020e-01, -1.0842e+00],\n",
            "         [-1.6373e+00, -6.5557e-01, -8.5031e-01,  2.3457e+00, -9.9497e-01,\n",
            "          -4.9228e-02,  5.5157e-01,  1.5285e+00],\n",
            "         [-2.7155e+00,  1.9022e+00, -8.4620e-01,  5.9058e-01,  2.1122e+00,\n",
            "           8.8971e-01, -2.0679e+00, -7.4249e-01],\n",
            "         [ 2.5044e+00, -4.9691e-01, -2.6300e-01, -1.6288e-01, -1.7459e+00,\n",
            "           8.6298e-02,  2.7739e+00, -2.4952e-02],\n",
            "         [-4.8634e-02,  4.9620e-01, -2.0859e-01, -8.4632e-02,  3.6811e-01,\n",
            "           7.8713e-01, -1.9678e-01,  4.1090e-01],\n",
            "         [-1.7485e+00,  4.6233e-01,  3.8654e-03,  2.1114e+00,  1.2731e+00,\n",
            "           2.1582e+00,  1.3125e+00,  2.0600e+00],\n",
            "         [-8.5500e-02, -1.5414e-02, -1.3915e+00,  6.3086e-02, -2.4530e-01,\n",
            "          -2.0677e-01, -2.2102e+00,  4.4531e-01]],\n",
            "\n",
            "        [[ 4.5165e-01,  3.2148e-01, -3.1926e+00,  3.0765e-01, -6.1612e-01,\n",
            "           2.5626e-01, -2.9891e-01, -2.1917e+00],\n",
            "         [-4.0009e-01, -9.6205e-01,  1.9568e+00,  6.6612e-01, -3.2630e-01,\n",
            "           2.6258e-01, -1.3973e+00, -8.9450e-01],\n",
            "         [-4.6199e-01,  5.8600e-01, -4.6738e+00, -3.2178e-01,  1.2684e+00,\n",
            "          -1.7402e-01,  1.2461e+00, -2.2283e+00],\n",
            "         [-7.1746e-01, -1.0279e+00, -2.0509e+00, -2.7234e+00,  3.1231e-01,\n",
            "          -1.6416e-01,  1.5162e+00, -7.7670e-01],\n",
            "         [-4.0388e-01,  5.1597e-01, -2.0697e+00, -4.0982e-01, -8.0534e-01,\n",
            "           5.2210e-01, -4.1242e-01,  1.3377e+00],\n",
            "         [ 8.2322e-01,  3.0237e+00, -3.0655e+00,  7.0404e-01,  6.7207e-01,\n",
            "          -4.6692e-01,  2.3746e+00,  3.1181e-01],\n",
            "         [-1.4141e+00, -1.4241e+00, -8.0387e-01, -1.7450e+00, -7.4035e-01,\n",
            "           9.8188e-01, -9.0056e-01, -2.3158e+00],\n",
            "         [-5.0277e-01,  1.6844e+00, -4.1847e-01,  1.0239e+00,  1.0275e+00,\n",
            "           1.3980e-01,  4.8822e-01,  1.5573e+00]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "wei masked_fill tensor([[[-1.7629e+00,        -inf,        -inf,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-3.3334e+00, -1.6556e+00,        -inf,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-1.0226e+00, -1.2606e+00,  7.6228e-02,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [ 7.8359e-01, -8.0143e-01, -3.3680e-01, -8.4963e-01,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-1.2566e+00,  1.8719e-02, -7.8797e-01, -1.3204e+00,  2.0363e+00,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-3.1262e-01,  2.4152e+00, -1.1058e-01, -9.9305e-01,  3.3449e+00,\n",
            "          -2.5229e+00,        -inf,        -inf],\n",
            "         [ 1.0876e+00,  1.9652e+00, -2.6213e-01, -3.1579e-01,  6.0905e-01,\n",
            "           1.2616e+00, -5.4841e-01,        -inf],\n",
            "         [-1.8044e+00, -4.1260e-01, -8.3061e-01,  5.8985e-01, -7.9869e-01,\n",
            "          -5.8560e-01,  6.4332e-01,  6.3028e-01]],\n",
            "\n",
            "        [[-7.3529e-01,        -inf,        -inf,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-3.0892e+00, -1.4943e+00,        -inf,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-5.0206e-01, -2.0745e+00,  5.3785e-01,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [ 1.3810e+00, -1.4713e-01,  1.2181e+00, -2.2266e-01,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-2.3568e+00, -4.6170e-01, -8.8196e-01,  2.3700e+00,  6.7828e-01,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-9.2435e-01, -6.2351e-01, -1.3938e+00,  1.3336e+00, -8.9731e-03,\n",
            "          -3.1789e+00,        -inf,        -inf],\n",
            "         [-6.5522e-01,  1.0991e+00, -2.1399e+00,  9.6468e-01,  9.9463e-01,\n",
            "           9.3899e-01,  4.6799e-01,        -inf],\n",
            "         [ 1.5463e+00, -4.9438e-01, -1.4180e-02, -9.7428e-01,  1.3779e+00,\n",
            "           7.8648e-03, -5.3590e-01, -4.5531e-01]],\n",
            "\n",
            "        [[-3.7898e-01,        -inf,        -inf,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [ 1.0377e-01,  1.7584e-01,        -inf,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-1.6373e+00, -6.5557e-01, -8.5031e-01,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-2.7155e+00,  1.9022e+00, -8.4620e-01,  5.9058e-01,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [ 2.5044e+00, -4.9691e-01, -2.6300e-01, -1.6288e-01, -1.7459e+00,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-4.8634e-02,  4.9620e-01, -2.0859e-01, -8.4632e-02,  3.6811e-01,\n",
            "           7.8713e-01,        -inf,        -inf],\n",
            "         [-1.7485e+00,  4.6233e-01,  3.8654e-03,  2.1114e+00,  1.2731e+00,\n",
            "           2.1582e+00,  1.3125e+00,        -inf],\n",
            "         [-8.5500e-02, -1.5414e-02, -1.3915e+00,  6.3086e-02, -2.4530e-01,\n",
            "          -2.0677e-01, -2.2102e+00,  4.4531e-01]],\n",
            "\n",
            "        [[ 4.5165e-01,        -inf,        -inf,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-4.0009e-01, -9.6205e-01,        -inf,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-4.6199e-01,  5.8600e-01, -4.6738e+00,        -inf,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-7.1746e-01, -1.0279e+00, -2.0509e+00, -2.7234e+00,        -inf,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [-4.0388e-01,  5.1597e-01, -2.0697e+00, -4.0982e-01, -8.0534e-01,\n",
            "                 -inf,        -inf,        -inf],\n",
            "         [ 8.2322e-01,  3.0237e+00, -3.0655e+00,  7.0404e-01,  6.7207e-01,\n",
            "          -4.6692e-01,        -inf,        -inf],\n",
            "         [-1.4141e+00, -1.4241e+00, -8.0387e-01, -1.7450e+00, -7.4035e-01,\n",
            "           9.8188e-01, -9.0056e-01,        -inf],\n",
            "         [-5.0277e-01,  1.6844e+00, -4.1847e-01,  1.0239e+00,  1.0275e+00,\n",
            "           1.3980e-01,  4.8822e-01,  1.5573e+00]]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "wei softmax tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
            "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
            "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
            "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
            "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
            "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
            "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
            "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
            "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "v tensor([[[-1.5713e-01,  8.8009e-01,  1.6152e-01, -7.8239e-01, -1.4289e-01,\n",
            "           7.4676e-01,  1.0068e-01, -5.2395e-01, -8.8726e-01,  1.9068e-01,\n",
            "           1.7616e-01, -5.9426e-01, -4.8124e-01, -4.8599e-01,  2.8623e-01,\n",
            "           5.7099e-01],\n",
            "         [ 8.3212e-01, -8.1437e-01, -3.2425e-01,  5.1913e-01, -1.2520e-01,\n",
            "          -4.8982e-01, -5.2867e-01, -3.1393e-02,  1.0723e-01,  8.2692e-01,\n",
            "           8.1322e-01, -2.7132e-02,  4.7754e-01,  4.9801e-01, -1.3769e-01,\n",
            "           1.4025e+00],\n",
            "         [ 6.0346e-01, -2.4995e-01, -6.1588e-01,  4.0678e-01,  3.3283e-01,\n",
            "          -3.9097e-01,  1.3119e-01,  2.1718e-01, -1.2991e-01, -8.8281e-01,\n",
            "           1.7242e-01,  4.6522e-01, -4.2710e-01, -7.6754e-02, -2.8524e-01,\n",
            "           1.3875e+00],\n",
            "         [ 6.6568e-01, -7.0960e-01, -6.0986e-01,  4.3484e-01,  8.9754e-01,\n",
            "          -9.2983e-01,  6.8325e-02,  1.8632e-01,  5.4002e-01,  2.4271e-01,\n",
            "          -6.9225e-01,  4.9774e-01,  4.8503e-01,  6.6076e-01,  8.7671e-01,\n",
            "           7.4566e-02],\n",
            "         [ 1.5357e-01,  1.0439e+00,  8.4574e-01,  2.3882e-01,  3.0046e-01,\n",
            "           1.0516e+00,  7.6373e-01,  4.5166e-01, -7.4263e-01, -1.4395e+00,\n",
            "          -4.9412e-01, -3.7087e-01, -1.1819e+00,  1.0001e-01, -1.8065e-01,\n",
            "           5.1291e-01],\n",
            "         [-8.9198e-01,  5.7820e-02, -3.3504e-01,  8.4768e-01,  3.8764e-01,\n",
            "           1.6644e-01, -4.5871e-01, -5.9737e-01,  4.9612e-01,  6.5476e-01,\n",
            "           5.4789e-02,  9.4680e-01,  4.5108e-01,  1.1999e-01,  1.0573e+00,\n",
            "          -2.2570e-01],\n",
            "         [-4.8492e-01,  1.6553e-01, -2.2215e-01, -1.3454e-01, -8.6441e-02,\n",
            "          -6.6281e-01, -9.3596e-02,  1.0496e-01, -2.6121e-01,  1.8538e-01,\n",
            "           3.1711e-01, -1.3927e-01,  5.4862e-01, -4.0864e-01, -3.8507e-01,\n",
            "           7.1057e-01],\n",
            "         [ 2.0424e-01,  3.7717e-01, -1.1255e+00,  3.9950e-01,  1.4892e-01,\n",
            "           3.5902e-01, -1.7912e-01,  1.3732e+00,  1.5880e-01, -2.3202e-01,\n",
            "           1.6507e-01,  7.6043e-01,  3.5211e-01, -1.0864e+00, -7.9393e-01,\n",
            "          -3.0253e-01]],\n",
            "\n",
            "        [[-1.3254e+00,  1.1236e+00,  2.2927e-01, -2.9970e-01, -7.6267e-03,\n",
            "           7.9364e-01,  8.9581e-01,  3.9650e-01, -6.6613e-01, -2.1844e-01,\n",
            "          -1.3539e+00,  4.1245e-01,  9.6011e-01, -1.0805e+00, -3.9751e-01,\n",
            "          -4.4439e-01],\n",
            "         [-1.9221e-01, -4.6449e-01,  5.9880e-02,  2.8408e-01, -1.0312e-01,\n",
            "          -1.7967e-03,  1.8920e-01, -3.7337e-01, -9.8137e-02,  2.3116e-02,\n",
            "           8.5743e-01,  5.6841e-01, -2.1939e-01, -2.9158e-01, -2.0158e-01,\n",
            "          -4.6876e-01],\n",
            "         [-1.1012e+00,  9.8266e-02,  5.8596e-01, -5.6413e-03,  3.7330e-01,\n",
            "          -6.1363e-02,  2.8833e-02,  2.6230e-01,  6.4099e-01,  7.1003e-02,\n",
            "           3.6877e-01,  5.0011e-01,  7.3872e-01,  1.1909e-01,  5.4246e-01,\n",
            "           6.8950e-02],\n",
            "         [ 4.9074e-01, -2.9978e-01,  1.0949e+00,  1.0131e+00,  3.5883e-01,\n",
            "           9.5771e-01, -1.8349e-01,  1.4002e-01,  1.4243e-01,  8.0787e-01,\n",
            "          -2.4476e-01,  1.3392e-01,  2.6700e-01,  3.2605e-01,  2.0296e-01,\n",
            "           1.4967e-01],\n",
            "         [ 4.5700e-02,  1.0993e+00,  4.6545e-01, -1.5803e-01, -7.2921e-01,\n",
            "           5.8145e-01,  4.0171e-01,  1.3040e+00, -2.2263e-02,  3.9847e-01,\n",
            "           6.3218e-01, -1.4205e-01,  5.0596e-01, -2.9585e-01, -3.5306e-02,\n",
            "          -7.2087e-01],\n",
            "         [ 3.6249e-01,  3.1444e-01,  3.7844e-01, -3.3100e-01, -1.1213e+00,\n",
            "          -6.8686e-01, -6.5431e-01, -2.1805e-01, -2.6552e-01,  6.7712e-01,\n",
            "           3.9176e-01, -1.3338e+00,  3.7350e-01, -1.1095e+00,  3.7270e-01,\n",
            "          -9.3442e-01],\n",
            "         [-2.0881e-01, -7.6620e-02, -1.5674e-01,  1.4457e-01,  8.7035e-01,\n",
            "           2.1136e-01, -4.8995e-01,  2.4986e-01,  5.1811e-01,  6.6507e-01,\n",
            "           3.2814e-01,  4.6015e-01,  9.2321e-01, -4.5579e-01, -4.8577e-01,\n",
            "          -2.7199e-01],\n",
            "         [-1.8408e-01,  1.7347e-01,  1.4034e-02, -4.8221e-01, -5.2118e-01,\n",
            "          -2.6668e-01, -1.0874e-01,  2.0809e-01,  3.0165e-01,  5.3594e-02,\n",
            "          -3.7746e-01, -7.4163e-01,  8.8692e-04, -1.2250e+00,  3.0022e-01,\n",
            "          -5.0357e-01]],\n",
            "\n",
            "        [[ 6.8925e-02,  1.2248e+00, -4.1194e-01, -1.7046e-01, -6.9224e-01,\n",
            "          -2.9201e-01,  1.2704e+00, -6.8596e-01,  4.3798e-01, -2.6366e-01,\n",
            "           1.1528e-01,  1.1676e+00, -7.2138e-01, -1.2308e+00,  8.3821e-01,\n",
            "          -5.5987e-01],\n",
            "         [-9.5939e-01,  9.2166e-02,  7.7470e-02, -9.8325e-02, -5.0263e-01,\n",
            "          -7.0076e-01, -7.3248e-01,  1.8081e-02,  4.7626e-01, -1.1356e-01,\n",
            "           2.6368e-01, -3.6124e-01, -2.1905e-02, -3.4626e-01, -1.0357e-01,\n",
            "           6.5548e-01],\n",
            "         [-5.7584e-01, -3.0022e-01, -6.9503e-02, -9.9645e-02, -2.8187e-01,\n",
            "          -6.7841e-01, -1.4310e-01, -3.7591e-01,  5.7496e-01,  4.6758e-04,\n",
            "           9.1726e-01,  1.6101e-01, -4.4098e-01,  5.3701e-03,  7.9788e-01,\n",
            "           5.6693e-01],\n",
            "         [ 3.4514e-01,  3.0841e-01,  1.0998e-01, -2.6316e-01,  1.0666e+00,\n",
            "          -5.6067e-02, -6.9560e-01,  3.0091e-01, -2.7254e-01,  8.2122e-01,\n",
            "          -8.6185e-01,  6.1082e-02, -1.2083e-01,  4.1112e-01, -1.0277e-01,\n",
            "          -2.9790e-01],\n",
            "         [-1.8289e+00, -8.6379e-01, -7.9821e-01,  2.4173e-01, -5.0344e-01,\n",
            "          -1.0447e+00,  8.7287e-01,  5.0584e-01,  5.6657e-02, -3.1938e-01,\n",
            "           1.0980e+00,  1.1729e+00, -5.4148e-01, -1.0805e+00,  7.3217e-02,\n",
            "          -2.8329e-01],\n",
            "         [-3.5718e-01, -3.2740e-01, -6.9867e-01,  7.8014e-01,  4.2778e-01,\n",
            "           3.3665e-01,  5.5142e-02,  5.9465e-01,  6.4841e-01, -8.7773e-02,\n",
            "          -4.3907e-02,  6.5681e-01,  1.2646e-01,  2.5969e-01,  6.7423e-01,\n",
            "          -7.6637e-01],\n",
            "         [ 7.6206e-01,  4.9035e-01,  8.2749e-01,  3.7294e-01, -7.1975e-01,\n",
            "          -3.3127e-01, -8.6443e-01, -1.6571e-03, -5.9054e-01,  6.3868e-01,\n",
            "           2.2889e-01, -5.5488e-02,  2.9504e-01,  5.3679e-01, -7.7014e-01,\n",
            "           4.9259e-01],\n",
            "         [ 4.3940e-01,  2.4456e-01, -6.1958e-01,  5.1417e-01,  8.1137e-01,\n",
            "           2.7439e-01,  1.6661e-01,  5.0555e-02,  9.1574e-02,  8.9894e-01,\n",
            "          -1.0681e-01,  3.1970e-01, -7.3390e-02,  3.0807e-01,  7.9702e-01,\n",
            "           7.5018e-01]],\n",
            "\n",
            "        [[ 9.7183e-02,  5.7301e-02, -1.0468e-01, -4.6654e-02, -1.4006e-01,\n",
            "          -8.4126e-01, -1.3625e-01, -6.7465e-01, -2.1541e-01,  1.0993e+00,\n",
            "           2.3427e-01,  3.2605e-02, -1.8521e-01,  1.4780e-01, -6.1045e-01,\n",
            "           1.5391e+00],\n",
            "         [ 3.6123e-01, -6.7973e-01, -7.7090e-01,  6.4828e-01, -2.4451e-01,\n",
            "          -5.7902e-01, -1.5354e+00, -7.2195e-01, -1.8834e-01,  1.0884e-02,\n",
            "           2.3991e-01, -5.4473e-02, -1.4373e-01,  4.9291e-02, -8.8639e-01,\n",
            "           7.2397e-01],\n",
            "         [-1.0977e-01,  8.0600e-01,  8.1140e-01, -3.4001e-01, -4.5837e-01,\n",
            "           5.4328e-03,  1.3075e+00, -7.7781e-01, -6.2820e-01,  7.4216e-02,\n",
            "          -2.1868e-01,  1.8126e-01, -2.0854e-01,  6.7201e-01,  6.9363e-02,\n",
            "           9.8662e-01],\n",
            "         [ 3.0428e-01,  1.1563e+00,  1.3803e-01, -2.0818e+00, -1.0470e-01,\n",
            "           5.2292e-01,  1.2301e+00,  5.3652e-01, -9.0009e-01, -1.0794e+00,\n",
            "          -2.4331e-01,  9.7983e-04,  2.4827e-01,  4.4169e-02, -6.7854e-01,\n",
            "          -3.3345e-01],\n",
            "         [-5.3004e-01, -9.2135e-01,  3.7915e-01, -2.0732e-02,  3.7330e-01,\n",
            "          -1.6131e-01, -7.0930e-01,  4.2039e-02,  1.6151e-01,  1.6618e-01,\n",
            "           5.6694e-01,  5.5056e-01, -7.1126e-02, -5.5536e-01, -1.2077e-01,\n",
            "          -4.5284e-01],\n",
            "         [-6.9652e-01,  4.4457e-01,  8.0947e-01, -6.0359e-01,  4.7886e-02,\n",
            "          -4.6401e-01, -2.0967e-01,  5.5984e-01,  5.7196e-01,  3.6429e-01,\n",
            "           5.9383e-02, -1.3565e+00,  6.8667e-01,  5.4511e-01, -6.7370e-01,\n",
            "           6.3525e-01],\n",
            "         [ 3.5459e-01,  1.1575e-01, -4.2291e-01, -4.7040e-01, -2.2670e-01,\n",
            "           1.5671e-01, -2.1000e-01, -1.0505e+00, -1.0665e+00, -8.3185e-01,\n",
            "           1.9891e-01,  9.0778e-01,  3.5189e-01,  5.6643e-02, -6.4876e-01,\n",
            "           5.5124e-02],\n",
            "         [-1.7223e+00,  5.1077e-01,  2.9681e-01,  2.3290e-01,  2.4183e-01,\n",
            "           3.3723e-01, -2.5232e-01,  6.4762e-01, -1.4068e+00, -6.4379e-01,\n",
            "           7.4489e-02, -5.8730e-01,  1.2959e-01, -2.1585e-01, -7.5063e-01,\n",
            "           3.2311e-01]]], grad_fn=<UnsafeViewBackward0>)\n",
            "out tensor([[[-1.5713e-01,  8.8009e-01,  1.6152e-01, -7.8239e-01, -1.4289e-01,\n",
            "           7.4676e-01,  1.0068e-01, -5.2395e-01, -8.8726e-01,  1.9068e-01,\n",
            "           1.7616e-01, -5.9426e-01, -4.8124e-01, -4.8599e-01,  2.8623e-01,\n",
            "           5.7099e-01],\n",
            "         [ 6.7643e-01, -5.4770e-01, -2.4780e-01,  3.1430e-01, -1.2799e-01,\n",
            "          -2.9521e-01, -4.2962e-01, -1.0891e-01, -4.9282e-02,  7.2679e-01,\n",
            "           7.1296e-01, -1.1639e-01,  3.2665e-01,  3.4315e-01, -7.0975e-02,\n",
            "           1.2716e+00],\n",
            "         [ 4.8227e-01, -1.0688e-01, -4.0555e-01,  1.7696e-01,  1.5811e-01,\n",
            "          -1.6967e-01,  1.6217e-02,  2.1509e-02, -2.4903e-01, -3.7725e-01,\n",
            "           2.7867e-01,  1.6295e-01, -2.8951e-01, -6.7610e-02, -1.4162e-01,\n",
            "           1.2194e+00],\n",
            "         [ 1.9708e-01,  2.8561e-01, -1.3028e-01, -2.6552e-01,  6.6781e-02,\n",
            "           1.9535e-01,  2.8073e-02, -2.4511e-01, -4.6466e-01,  6.9287e-02,\n",
            "           1.5284e-01, -2.0324e-01, -2.4789e-01, -1.6213e-01,  1.9474e-01,\n",
            "           7.6778e-01],\n",
            "         [ 2.5104e-01,  7.3457e-01,  5.9385e-01,  2.5159e-01,  2.6064e-01,\n",
            "           7.5820e-01,  5.5947e-01,  3.5387e-01, -5.9338e-01, -1.0807e+00,\n",
            "          -3.1110e-01, -2.7809e-01, -9.0541e-01,  1.3181e-01, -1.3818e-01,\n",
            "           6.3715e-01],\n",
            "         [ 3.4277e-01,  4.9605e-01,  4.7248e-01,  3.0277e-01,  1.8440e-01,\n",
            "           5.8144e-01,  3.8245e-01,  2.9521e-01, -4.8969e-01, -7.7051e-01,\n",
            "          -1.1721e-01, -2.5412e-01, -6.8921e-01,  1.9795e-01, -1.5135e-01,\n",
            "           7.6659e-01],\n",
            "         [ 1.8658e-01, -9.6351e-02, -1.4300e-01,  3.0587e-01,  8.3441e-02,\n",
            "          -6.8646e-03, -2.0472e-01, -1.5350e-01, -7.6250e-02,  3.2689e-01,\n",
            "           3.0896e-01,  7.6626e-02,  9.9243e-02,  1.6560e-01,  1.9745e-01,\n",
            "           7.6248e-01],\n",
            "         [ 1.3013e-01, -3.2832e-02, -4.9645e-01,  2.8652e-01,  2.7042e-01,\n",
            "          -2.6357e-01, -7.3756e-02,  3.7857e-01,  7.4580e-02,  3.3827e-02,\n",
            "           1.4695e-02,  3.1937e-01,  2.9926e-01, -1.6530e-01, -3.8630e-02,\n",
            "           3.3748e-01]],\n",
            "\n",
            "        [[-1.3254e+00,  1.1236e+00,  2.2927e-01, -2.9970e-01, -7.6267e-03,\n",
            "           7.9364e-01,  8.9581e-01,  3.9650e-01, -6.6613e-01, -2.1844e-01,\n",
            "          -1.3539e+00,  4.1245e-01,  9.6011e-01, -1.0805e+00, -3.9751e-01,\n",
            "          -4.4439e-01],\n",
            "         [-3.8338e-01, -1.9659e-01,  8.8455e-02,  1.8560e-01, -8.7010e-02,\n",
            "           1.3239e-01,  3.0841e-01, -2.4350e-01, -1.9396e-01, -1.7634e-02,\n",
            "           4.8439e-01,  5.4210e-01, -2.0407e-02, -4.2467e-01, -2.3463e-01,\n",
            "          -4.6465e-01],\n",
            "         [-1.1100e+00,  3.2334e-01,  4.7054e-01, -6.3595e-02,  2.5443e-01,\n",
            "           1.5352e-01,  2.5186e-01,  2.6286e-01,  2.7916e-01, -3.1662e-03,\n",
            "          -3.2880e-02,  4.8191e-01,  7.4431e-01, -1.9921e-01,  2.7134e-01,\n",
            "          -8.5871e-02],\n",
            "         [-9.7190e-01,  4.6124e-01,  4.2349e-01, -1.7230e-02,  1.5847e-01,\n",
            "           4.1175e-01,  4.0764e-01,  2.4982e-01, -5.0322e-02,  4.1514e-03,\n",
            "          -3.9853e-01,  4.3551e-01,  7.0285e-01, -4.3081e-01,  2.6684e-02,\n",
            "          -2.0169e-01],\n",
            "         [ 3.3586e-01, -8.5915e-02,  9.3660e-01,  7.7311e-01,  1.8037e-01,\n",
            "           8.2853e-01, -6.9183e-02,  2.8814e-01,  1.1734e-01,  6.8448e-01,\n",
            "          -5.8500e-02,  1.2726e-01,  2.9780e-01,  1.9324e-01,  1.5655e-01,\n",
            "          -9.3005e-03],\n",
            "         [ 1.6984e-01,  3.0993e-02,  8.1557e-01,  6.1679e-01,  1.0429e-01,\n",
            "           7.4573e-01,  2.3072e-02,  3.0572e-01,  5.8163e-02,  5.7122e-01,\n",
            "          -4.5275e-02,  1.5051e-01,  3.2901e-01,  5.6984e-02,  1.0311e-01,\n",
            "          -9.9174e-02],\n",
            "         [ 4.6497e-02,  1.5765e-01,  3.9760e-01,  1.7619e-01, -2.1168e-01,\n",
            "           2.3365e-01, -6.2083e-02,  2.1726e-01, -7.8725e-03,  4.5389e-01,\n",
            "           3.4349e-01, -5.5631e-02,  3.3726e-01, -3.7591e-01, -1.0140e-02,\n",
            "          -4.5806e-01],\n",
            "         [-5.3896e-01,  7.5555e-01,  3.3034e-01, -1.5849e-01, -2.6740e-01,\n",
            "           4.3495e-01,  3.7772e-01,  5.5794e-01, -1.8369e-01,  1.5938e-01,\n",
            "          -2.1042e-01,  5.5790e-02,  6.3184e-01, -6.4884e-01, -9.6084e-02,\n",
            "          -5.0751e-01]],\n",
            "\n",
            "        [[ 6.8925e-02,  1.2248e+00, -4.1194e-01, -1.7046e-01, -6.9224e-01,\n",
            "          -2.9201e-01,  1.2704e+00, -6.8596e-01,  4.3798e-01, -2.6366e-01,\n",
            "           1.1528e-01,  1.1676e+00, -7.2138e-01, -1.2308e+00,  8.3821e-01,\n",
            "          -5.5987e-01],\n",
            "         [-4.6375e-01,  6.3807e-01, -1.5842e-01, -1.3309e-01, -5.9402e-01,\n",
            "          -5.0374e-01,  2.3289e-01, -3.2126e-01,  4.5781e-01, -1.8590e-01,\n",
            "           1.9215e-01,  3.7566e-01, -3.5905e-01, -7.7262e-01,  3.5036e-01,\n",
            "           6.9694e-02],\n",
            "         [-6.4044e-01,  1.3831e-01, -6.1007e-02, -1.1112e-01, -4.5228e-01,\n",
            "          -6.2271e-01, -1.7030e-01, -2.4949e-01,  5.0670e-01, -9.6444e-02,\n",
            "           4.8315e-01,  9.4986e-02, -2.9810e-01, -3.6538e-01,  3.9458e-01,\n",
            "           4.1512e-01],\n",
            "         [-6.7193e-01,  1.2516e-01,  7.3386e-02, -1.3198e-01, -1.7880e-01,\n",
            "          -5.6740e-01, -6.8226e-01,  5.0844e-02,  3.3051e-01,  7.8242e-02,\n",
            "           6.8022e-02, -2.4041e-01, -6.6864e-02, -1.8411e-01, -5.3514e-02,\n",
            "           4.5113e-01],\n",
            "         [-1.4270e-02,  1.0195e+00, -3.4792e-01, -1.6421e-01, -5.5846e-01,\n",
            "          -3.2457e-01,  9.9404e-01, -5.6891e-01,  4.0097e-01, -1.8123e-01,\n",
            "           1.1856e-01,  9.8704e-01, -6.4057e-01, -1.0320e+00,  7.3320e-01,\n",
            "          -4.3167e-01],\n",
            "         [-6.3858e-01, -7.6533e-02, -3.6510e-01,  1.7782e-01, -6.5426e-02,\n",
            "          -3.5158e-01,  7.9591e-02,  1.7384e-01,  3.6676e-01, -4.2302e-02,\n",
            "           2.4923e-01,  4.8239e-01, -2.1295e-01, -2.9492e-01,  3.4749e-01,\n",
            "          -1.7111e-01],\n",
            "         [-2.2366e-01, -5.5317e-02, -1.8296e-01,  2.4258e-01,  2.5357e-01,\n",
            "          -1.6154e-01, -2.3908e-01,  3.3243e-01,  1.0304e-01,  2.6067e-01,\n",
            "          -5.0670e-02,  3.6947e-01, -4.9856e-02,  1.1197e-01,  1.1752e-01,\n",
            "          -2.5078e-01],\n",
            "         [-2.4821e-01,  1.4845e-01, -3.5033e-01,  1.7102e-01,  1.6613e-01,\n",
            "          -2.0643e-01,  8.6633e-02,  8.8414e-02,  2.1188e-01,  2.5805e-01,\n",
            "           5.5145e-02,  4.2668e-01, -2.0443e-01, -1.7372e-01,  3.8899e-01,\n",
            "           5.1725e-02]],\n",
            "\n",
            "        [[ 9.7183e-02,  5.7301e-02, -1.0468e-01, -4.6654e-02, -1.4006e-01,\n",
            "          -8.4126e-01, -1.3625e-01, -6.7465e-01, -2.1541e-01,  1.0993e+00,\n",
            "           2.3427e-01,  3.2605e-02, -1.8521e-01,  1.4780e-01, -6.1045e-01,\n",
            "           1.5391e+00],\n",
            "         [ 1.9305e-01, -2.1031e-01, -3.4658e-01,  2.0567e-01, -1.7799e-01,\n",
            "          -7.4604e-01, -6.4427e-01, -6.9183e-01, -2.0558e-01,  7.0413e-01,\n",
            "           2.3632e-01,  9.8800e-04, -1.7015e-01,  1.1203e-01, -7.1064e-01,\n",
            "           1.2431e+00],\n",
            "         [ 2.9114e-01, -4.8343e-01, -5.9254e-01,  4.6477e-01, -2.1832e-01,\n",
            "          -6.4460e-01, -1.1627e+00, -7.0993e-01, -1.9703e-01,  2.9262e-01,\n",
            "           2.3669e-01, -3.1050e-02, -1.5471e-01,  7.7153e-02, -8.1137e-01,\n",
            "           9.3578e-01],\n",
            "         [ 1.7549e-01, -3.4260e-02, -2.0523e-01,  2.7644e-02, -2.1312e-01,\n",
            "          -5.6022e-01, -3.5273e-01, -6.2722e-01, -3.0037e-01,  4.6061e-01,\n",
            "           1.5004e-01,  1.9040e-02, -1.4646e-01,  1.7220e-01, -6.2559e-01,\n",
            "           1.0722e+00],\n",
            "         [ 1.7354e-01, -1.7962e-01, -2.7874e-01, -1.0590e-01, -1.2952e-01,\n",
            "          -3.5086e-01, -5.5830e-01, -3.8638e-01, -2.9719e-01,  3.3368e-02,\n",
            "           1.7392e-01,  5.5898e-02, -7.2007e-02,  1.3182e-02, -6.6710e-01,\n",
            "           5.4229e-01],\n",
            "         [ 2.4678e-01, -4.7274e-01, -5.2827e-01,  3.1212e-01, -1.7528e-01,\n",
            "          -4.8636e-01, -1.1223e+00, -5.4196e-01, -2.0142e-01,  4.0103e-02,\n",
            "           2.2231e-01, -2.9380e-02, -9.4354e-02,  2.6374e-02, -7.8726e-01,\n",
            "           6.2836e-01],\n",
            "         [-3.9784e-01,  2.5915e-01,  5.0358e-01, -4.6864e-01, -2.2024e-02,\n",
            "          -3.2242e-01, -1.2578e-01,  1.0634e-01,  1.3618e-01,  1.7780e-01,\n",
            "           1.0391e-01, -6.2540e-01,  3.8904e-01,  3.3690e-01, -5.5140e-01,\n",
            "           5.2246e-01],\n",
            "         [-3.5927e-01,  3.3935e-02, -2.9863e-02, -1.5019e-01, -6.0354e-03,\n",
            "          -6.5733e-02, -3.9659e-01, -6.0435e-02, -5.7551e-01, -2.9157e-01,\n",
            "           1.4899e-01, -7.5002e-02,  7.3228e-02, -4.7413e-02, -6.4394e-01,\n",
            "           2.8560e-01]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT1hdtzXCjgL",
        "outputId": "49715495-134b-49cd-d7f0-ae1e757c60b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n"
      ],
      "metadata": {
        "id": "M5CvobiQ0pLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5\n",
        "wei"
      ],
      "metadata": {
        "id": "4SNbLq5z3oBw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "015bd9fb-9bb3-4aed-8de8-8d3a17cf151f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.9857, -1.7370,  0.4189,  0.2985, -0.5451,  0.4942, -0.7267,\n",
              "          -0.4810],\n",
              "         [-0.6151,  0.7704,  0.1215,  0.1193, -1.0559, -0.1234,  0.3918,\n",
              "          -0.2687],\n",
              "         [ 0.4511,  0.6600,  0.8736, -0.5065,  0.8595,  0.2483,  0.7095,\n",
              "          -0.3241],\n",
              "         [ 1.4350, -0.5599,  1.2163, -0.0813,  1.7313,  0.3421, -0.3146,\n",
              "          -0.9178],\n",
              "         [-2.0204,  1.8716, -1.1214, -0.1317, -0.4320,  0.8461,  1.0991,\n",
              "           1.8651],\n",
              "         [ 1.0000,  0.5394,  0.9807, -0.0900,  0.7364,  1.3018,  1.4779,\n",
              "           1.2385],\n",
              "         [ 1.0542, -0.5249,  0.1258, -0.0781,  0.8236, -1.0546,  0.3601,\n",
              "          -0.5679],\n",
              "         [ 0.2587,  0.1620,  0.6471,  0.2837,  1.2641,  0.3890, -0.6218,\n",
              "          -0.4601]],\n",
              "\n",
              "        [[ 0.2958,  0.3852, -0.7456,  0.0486, -0.1722, -0.3054,  0.8299,\n",
              "           0.4363],\n",
              "         [-0.8550,  0.2635,  1.0761,  0.9544,  0.7529, -0.9505,  0.2712,\n",
              "          -0.7474],\n",
              "         [-0.9295, -0.1556, -0.0649, -0.3967, -0.1137, -1.0016,  0.5251,\n",
              "           2.1469],\n",
              "         [ 0.4260,  0.5476, -0.1825,  0.4262,  0.4614,  0.7707, -0.0319,\n",
              "           0.0419],\n",
              "         [ 0.9852, -1.8435,  0.5392,  0.9052,  0.0147, -0.0083, -0.3682,\n",
              "          -1.4787],\n",
              "         [ 0.0243, -0.7921,  1.1611, -2.1399,  0.8785, -3.7707,  0.2854,\n",
              "           0.2840],\n",
              "         [ 1.2051,  0.7425,  0.1625,  0.1487, -0.3684,  1.1103, -0.6565,\n",
              "          -0.5399],\n",
              "         [ 0.4974, -0.6444, -0.5425,  1.1529,  1.4586,  2.1972, -1.4472,\n",
              "          -2.8702]],\n",
              "\n",
              "        [[ 0.8568,  2.0571, -0.7006,  0.3831,  0.3106, -0.0532, -0.3264,\n",
              "          -1.9622],\n",
              "         [ 0.2715,  1.6624, -1.0427,  0.8949,  0.9935, -1.7049, -0.6244,\n",
              "          -0.0398],\n",
              "         [-0.2850, -1.0568,  1.0730, -0.3639,  1.4113,  0.4204,  0.0832,\n",
              "           1.8698],\n",
              "         [ 0.2770, -0.6132,  0.0788, -0.9589,  0.1094, -0.6141,  0.2004,\n",
              "          -1.6000],\n",
              "         [-0.6044,  0.2721,  2.6773, -0.6017,  0.0375, -1.9897, -0.9305,\n",
              "           0.2045],\n",
              "         [ 0.1326,  1.4459,  0.4910,  0.8230,  0.4905, -0.6744,  0.1504,\n",
              "           0.7052],\n",
              "         [ 0.9300,  0.8989, -0.2437,  1.2802,  1.8367, -1.6352,  1.1763,\n",
              "          -1.7056],\n",
              "         [-0.6255,  0.1929,  0.2220, -1.3950,  1.2366, -0.2456,  1.7097,\n",
              "           0.0183]],\n",
              "\n",
              "        [[ 0.7970, -0.1589,  2.1649,  1.3842,  1.0258, -0.0408, -0.9887,\n",
              "          -2.0557],\n",
              "         [-0.8892,  1.2347, -1.0261,  0.4946, -0.5050,  0.4870,  0.8439,\n",
              "           0.4716],\n",
              "         [-0.3764,  0.7035,  1.2153,  2.7291, -0.8751,  1.2909, -0.0223,\n",
              "          -2.2251],\n",
              "         [-3.0034, -0.0472, -1.8734,  0.4687,  1.1202,  0.0969,  1.7634,\n",
              "           0.6773],\n",
              "         [-0.8947, -0.0660, -1.8381,  0.0621, -0.6480, -0.0997,  0.6156,\n",
              "           0.9758],\n",
              "         [ 0.1174, -0.0431, -0.3131, -1.1377, -0.2750,  0.2928,  0.8689,\n",
              "           1.4676],\n",
              "         [-0.1954,  0.3430,  0.8964, -0.2087,  0.9346, -0.0980,  0.7557,\n",
              "          -0.7743],\n",
              "         [ 2.0831,  0.4608,  4.1820, -0.3133,  1.4190,  1.7620,  0.2617,\n",
              "          -1.2753]]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl6I9n9IRTSo",
        "outputId": "1fe54d5f-3991-4082-d81f-700c599ef269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1tQx7oeRvtc",
        "outputId": "b442fc2a-f555-4fb0-ca18-22dbc2ff95b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLb_odHU3iKM",
        "outputId": "71200514-1ef9-4c0e-c24f-98ec69f9eb1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB82yzt44REI",
        "outputId": "76bae16d-e233-42ef-d49e-1b952321be1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8,)\n",
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpt8569BB9_f",
        "outputId": "85b32c09-ebe4-4489-865f-690115b6ecbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.8000, -1.6000,  2.4000, -1.6000,  4.0000])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Num7sX9CKOH",
        "outputId": "a795c3f2-f532-4d9c-8093-b763cb15bb86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633T2cmnW1uk",
        "outputId": "313b2c6d-9064-41fa-f3ce-6eb25e153f75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9cK9BoXCYb",
        "outputId": "6ba631fa-3b32-4f48-e34d-88f8ce4562d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "#!wget https://raw.githubusercontent.com/DING-1994/llm2023/main/icat.txt\n",
        "with open('icat.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "9f856d05-7766-462c-fbfb-8640e6de1946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.310091 M parameters\n",
            "step 0: train loss 6.9366, val loss 6.9089\n",
            "step 100: train loss 4.6500, val loss 4.8779\n",
            "step 200: train loss 3.9514, val loss 4.4534\n",
            "step 300: train loss 3.5006, val loss 4.2476\n",
            "step 400: train loss 3.2005, val loss 4.2169\n",
            "step 500: train loss 2.9244, val loss 4.2501\n",
            "step 600: train loss 2.6884, val loss 4.3490\n",
            "step 700: train loss 2.4260, val loss 4.3811\n",
            "step 800: train loss 2.1704, val loss 4.5203\n",
            "step 900: train loss 1.8574, val loss 4.7124\n",
            "step 1000: train loss 1.5916, val loss 4.9142\n",
            "step 1100: train loss 1.3472, val loss 5.1477\n",
            "step 1200: train loss 1.1115, val loss 5.3431\n",
            "step 1300: train loss 0.9578, val loss 5.5507\n",
            "step 1400: train loss 0.7975, val loss 5.8449\n",
            "step 1500: train loss 0.7027, val loss 6.0773\n",
            "step 1600: train loss 0.6187, val loss 6.2443\n",
            "step 1700: train loss 0.5510, val loss 6.3875\n",
            "step 1800: train loss 0.5111, val loss 6.5195\n",
            "step 1900: train loss 0.4833, val loss 6.6413\n",
            "step 2000: train loss 0.4382, val loss 6.7918\n",
            "step 2100: train loss 0.4234, val loss 6.7083\n",
            "step 2200: train loss 0.3990, val loss 6.9166\n",
            "step 2300: train loss 0.3896, val loss 6.9225\n",
            "step 2400: train loss 0.3788, val loss 7.0803\n",
            "step 2500: train loss 0.3739, val loss 7.0728\n",
            "step 2600: train loss 0.3569, val loss 7.2291\n",
            "step 2700: train loss 0.3466, val loss 7.2810\n",
            "step 2800: train loss 0.3449, val loss 7.3628\n",
            "step 2900: train loss 0.3321, val loss 7.3955\n",
            "step 3000: train loss 0.3339, val loss 7.4149\n",
            "step 3100: train loss 0.3148, val loss 7.5065\n",
            "step 3200: train loss 0.3154, val loss 7.6152\n",
            "step 3300: train loss 0.3138, val loss 7.6344\n",
            "step 3400: train loss 0.3069, val loss 7.6463\n",
            "step 3500: train loss 0.3075, val loss 7.7066\n",
            "step 3600: train loss 0.2929, val loss 7.8272\n",
            "step 3700: train loss 0.2877, val loss 7.7904\n",
            "step 3800: train loss 0.2936, val loss 7.7739\n",
            "step 3900: train loss 0.2926, val loss 7.7568\n",
            "step 4000: train loss 0.2797, val loss 7.9007\n",
            "step 4100: train loss 0.2806, val loss 7.8555\n",
            "step 4200: train loss 0.2777, val loss 8.0030\n",
            "step 4300: train loss 0.2837, val loss 7.9064\n",
            "step 4400: train loss 0.2829, val loss 8.0546\n",
            "step 4500: train loss 0.2804, val loss 8.0293\n",
            "step 4600: train loss 0.2710, val loss 8.1688\n",
            "step 4700: train loss 0.2686, val loss 8.1070\n",
            "step 4800: train loss 0.2683, val loss 8.0459\n",
            "step 4900: train loss 0.2609, val loss 8.1291\n",
            "step 4999: train loss 0.2607, val loss 8.2027\n",
            "\n",
            "　この小供の寝床へもぐり込んでいっしょにねる事である。この主人がどういう考えないものはないが誰だ不人がここで泣いたりするが、別段恐しいだけは記憶している。その時におさんと云う者はつくづくいやになった。この間おさんの三馬《さんま》を偸《ぬす》んでこの返報をしてやってかららない。そのうちと写生をしたり、灰色でもないのに勝《ず》れた穴もにはなは物指《わが》き交ぜたぎたの間とっては到底悪《どな》い。この板の間で他《ひと》が顫《ふる》えていても仕方がない。どうだそこの馬鹿野郎」と力量のそのそ這いた。どうし人間つって眠く。勝の木の時々我慢する筋《あが》する。時に人間おれたらするが、吾輩は人間《の書》の主人以外の人を再び見るが、西側《えんがわ》へ寝る事とした。しかし一番心持の真中太《うよばなら》」昂に痛い。主人の裏に述懐《じゅっか》してみずから直《の》ら出しにない。ここうだって見た。\n",
            "　何でも自然その問に接したくする。最早《もはや》に入《あ》れたるものだ。ここの蔭関すとが彼の気焔《きえん》を感心したように咽喉《のど》をころころころ鳴らして謹聴していた。\n",
            "　にこうよりも数倍悲しむべき報道を耳にした事がある。至る」智識は黒よりも余程発達しているつもりだが腕力と勇気に至っては到底《とうてい》黒の比較にはならないと覚悟はしていたもののの、この問に接したる時は、さすがに極《きま》りが善《よ》くはなかった。けれども事実は事実で詐《け》ねも出来ぬ事にいべき報道を耳になった事がある。みくなるせら何茶畠《ちゃばたけ》ばかり細目に餓死《がこ》なしたから、あるくて腹の傍点］に減の家《かんま》へ寝る。これはあまり誰に坐っていると、さしたり、どこまで増長するか分らない。\n",
            "　我儘《わがまま》も甘《この》ますが最後大変な事になまった。小供は――ことに小さい方が質《たち》がわるい甘ん漫罵《ふか》ない。どうだ君も画らしい知らないのだ。なるべくしかに今主人の的生に機やりするじに這入っていたなら、好奇の心に坐くってえ付けた。第一になった。主人はあまり甘《うま》くなかった。けはあまり誰も交際しない。同盟敬遠主義の的《まと》になっている奴だ。吾輩は彼の名を聞いて少々尻ここそれから食物にりたぷたと知己《ちき》になってかられらなめるせ。ど小春の小盛い返報をしているかと今日して装飾さらないので気が眼かけてやりの茶園へ寝転《わ》るねえて両足色が暮に暗くならアンドレア・デル・サルトを極《き》め込んでいる。吾輩はこの有様を見て覚えず失敬している。敬遠こうしてはず彼の前に坐の先生《こ》けられたが、運ばたまの垣方で書生然《うちゃえん》がある。広くはないが瀟洒《さっぱり》とした心持ち好く日の当《あ》に障《さわ》った様子で尻ぺた様以外の前にはだりちつかせねえ出て両に出来るく。下《こたま》の上から付いに坐またものはない。人間やこれだその書生というのは時々我々を捕《つかま》えて煮猫の猫にも何をもも同じ池リージヤスターゼを起す」に傍《じ》な》へ乗る額《よ》かっていた。ここうものは少々言葉が卑《出来んまん》くな片輪《かたわ》にこの呼吸を足《ば》わっている事がない。どうだ君も画らしい画をかこうと思うならちと写生をしたら」\n",
            "「へえアンドレア・デル・サルトが言った事がある。画をかくなら何でも自然その物を写せ。天に星辰《せん》あり。地に露華《ろか》あり。飛ぶに禽《とり》あり。走るに獣《あざ》を過ぎた主人があまり甘《す》恐にているの眼鏡越《めがねごし》に主人の顔を見ながら、「そう初めから上手にはかけないさ、第一室内の想像ばかり細目に眼をあけて見た。泣き上げから時はかけたこの頃知ってちた。かしかしかにりまり甘《ほ》点］がついたこの漫罵《まんば》も甘んじて受けるが、こういつが降っと見えた。吾輩が最だにかってまた》の上である。\n",
            "　その後《ご》猫にあついてこうと思って左胸の先に見た。これが人間の教師という不徳事件も実は黒から聞いたのである。\n",
            "　或る日例に例れて煮《に》せられて候《そい》を交ぜたように思われる」\n",
            "「箆棒《べらぼう》め、うちなんかいくら大きくたって腹の足《た》しになるもんか」\n",
            "　彼は大《おおい》に餓死《がし》したかも知れんのである。しかし暖かい。そうな方のは夜中が聞いつで夜に裏のよく手に出て出され一番｜産《ふい》もこのは忍び出ても出来ぬ仕儀《しぎ》となったから、やむをえず失敬して両足《よう》しろう」とそうやくの思って下《い》の大文気もだろう。そのうちに見え［＃「いつでに路傍点］の家《うち》の返報をしてだらない。それまで思べき報ャさには一度々｜呼《よば》よいす。する柔毛《すぐ》君が君なる。他《ひと》の庭内に忍び入りたるものがかくまで平気に睡《ねむ》られるもの眼話利になっている。彼五遍繰り返さつ》をさ不数寝た。ふと眼が\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fjjvMifYZf7x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}