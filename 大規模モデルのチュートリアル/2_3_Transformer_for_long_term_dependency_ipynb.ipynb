{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORK7r1w-G1xw",
        "outputId": "8083f5fe-ae42-4400-ac2e-08368f0d64d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c5029fb2730>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 1: Understanding the Problem\n",
        "# ---------------------------------\n",
        "# Let's create a simple long-term dependency task:\n",
        "# Predict the last token of a sequence given two tokens that should match\n",
        "# Example: [1, 2, 3, 4, 1] should predict 1 because the first token is 1\n",
        "\n",
        "def generate_matching_data(seq_length=10, num_samples=1000, vocab_size=5):\n",
        "    \"\"\"Generate data where the target matches the first token\"\"\"\n",
        "    X = torch.randint(1, vocab_size, (num_samples, seq_length))\n",
        "    # Target is the same as the first token\n",
        "    y = X[:, 0]\n",
        "    return X, y\n",
        "\n",
        "# Generate training data\n",
        "seq_length = 20  # Longer sequences to demonstrate long-term dependency\n",
        "X_train, y_train = generate_matching_data(seq_length, 1000)\n",
        "X_test, y_test = generate_matching_data(seq_length, 200)\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"DATA GENERATION RESULTS:\")\n",
        "print(f\"Input shape: {X_train.shape}, Target shape: {y_train.shape}\")\n",
        "print(f\"Sample input sequence: {X_train[0]}\")\n",
        "print(f\"Sample target (should match first token): {y_train[0]}\")\n",
        "print(f\"First token in sequence: {X_train[0][0]}\")\n",
        "print(\"This is a long-term dependency task: model must remember the first token\")\n",
        "print(\"=\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcq7jG-wHA0N",
        "outputId": "f081a12c-992b-491b-b399-68fe0d681331"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "DATA GENERATION RESULTS:\n",
            "Input shape: torch.Size([1000, 20]), Target shape: torch.Size([1000])\n",
            "Sample input sequence: tensor([2, 4, 3, 3, 1, 1, 3, 2, 3, 4, 1, 1, 3, 2, 2, 4, 1, 3, 3, 1])\n",
            "Sample target (should match first token): 2\n",
            "First token in sequence: 2\n",
            "This is a long-term dependency task: model must remember the first token\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 2: Self-Attention - The Key to Solving Long-Term Dependencies\n",
        "# -----------------------------------------------------------------\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (self.head_dim * heads == embed_size), \"Embedding size needs to be divisible by heads\"\n",
        "\n",
        "        # Linear projections for Q, K, V\n",
        "        self.query = nn.Linear(embed_size, embed_size)\n",
        "        self.key = nn.Linear(embed_size, embed_size)\n",
        "        self.value = nn.Linear(embed_size, embed_size)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_length, embed_size)\n",
        "        batch_size = x.shape[0]\n",
        "        seq_length = x.shape[1]\n",
        "\n",
        "        # Perform linear projections\n",
        "        queries = self.query(x)  # (batch_size, seq_length, embed_size)\n",
        "        keys = self.key(x)       # (batch_size, seq_length, embed_size)\n",
        "        values = self.value(x)   # (batch_size, seq_length, embed_size)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        queries = queries.reshape(batch_size, seq_length, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(batch_size, seq_length, self.heads, self.head_dim)\n",
        "        values = values.reshape(batch_size, seq_length, self.heads, self.head_dim)\n",
        "\n",
        "        # Transpose to compute attention across seq_length\n",
        "        # New shape: (batch_size, heads, seq_length, head_dim)\n",
        "        queries = queries.permute(0, 2, 1, 3)\n",
        "        keys = keys.permute(0, 2, 1, 3)\n",
        "        values = values.permute(0, 2, 1, 3)\n",
        "\n",
        "        # Compute attention scores\n",
        "        # (batch_size, heads, seq_length, seq_length)\n",
        "        energy = torch.matmul(queries, keys.permute(0, 1, 3, 2))\n",
        "\n",
        "        # Scale attention scores\n",
        "        scaling = self.head_dim ** 0.5\n",
        "        attention = energy / scaling\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention = torch.softmax(attention, dim=-1)\n",
        "\n",
        "        # Multiply with values\n",
        "        # (batch_size, heads, seq_length, head_dim)\n",
        "        out = torch.matmul(attention, values)\n",
        "\n",
        "        # Reshape and concatenate heads\n",
        "        # (batch_size, seq_length, embed_size)\n",
        "        out = out.permute(0, 2, 1, 3).reshape(batch_size, seq_length, self.embed_size)\n",
        "\n",
        "        # Final linear layer\n",
        "        out = self.fc_out(out)\n",
        "        return out, attention\n"
      ],
      "metadata": {
        "id": "85YvqwMEA4Si"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrate Self-Attention with a simple example\n",
        "def demonstrate_self_attention():\n",
        "    print(\"=\" * 50)\n",
        "    print(\"SELF-ATTENTION DEMONSTRATION:\")\n",
        "\n",
        "    # Create a small example batch\n",
        "    batch_size = 1\n",
        "    seq_length = 5\n",
        "    embed_size = 8\n",
        "    heads = 2\n",
        "\n",
        "    # Create a random embedding tensor\n",
        "    x = torch.randn(batch_size, seq_length, embed_size)\n",
        "    print(f\"Input shape: {x.shape}\")\n",
        "\n",
        "    # Create self-attention module\n",
        "    attention = SelfAttention(embed_size, heads)\n",
        "\n",
        "    # Forward pass\n",
        "    output, attention_weights = attention(x)\n",
        "\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(f\"Attention weights shape: {attention_weights.shape}\")\n",
        "\n",
        "    # Print attention weights for one head\n",
        "    print(\"Attention weights for head 0:\")\n",
        "    print(attention_weights[0, 0])\n",
        "    print(\"This shows how each position attends to all other positions\")\n",
        "\n",
        "    # Check if first token (position 0) has high attention score with last token\n",
        "    print(f\"Attention from last position to first: {attention_weights[0, 0, -1, 0].item():.4f}\")\n",
        "    print(\"Higher values mean stronger attention between positions\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "# Run the demonstration\n",
        "demonstrate_self_attention()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naWBF7NUA59B",
        "outputId": "0750e2f4-d80f-4dff-8df6-6be208a92623"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "SELF-ATTENTION DEMONSTRATION:\n",
            "Input shape: torch.Size([1, 5, 8])\n",
            "Output shape: torch.Size([1, 5, 8])\n",
            "Attention weights shape: torch.Size([1, 2, 5, 5])\n",
            "Attention weights for head 0:\n",
            "tensor([[0.1460, 0.1890, 0.3634, 0.1926, 0.1089],\n",
            "        [0.0897, 0.2782, 0.3777, 0.0855, 0.1690],\n",
            "        [0.1175, 0.3344, 0.2613, 0.1146, 0.1722],\n",
            "        [0.0894, 0.1968, 0.5028, 0.1089, 0.1022],\n",
            "        [0.1708, 0.2192, 0.2385, 0.1758, 0.1957]], grad_fn=<SelectBackward0>)\n",
            "This shows how each position attends to all other positions\n",
            "Attention from last position to first: 0.1708\n",
            "Higher values mean stronger attention between positions\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 3: Visualizing Self-Attention\n",
        "# ----------------------------------\n",
        "\n",
        "def visualize_attention(model, input_seq):\n",
        "    \"\"\"Visualize the attention weights for a single sequence\"\"\"\n",
        "    # Convert to tensor and add batch dimension\n",
        "    input_tensor = input_seq.unsqueeze(0)  # (1, seq_length)\n",
        "\n",
        "    # Get embeddings\n",
        "    embeddings = model.embedding(input_tensor)  # (1, seq_length, embed_size)\n",
        "\n",
        "    # Pass through self-attention\n",
        "    _, attention = model.attention(embeddings)  # attention: (1, heads, seq_length, seq_length)\n",
        "\n",
        "    # Average attention across heads\n",
        "    avg_attention = attention.mean(dim=1).squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(avg_attention, cmap='viridis')\n",
        "    plt.colorbar()\n",
        "    plt.title(\"Self-Attention Weights\")\n",
        "    plt.xlabel(\"Key Positions\")\n",
        "    plt.ylabel(\"Query Positions\")\n",
        "    plt.show()\n",
        "\n",
        "    return avg_attention"
      ],
      "metadata": {
        "id": "N1gzanLIA_aR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 4: The Transformer Encoder\n",
        "# ------------------------------\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Self-attention block\n",
        "        attention_out, attention_weights = self.attention(x)\n",
        "\n",
        "        # Add & norm (residual connection)\n",
        "        x = self.norm1(attention_out + x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Feed forward block\n",
        "        forward_out = self.feed_forward(x)\n",
        "\n",
        "        # Add & norm (residual connection)\n",
        "        x = self.norm2(forward_out + x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x, attention_weights\n",
        "\n",
        "# Demonstrate Transformer Block functioning\n",
        "def demonstrate_transformer_block():\n",
        "    print(\"=\" * 50)\n",
        "    print(\"TRANSFORMER BLOCK DEMONSTRATION:\")\n",
        "\n",
        "    # Create a sample input\n",
        "    batch_size = 2\n",
        "    seq_length = 5\n",
        "    embed_size = 8\n",
        "    heads = 2\n",
        "\n",
        "    # Create random embedding tensor\n",
        "    x = torch.randn(batch_size, seq_length, embed_size)\n",
        "    print(f\"Input shape: {x.shape}\")\n",
        "\n",
        "    # Create transformer block\n",
        "    block = TransformerBlock(\n",
        "        embed_size=embed_size,\n",
        "        heads=heads,\n",
        "        dropout=0.1,\n",
        "        forward_expansion=2\n",
        "    )\n",
        "\n",
        "    # Forward pass\n",
        "    out, attention = block(x)\n",
        "\n",
        "    print(f\"Output shape: {out.shape}\")\n",
        "    print(f\"Attention weights shape: {attention.shape}\")\n",
        "\n",
        "    # Demonstrate residual connections by checking if output is similar to input\n",
        "    input_norm = torch.norm(x).item()\n",
        "    output_norm = torch.norm(out).item()\n",
        "    diff_norm = torch.norm(x - out).item()\n",
        "\n",
        "    print(\"\\nResidual connection demonstration:\")\n",
        "    print(f\"Input norm: {input_norm:.4f}\")\n",
        "    print(f\"Output norm: {output_norm:.4f}\")\n",
        "    print(f\"Difference norm: {diff_norm:.4f}\")\n",
        "    print(\"Residual connections allow information to flow directly through the network\")\n",
        "    print(\"This helps prevent the vanishing gradient problem in deep networks\")\n",
        "\n",
        "    # Demonstrate layer normalization\n",
        "    sample_vector = x[0, 0]\n",
        "    normalized = block.norm1(sample_vector.unsqueeze(0)).squeeze(0)\n",
        "\n",
        "    print(\"\\nLayer normalization demonstration:\")\n",
        "    print(f\"Original vector mean: {sample_vector.mean().item():.4f}, std: {sample_vector.std().item():.4f}\")\n",
        "    print(f\"Normalized vector mean: {normalized.mean().item():.4f}, std: {normalized.std().item():.4f}\")\n",
        "    print(\"Layer normalization stabilizes training by standardizing activations\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "# Run the demonstration\n",
        "demonstrate_transformer_block()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SWQBAvzBCUW",
        "outputId": "d4a3ff05-dc62-4116-9207-8d61489b7d64"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "TRANSFORMER BLOCK DEMONSTRATION:\n",
            "Input shape: torch.Size([2, 5, 8])\n",
            "Output shape: torch.Size([2, 5, 8])\n",
            "Attention weights shape: torch.Size([2, 2, 5, 5])\n",
            "\n",
            "Residual connection demonstration:\n",
            "Input norm: 10.6661\n",
            "Output norm: 9.1173\n",
            "Difference norm: 6.1627\n",
            "Residual connections allow information to flow directly through the network\n",
            "This helps prevent the vanishing gradient problem in deep networks\n",
            "\n",
            "Layer normalization demonstration:\n",
            "Original vector mean: 0.2208, std: 1.8771\n",
            "Normalized vector mean: 0.0000, std: 1.0690\n",
            "Layer normalization stabilizes training by standardizing activations\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3I70L6tlBG_c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}